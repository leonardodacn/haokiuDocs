[{"content":"","permalink":"http://121.199.2.5:6080/1/","summary":"","title":"1"},{"content":"Gate.io芝麻开门创立于2013年，是全球真实交易量TOP10的数字资产交易平台。\n作为全球首家提供100%保证金审计证明的交易所，Gate.io在全球多个国家进行合法注册，向全球数千万用户提供安全可靠、真实透明的数字资产交易服务。\n使用下面的链接注册，可以获得平台的奖励\nhttps://www.gate.io/ref/XgBMB1A/haokiu?ref_type?=102\n","permalink":"http://121.199.2.5:6080/gateio/","summary":"Gate.io芝麻开门创立于2013年，是全球真实交易量TOP10的数字资产交易平台。\n作为全球首家提供100%保证金审计证明的交易所，Gate.io在全球多个国家进行合法注册，向全球数千万用户提供安全可靠、真实透明的数字资产交易服务。\n使用下面的链接注册，可以获得平台的奖励\nhttps://www.gate.io/ref/XgBMB1A/haokiu?ref_type?=102","title":"数字货币交易所-gateio"},{"content":"The enkrypt invite code is FC433A\nMint tickets with Enkrypt every day, invite friends to earn more tickets.\nMore tickets — more chances to win!\nShare your code with friends and get +1 ticket for every friend’s mint.\nCode FC433A\naddress：https://raffle.enkrypt.com\n","permalink":"http://121.199.2.5:6080/wgJnpu/","summary":"The enkrypt invite code is FC433A\nMint tickets with Enkrypt every day, invite friends to earn more tickets.\nMore tickets — more chances to win!\nShare your code with friends and get +1 ticket for every friend’s mint.\nCode FC433A\naddress：https://raffle.enkrypt.com","title":"enkrypt invite code FC433A"},{"content":"googo 可以让你更好的浏览外文网站，比如meta、youtube，google等。 它之前的网址是googo.in，现在已经不用访问。是一款非常好用的科学上网工具，使用简单。\n使用这个链接可以获取优惠：https://cn.googo.us/#/register?code=lbVOQjeS\n或者使用这个邀请码：lbVOQjeS\n","permalink":"http://121.199.2.5:6080/googo/","summary":"googo 可以让你更好的浏览外文网站，比如meta、youtube，google等。 它之前的网址是googo.in，现在已经不用访问。是一款非常好用的科学上网工具，使用简单。\n使用这个链接可以获取优惠：https://cn.googo.us/#/register?code=lbVOQjeS\n或者使用这个邀请码：lbVOQjeS","title":"googo 邀请码 lbVOQjeS"},{"content":"第一章 并发编程的挑战第2章 InnoDB存储引擎第1章 Java的I/O演进之路第1章 Spring框架的由来第2章 Tomcat总体架构三、Paxos的工程实践序第1章 概述第6章 深入分析ClassLoader工作机制第8章 虚拟机字节码执行系统\n","permalink":"http://121.199.2.5:6080/bk-5/","summary":"第一章 并发编程的挑战第2章 InnoDB存储引擎第1章 Java的I/O演进之路第1章 Spring框架的由来第2章 Tomcat总体架构三、Paxos的工程实践序第1章 概述第6章 深入分析ClassLoader工作机制第8章 虚拟机字节码执行系统","title":"Hobert读书笔记"},{"content":"2.2.3 Paxos算法详解 假设有一组可以提出提案的进程集合，那么对于一个一致性算法来说需要保证以下几点：\n在这些被提出的提案中，只有一个会被选定。\n如果没有填被提出，那么就不会有被选定的提案。\n当一个填被选定后，进程应该可以获取被选定的提案信息。\n对于一致性来说，安全性（Safety）需求如下：\n只有被提出的提案才能被选定。 只有一个值被选定。 如果某么进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个。 提案的选定\n大多数。\n推导过程\nP1：一个Accptor必须批准它收到的第一个提案\nP1a：一个Acceptor只要尚未响应过任何编号大于Mn的Prepare请求，那么它就可以接收这个编号为Mn的提案。（优化：可以忽略已批准过的提案的Prepare请求）\nP2：如果编号为M0，Value值为V0的提案（即[M0,V0]）被选定了，那么所有比编号M0更高的，且被选定的提案，其Value值必须为V0。\nP2a：如果编号为M0，Value值为V0的提案（即[M0,V0]）被选定了，那么所有比编号M0更高的，且被Acceptor批准的提案，其Value值必须为V0。\nP2b：如果一个提案[M0,V0]被选定后，那么之后任何Proposer产生的编号更高的提案，其Value值都为V0。\nP2c：对于任意的Mn和Vn，如果提案[Mn,Vn]被提出，那么肯定存在一个由半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个：\nS中不存在任何批准过编号小于Mn的提案的Acceptor。 选取S中所有Acceptor批准的编号小于Mn的提案，其中编号最大的那个提案其Value值是Vn。 推导过程为第二数学归纳法。略\nProposer生成提案\n对于Proposer，获取被通过的提案比预测可能会被通过的提案简单。\nProposer选择一个新的提案编号Mn，然后向某个Acceptor集合的成员发送请求，要求该集合中的Acceptor做出如下回应。 像Proposer承诺，保证不再批准任何编号小于Mn的提案。 如果Acceptor已经批准过任何提案，那么其就向Proposer反馈当前该Acceptor已经批准的编号小于Mn但为最大编号的那个提案的值。 如果Proposer收到了来自半数以上的Acceptor的响应结果，那么它就可以产生编号为Mn、Value值为Vn的提案，这里的Vn是所有响应中编号最大的提案Value值。当然还存在一种情况，就是半数以上的Acceptor都没有批准过任何提案，即响应中不包含任何的提案，那么此时Vn值就可以由Proposer任意选择。 Acceptor批准提案\n一个Acceptor可能会收到来自Proposer的两种请求，分别是Prepare请求和Accept请求，分别相应如下：\nPrepare请求：Acceptor可以在任何时候响应一个Prepare请求。 Accept请求：在不违背Accept现有承诺的前提下，可以任意响应Accept请求。 算法陈述\n阶段一：\nProposer发送提案编号Mn； Acceptor根据约束接收提案，如果接收过返回接收最大值Vn； 阶段二：\n如果Proposer收到大多数A的响应，发送[Mn,Vn]； Acceptor根据约束接收提案； 提案的获取\n通知全部Learner\n选取主Learner\n将主Learner改为Learner集合\n通过选取主Proposer保证算法的活性\n三、Paxos的工程实践 3.1 Chubby 一个分布式锁服务。解决分布式协作，元数据存储，Master选举等一系列与分布式锁服务相关的问题。\n底层为Paxos算法。\n3.1.1 概述 ","permalink":"http://121.199.2.5:6080/3426588a7834499f8a9a1b7e46817c0c/","summary":"2.2.3 Paxos算法详解 假设有一组可以提出提案的进程集合，那么对于一个一致性算法来说需要保证以下几点：\n在这些被提出的提案中，只有一个会被选定。\n如果没有填被提出，那么就不会有被选定的提案。\n当一个填被选定后，进程应该可以获取被选定的提案信息。\n对于一致性来说，安全性（Safety）需求如下：\n只有被提出的提案才能被选定。 只有一个值被选定。 如果某么进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个。 提案的选定\n大多数。\n推导过程\nP1：一个Accptor必须批准它收到的第一个提案\nP1a：一个Acceptor只要尚未响应过任何编号大于Mn的Prepare请求，那么它就可以接收这个编号为Mn的提案。（优化：可以忽略已批准过的提案的Prepare请求）\nP2：如果编号为M0，Value值为V0的提案（即[M0,V0]）被选定了，那么所有比编号M0更高的，且被选定的提案，其Value值必须为V0。\nP2a：如果编号为M0，Value值为V0的提案（即[M0,V0]）被选定了，那么所有比编号M0更高的，且被Acceptor批准的提案，其Value值必须为V0。\nP2b：如果一个提案[M0,V0]被选定后，那么之后任何Proposer产生的编号更高的提案，其Value值都为V0。\nP2c：对于任意的Mn和Vn，如果提案[Mn,Vn]被提出，那么肯定存在一个由半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个：\nS中不存在任何批准过编号小于Mn的提案的Acceptor。 选取S中所有Acceptor批准的编号小于Mn的提案，其中编号最大的那个提案其Value值是Vn。 推导过程为第二数学归纳法。略\nProposer生成提案\n对于Proposer，获取被通过的提案比预测可能会被通过的提案简单。\nProposer选择一个新的提案编号Mn，然后向某个Acceptor集合的成员发送请求，要求该集合中的Acceptor做出如下回应。 像Proposer承诺，保证不再批准任何编号小于Mn的提案。 如果Acceptor已经批准过任何提案，那么其就向Proposer反馈当前该Acceptor已经批准的编号小于Mn但为最大编号的那个提案的值。 如果Proposer收到了来自半数以上的Acceptor的响应结果，那么它就可以产生编号为Mn、Value值为Vn的提案，这里的Vn是所有响应中编号最大的提案Value值。当然还存在一种情况，就是半数以上的Acceptor都没有批准过任何提案，即响应中不包含任何的提案，那么此时Vn值就可以由Proposer任意选择。 Acceptor批准提案\n一个Acceptor可能会收到来自Proposer的两种请求，分别是Prepare请求和Accept请求，分别相应如下：\nPrepare请求：Acceptor可以在任何时候响应一个Prepare请求。 Accept请求：在不违背Accept现有承诺的前提下，可以任意响应Accept请求。 算法陈述\n阶段一：\nProposer发送提案编号Mn； Acceptor根据约束接收提案，如果接收过返回接收最大值Vn； 阶段二：\n如果Proposer收到大多数A的响应，发送[Mn,Vn]； Acceptor根据约束接收提案； 提案的获取\n通知全部Learner\n选取主Learner\n将主Learner改为Learner集合\n通过选取主Proposer保证算法的活性\n三、Paxos的工程实践 3.1 Chubby 一个分布式锁服务。解决分布式协作，元数据存储，Master选举等一系列与分布式锁服务相关的问题。\n底层为Paxos算法。\n3.1.1 概述 ","title":"三、Paxos的工程实践"},{"content":"序 大、快、多样性只是表象，大数据的真正价值在于生命性和生态性。（活数据）\n第1 章 总述 如果不能对数据进行有序、有结构地分类组织和存储，如果不能有效利用并发掘它，继而产生价值，那么它同时也成为一场“灾难”。无需、无结构的数据犹如堆积如山的垃圾，给企业带来的是有令人咋舌的高额成本。\n要求：\n如何建设高效的数据模型和体系，是数据易用，避免重复建设和数据不一致性； 如何提供高效易用的数据开发工具； 如何做好数据质量保障； 如何有效管理和控制日益增长的存储和计算消耗； 如何保证数据服务的稳定，保证其性能； 如何设计有效的数据产品高效赋能于外部客户和内部员工 数据采集层 日志采集体系方案包括两大体系：Aplus.JS是Web端日志采集技术方案；UserTrack是App端日记采集技术方案。\n在采集技术基础上面向各个场景的埋点规范。\n在传输方面采用TimeTunel（TT）,它既包括数据库的增量数据传输，也包括日志数据的传输；既支持实时流式计算，也知乎此各种时间窗口的批量计算。\n也通过数据同步工具（DataX和同步中心，其中同步中心是给予DataX易用性封装的）直连异构数据库（备库）来抽取各种时间窗口的数据。\n数据计算层 数据只有被整合和计算,才能被用于洞察商业规律,挖掘潜在信息，从而实现大数据价值,达到赋能于商业和创造价值的目的。\n阿里巴巴的数据计算层包括两大体系:数据存储及计算云平台（离线计算平台MaxCompute和实时计算平台StreamCompute）和数据整合及管理体系（内部称之为“ OneData ”） 。\nMaxCompute是阿里巴巴自主研发的离线大数据平台。\nStreamCompute是阿里巴巴自主研发的流式大数据平台。\nOneData是数据整合及管理的方法体系和工具。\n借助此体系，构建了数据公共层。\n从数据计算频率角度来看，阿里数据仓库可以分为离线数据仓库（传统的数据仓库概念）和实时数据仓库（典型应用：双11实时数据）。\n阿里数据仓库的数据加工链路也是遵循业界的分层理念，包括：\n操作数据层（Operational Data Store，ODS）; 明细数据层（Data WareHouse Detail，DWD）； 应用数据层（Application Data Store，ADS）。 通过数据仓库不同层次之间的加工过程实现从数据资产向信息资产的转化，并且对整个过程进行有效的元数据管理及数据质量处理。\n元数据模型整合及应用是一个重要的组成部分，主要包含：\n数据源元数据 数据仓库元数据 数据链路元数据 工具类元数据 数据质量类元数据 元数据应用主要面向数据发现、数据管理等，如用于存储、计算和成本管理。\n数据服务层 当数据已被整合和计算好之后，需要提供给产品和应用进行数据消费。\n针对不同的需求，数据服务层的数据源架构在多种数据库之上，如Mysql和HBase。\n数据服务层主要考虑性能、稳定性、扩展性。\nOneService（数据服务平台）一数据仓库整合计算好的数据作为数据源，对外通过接口的方式提供数据服务，主要提供简单数据查询服务、复杂数据查询服务（用户识别、用户画像等）和实时数据推送服务。\n数据应用层 第1篇 数据技术篇 第2章 日志采集 第2篇 数据模型篇 第8章 大数据领域建模综述 8.1 为什么需要数据建模 如何将数据进行有序、有结构地分类组织和存储？\n数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用角度合理存储数据。有了适合业务和基础数据存储环境的模型，那么大数据就能获得以下好处：\n性能：良好的数据模型能帮助我们快速查询所需要的数据，减少数据的I/O吞吐。 成本：良好的数据模型能极大地减少不必要的数据冗余，也能实现计算结果复用，极大地降低大数据系统中的存储和计算成本。 效率：良好的数据模型能极大地改善用户使用数据的体验，提高使用数据的效率。 质量：良好的数据模型能改善数据统计口径的不一致性，减少数据计算错误的可能性。 8.2 关系行数据库系统和数据仓库 大数据领域仍然使用关系型数据库，使用关系理论描述数据之间的关系，只是基于其数据存储的特点关系数据模型的范式上有了不同的选择。\n8.3 从OLTP和OLAP系统特别看模型方法论的选择 OLTP系统通常面向的主要数据操作是随即读写，主要采用满足3NF的实体关系模型存储数据，从而在事务处理中解决数据的冗余和一致性问题；而OLAP系统面向的主要数据操作时批量读写，事物处理中的一致性不是OLAP所关注的，其主要关注数据的集合，以及在一次性的复杂大数据查询和处理中的性能，因此它需要采用一些不同的数据建模方法。\n8.4 典型的数据仓库建模方法论 8.4.1 ER模型 数据仓库中的3NF和OLPT系统中的3NF的却别在于，它是站在企业角度面向主题的抽象，而不是针对某个具体业务流程的实体对象关系的抽象。其具有以下几个特点：\n需要全面了解企业业务和数据； 事实周期非常长； 对建模人员的能力要求非常高。 采用ER模型建设数据仓库魔性的出发点是整合数据将个系统中的数据以整个企业角度按主题进项相似性组合和合并，并进行一致性处理，为数据分析决策服务，但是并不能直接用于分析决策。\n其建模步骤分为三个阶段：\n高层模型：一个高度抽象的模型，描述主要的主题以及主题间的关系，用语描述企业的业务总体概况。 中层模型：在高层模型的基础上，细化主题的数据项。 物理模型（也叫底层模型）：在中层模型的基础上，考虑物理存储，同时基于性能和平台特点进行物理属性的设计，也可能作一些表的合并、分区的设计等。 实践典型：金融业务FS-LDM。\n8.4.2 维度模型 是数据仓库工程领域最流行的数据仓库建模的经典。\n维度建模从分析决策的需求出发构建模型，为分析需求服务，因此它重点关注用户如何更快速地完成需求分析，同时具有较好的大规模复杂查询的响应性能。其典型的代表是星形模型，以及在一些特殊场景下使用的雪花模型。其设计分为一下几个步骤：\n选择需要进行分析决策的业务过程。业务过程可以是单个业务事件，比如交易的支付、退款等；也可以时某个时间的状态，比如当前的账户余额等；还可以是一系列相关业务时间组成的业务流程，具体需要看我们分析的是某些事件发生情况，还是当前状态，或是事件流转效率； 选择粒度。在事件分析中，我们要预判所有分析需要细分的程度，从而决定选择的粒度。粒度是维度的一个组合； 识别维表。选择好粒度之后，就需要细雨此粒度设计维表，包括维度属性，用于分析时进行分组和筛选； 选择事实。确定分析需要衡量的指标。 8.4.3 Data Vault模型 ER模型的衍生。其设计的出发点也是为了实现数据的整合，但不能直接用于数据分析决策。\n8.4.4 Anchor模型 对Data Vault模型做了进一步规范化处理。\n8.5 阿里巴巴数据模型实践综述 第一个阶段：完全应用驱动的时代，构建在Oracle上，数据完全一满足报表需求为目的，将数据以与源结构相同的方式同步不到Oracle（ODS层），数据工程师基于ODS数据进行统计，基本没有系统化的模型方法体系，完全基于对Oracle数据库特性的利用进行数据存储和加工，部分采用一些维度建模的缓慢变化维方式进行历史数据处理。这时候的数据架构只有两层，即ODS+DSS。\n第二个阶段：随着业务发展，数据量增加，性能成为一个较大的问题，因此引入了当时MPP架构体系的Greenplum，同时进行数据架构优化希望通过模型技术改变开发模型，消除一些冗余，提升数据的一致性。开始尝试将工程领域比较流行的ER模型+维度模型方式，构建出一个四成的模型架构，即ODL（数据操作层）+BDL（基础数据层）+IDL（接口数据层）+ADL（应用数据层）。ODL和源系统保持一致；BDL希望以引入ER模型，加强市局的整合，构建一致的基础数据模型；IDL基于维度模型方法构建集市层；ADL完成应用的个性化和基于展现需求的数据组装。遇到的困难与挑战：构建ER模型（业务发展迅速，人员快速变化、业务知识功底的不够全面）。\n在不太成熟、快速变化的业务面前，构建ER模型的风险非常大，不太适合去构建ER模型。\n第三个阶段：业务继续发展，Hadoop为代表的分布式存储计算平台也在快速发展，阿里巴巴自主研发的分布式计算平台MaxCompute。选择了以Kimball的维度建模为核心理念的模型方法论，同时对其进行了一定的升级和扩展，构建了阿里巴巴集团的公共层模型数据架构体系。\n数据公共层建设的目的是着力解决数据存储和计算的共享问题。\n阿里巴巴数据公共层建设的指导方法是一套同意话的集团数据整合及管理的方法体系（内部成为OneData），其包括一致性的指标定义体系、模型设计方法体系及配套工具。\n第9章 阿里巴巴数据整合及管理体系 面对爆炸式增长的数据，如何建设高效的数据模型和体系，对这些数据进行有序和有结构地分类组织和存储，避免重复建设和数据不一致性，保证数据的规范性，一直时大数据系统建设不断追求的方向。\n9.1 概述 从业务架构设计到模型设计，从数据研发到数据服务，做到数据可管理、可追溯、可规避重复建设。\n9.1.1 定位及价值 建设统一的、规范化的数据介入层（ODS）和数据中间层（DWD和DWS），通过数据服务和数据产品，完成服务于阿里巴巴的数据系统建设，即数据公共层建设。提供标准化的（Standard）、共享的（Shared）、数据服务（Service）能力，将数据互通成本，释放计算、存储、人力等资源，以消除业务和技术之痛。\n9.1.2 体系架构 业务板块：由于阿里巴巴集团业务生态庞大、所以根据业务的属性划分出几个相对独立的业务板块，业务板块之间的指标或业务重叠性较小，如点上业务板块涵盖淘系、B2B系和AliExpress系等。\n规范定义：阿里数据业务庞大，结合行业的数据仓库建设经验和阿里数据自身特点，设计出的一套数据规范命名体系，规范定义将会被用在模型设计中。\n模型设计：以维度建模理论为基础，给予维度建模总线架构，构建一致性的维度和事实（进行规范定义）。同时，在落地标模型时，给予阿里自身业务特点，设计出一套表规范命名体系。\n9.2 规范定义 规范定义指以维度建模作为理论基础，构建总线矩阵，划分和定义数据域、业务过程、维度、度量/原子指标、修饰类型、修饰词、时间周期、派生指标。\n9.2.1 名词术语 名词术语 解释 数据域 指面向业务分析，将业务过程或者维度进行抽象的集合。其中，业务过程可以概括为一个个不可拆分的行为事件，在业务过程之下，可以定义指标；维度是指度量的环境，如买家下单时间，买家是维度。为保障整个体系的生命力，数据域是需要抽象提炼的，并且长期维护和更新的，但不轻易变动。在划分数据域时，既能涵盖当前所有的业务需求，又能在新业务进入时无影响地被包含进已有的数据域中和扩展新的数据域。 业务过程 指企业的业务活动事件，如下单、支付、退款都是业务过程。请注意，业务过程是一个不可拆分的行为事件，通俗地讲，业务过程就是企业活动中的事件。 时间周期 用来明确数据统计的时间范围或者时间点，如最近30天、自然周、截至当日等。 修饰类型 是对修饰词的一种抽象划分。修饰类型从术语某个业务域，如日志域的访问终端类型涵盖无线端、PC端等修饰词 修饰词 指除了统计维度以外指标的业务场景限定抽象。修饰词隶属于一种修饰类型，如在日志域的访问终端类型下，有修饰词PC端、无线端等 度量/原子指标 原子指标和度量含义相同，基于某一业务事件行为下的度量，是业务定义中不可再拆分的指标，具有明确业务含义的名词，如支付金额 维度 维度是度量的环境，用来反映业务的一类属性，这类属性的集合构成一个维度，也可称之为实体对象。维度属于一个数据域，如地理纬度（其中包括国家、地区、省以及城市等级别的内容）、时间维度（其中包括年、季、月、周、日等级别的内容） 维度属性 维度属性隶属于一个维度，如地理维度里面的国家名称、国家ID、省份名称等都属于维度属性 派生指标 派生指标=一个原子指标+多个修饰词（可选）+时间周期。可以理解为对原子指标业务统计范围的圈定。如原子指标：支付金额，最近1天海外买家支付金额则为派生指标（最近1天为时间周期，海外为修饰词，买家作为维度，而不作为修饰词） 9.2.2 指标体系 本文在讲述指标时，会涵盖其组成体系（原子指标、派生指标、修饰类型、修饰词、时间周期），将它们作为一个整体来解读。\n基本原则\n（1）组成体系之间的关系\n派生指标有原子指标、时间周期修饰词、若干其他修饰词组合得到 原子指标、修饰类型及修饰词，直接归属在业务过程下，其中修饰词继承修饰类型的数据域。 派生指标可以选择多个修饰词，修饰词之间的关系为“或”或者“且”，有具体的派生指标语义定义决定 派生指标唯一归属一个原子指标，继承原子指标的数据域，与修饰词的数据域无关。 一般而言，事务型指标和存量型指标智慧为一定味道一个业务过程，如果遇到同时有两个行为发生、需要多个修饰词、生成一个派生指标的情况，则选择时间靠后的行为创建原子指标，选择时间靠前的行为创建修饰词。\n原子指标有确定的英文字段名、数据类型和算法说明；派生指标要继承原子指标的英文名，数据类型和算法要求。 （2） 命名约定\n命名所用术语。指标命名，尽量使用英文简写，其次是英文，当指标英文名太长时，可考虑用汉语拼音首字母命名。如中国制造，用zgzc。在OneData工具中维护着常用的名词属于，以用来进行命名。 业务过程。英文名：用英文或英文的缩写或者中文拼音简写；中文名：具体的业务过程中文即可。 关于存量型指标对应的业务过程的约定：实体对象英文名+_stock。如在线会员数、一星会员数等，其对应的业务过程为mbr_stock；在线商品数、商品SKU种类小于5的商品书，其对应的业务过程为itm+stock。\n原子指标。英文名：动作+度量；中文名：动作+度量。原子指标必须挂靠在某个业务过程下。 修饰词。只有时间周期才会有英文名，且长度为2位，加上“_”位3位，例如_1d。其他修饰词无英文名。 派生指标。英文名+原子指标英文名+时间周期修饰词（3位）+序号（4位，例如_001）；中文名：时间周期修饰词+[其他修饰词]+原子指标。 为了控制派生指标的英文名称过长，在英文名的理解和规范上做了取舍，所有修饰词的含义都含入了序号中。序号是跟觉原子指标+派生指标自增的。\n（3）算法\n原子指标、修饰词、派生指标的算法说明必须让各种使用人员看得明白，包括：\n算法该书——算法对应的用户容易理解的阐述。 举例——通过具体例子帮助理解指标算法。 SQL算法说明——对于派生指标给出SQL的写法或者伪代码。 操作细则\n（1）派生指标的种类\n分为三类：事物型指标、存量型指标和复合型指标。按照其特性不同，有些必须新建原子指标，有些可以在其他类型原子指标的基础上增加修饰词形成派生指标。\n事物型指标：指对业务活动进行衡量的指标。（例如新发商品数，重发商品数等）需要维护原子指标及修饰词，在此基础上创建派生指标。 存量型指标：只对对实体对象某些状态（商品、会员）的统计。（例如商品总数、注册会员总数），需要维护原子指标及修饰词，再次基础上创建派生指标，对应的时间周期一般位“历史截止至当前某个时间”。 复合型指标：是在事物型指标和存量型指标的基础上复合而成的。例如浏览UV-下单买家数转化率，有些需要创建新原子指标，有些则可以在事物型或存量型原子指标的基础上增加修饰词得到派生指标。 （2）复合型指标的规则\n比率型：创建原子指标。\n比例型：创建原子指标，如百分比、占比。\n变化量型：不创建原子指标，增加修饰词，在此基础上创建派生指标。\n变化率型：创建原子指标。\n统计型（均值、分位数等）：不创建原子指标，增加修饰词，在此基础上创建派生指标；在修饰类型“统计方法”下增加修饰词，如人均、日均、行业平均、商品平均等。\n排名型：创建原子指标，一般为top_xxx_xxx或者rank组合，创建派生指标时选择对应的修饰词：\n统计方法 排名名次 排名范围 根据什么排序 对象集合型：主要是指数据产品和应用需要展现数据时，将一些对象那个以k-v对的方式存储在一个字段，方便前端展现。创建原子指标，一般为xx串；创建派生指标时选择对应的修饰词如下：\n统计方法（如排序、升序）。 排名名次（如TOP10）。 排名范围（如行业、行业）。 其他规则 （1）上下层级派生指标同时存在时\n如最近1天支付金额和最近1天PC端支付金额，建议使用前者，把PC端作为维度属性存放在物理表中体现。\n（2）父子关系原子指标存在时\n当父子关系原子指标存在时，派生指标使用原子指标创建派生指标。如PV，IPV（商品详情页PV），当统计商品详情也PV时，优先选择子原子指标。\n9.3 模型设计 9.3.1 指导理论 遵循维度建模思想。数据模型的维度设计主要以维度建模理论为基础，基于维度数据模型总线架构，构建一致性的维度和事实。\n9.3.2 模型层次 三层：数据操作层ODS、公共维度模型层CDM、应用数据层ADS，其中公共维度模型层包括明细数据层DWD和汇总数据层DWS。\n操作数据层ODS：把操作系统数据几乎无处理地存放在数据仓库系统中。\n同步：结构化数据增量或全量同步到MaxCompute。 结构化：非结构化（日志）结构化处理并存储到MaxCompute。 累积历史、清洗：根据数据业务需求及稽核和审计要求保存历史数据、清洗数据。 公共维度模型层CDM：存放明细事实数据、维表数据及公共指标汇总数据，其中明细事实数据、维表数据一般根据ODS层数据加工生成；公共指标汇总数据一般根据维表数据和明细事实数据加工生成。\nCDM层又细分为DWD层和DWS层，采用维度模型方法作为理论基础，更多地采用一些维度退化受大，将唯独退化至事实表中，减少事实表和维表的关联，提高明细数据表的易用性；同时在汇总数据层，加强指标的维度退化，采用更多的宽表化手段构建公共指标数据层，提升公共指标的复用性，减少重复加工。其主要功能如下：\n组合相关和相似数据：采用明细宽表，复用关联计算，减少数据扫描。 公共指标统一加工：基于OneData体系构建命名规范、口径一致和算法统一的统计指标，为上层数据产品、应用和服务提供公共指标；建立逻辑汇总宽表。 建立一致性维度：建立一致的数据分析维表，降低数据计算口径、算法不统一的风险。 应用数据层ADS：存放数据产品个性化的统计指标数据，根据CDM层和ODS层加工生成。\n个性化指标加工：不公用性、复杂性（指数型、比值型、排名型指标）。 基于应用的数据组装：大宽表集市、横表转纵表、趋势指标串。 数据调用服务优先使用公共维度模型层（CDM）数据，当公共层没有数据时，需评估是否需要创建公共层数据，当不需要建设公用的公共层时，方可直接使用操作数据层ODS数据。应用数据层ADS作为产品特有的个性化数据一般部队外提供数据服务，但是ADS作为被服务方也需要遵守这个约定。\n9.3.3 基本原则 高内聚低耦合 核心模型和扩展模型分离 公共处理逻辑下沉及单一 成本与性能平衡 数据可回滚 一致性 命名清晰、可理解 9.4 模型实施 9.4.1 业界常用的模型实施过程 Kimball模型实施过程 （1）高层模型\n（2）详细模型\n（3）模型审查、再设计和验证\n（4）提交ETL设计和开发\nInmon模型实施过程\n其他模型实践过程\n业务建模，生成业务模型，主要解决业务层面的分解和程序化。 领域建模，生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。 逻辑建模，生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系数据库层次的逻辑化。 物理建模，生成模型逻辑，主要解决逻辑模型针对不同关系数据库的物理化以及性能等一些具体的技术问题。 9.4.2 OneData实施过程 指导方针 首先，在建设大数据数据仓库时，要进行充分的业务调研和需求分析。\n其次，进行数据总体架构设计，主要是跟觉数据域对数据进行划分，按照维度建模理论，构建总线矩阵、抽象出业务过程和维度。\n再次，对报表需求进行抽象整理出相关指标体系。\n最后，就是代码研发和运维。\n实施工作流 （1）数据调研\n业务调研 一般各个业务领域独资建设数据仓库，业务领域内的业务线由于业务相似、业务相关性较大，进行统一集中建设。\n需求调研 与BI，运营商讨数据诉求（大部分为报表需求）。\n对报表系统中现有的报表进行研究分析。\n（2）架构设计\n数据域划分 数据域面向业务分析，将业务过程或者维度进行抽象的集合。\n业务过程可以概括为一个个不可拆分的行为事件。\n为保障整个体系的生命力，数据域需要抽象提炼，并且长期维护和更新，但不轻易变动。\n构建总线矩阵 明确每个数据域下有哪些业务过程。\n业务过程与哪些维度相关，并定义每个数据域下的业务过程和维度。\n（3）规范定义\n定义指标体系、修饰词、时间周期和派生指标。\n（4）模型设计\n包括维度和属性的规范定义，维表、明细事实表和汇总事实表的模型设计。\n（5）总结\n该实施过程是一个高度迭代和动态的过程，一般采用螺旋式实施方法。在总体架构设计完成之后，开始根据数据域进行迭代式模型设计和评审。在架构设计、规范定义和模型设计等模型实施过程中，都会引入评审机制,以确保模型实施过程的正确性。\n第10章 维度设计 10.1 维度设计基础 10.1.1 维度的基本概念 维度是维度建模的基础和灵魂。再维度建模中，将度量称为“事实”，将环境描述为“维度”，维度是用于分析事实所需要的多样环境。\n维度所包含的表示维度的列，称为维度属性。维度属性是查询约束条件、分组和报表标签生成的基本来源，是数据易用性的关键。维度的作用一般是查询约束、分类汇总以及排序等。\n获取维度或维度属性：\n在报表中获取 和业务人员的交谈中发现维度或维度属性 维度使用主键标识其唯一性，主见也是确保与之相连的任何事实表之间存在引用完整性的基础。\n主键有两种：代理键和自然键。\n10.1.2 维度的基本设计方法 维度的设计过程就是确定维度属性的过程。数据仓库的能力直接与维度属性的质量和深度成正比。\n淘宝商品的维度设计：\n选择维度或新建维度。必须保证维度的唯一性。\n确定主维表。此处一般是ODS表，直接与业务系统同步。\n确定相关维表。根据对业务的树立，确定哪些表和主维表存在关联关系，并选择其中的某些表用于生成维度属性。\n确定维度属性。先从主维表中选择维度属性或生成新的维度属性，然后从相关维表中选择维度属性或生成心的维度属性。\n确定维度属性的几点提示：\n（1）尽可能生成丰富的维度属性\n（2）尽可能多地给出包括一些富有意义的文字性描述。\n（3）区分数字型属性和事实。需要参考字段的具体用途。\n（4）尽量沉淀出通用的维度属性。\n10.1.3 维度的层次机构 维度中的一些描述属性以层次方式或一对多的方式相互关联，可以被理解为包含连续主从关系的属性层次。\n层次的最底层代表维度中描述最低级别的详细信息，最高层代表最高级别的概要信息。\n在属性的层次结构中进行钻取是数据钻取的方法之一。\n10.1.4 规范化和反规范化 当属性层次被实例化为一系列维度，而不是单一的维度时，被称为雪花模式。\n维度的属性层次合并到单个维度中的操作称为反规范化。分析系统的主要目的是用于数据分析和统计，如何更方便用户进行统计分析决定了分析系统的优劣。采用雪花模式，用户在统计分析的过程中需要大量的关联操作，使用复杂度高，同时查询性能很差；而采用反规范化处理，则方便、易用且性能好。\n采用雪花模式，除了可以节约一部分存储外，对于OLAP系统来说没有其他效用。\n10.1.5 一致性维度和交叉探查 迭代式的构建数据仓库存在的问题：单独构建会形成独立性数据即使，导致严重的不一致性。\n通过构建企业范围内一致性维度和事实来构建总线架构。\n数据仓库总线架构的重要基石之一就是一致性维度。将不同数据域的商品的事实合并在一起进行数据探查，如计算转化率等，成为交叉探查。\n如果不同数据域的计算过程使用的维度不一致，就会导致交叉探查存在问题。当存在重复的维度，但维度属性或维度属性的值不一枝时，会导致交叉探查无法进行或交叉探查结果错误。基本可以划分为维度格式和内容不一致两种类型。\n维度一致性的几种表现形式：\n共享维表。 一致性上卷，其中一个维度的维度属性是另一个维度的维度属性的子集，且两个维度的公共维度属性结构和内容相同。 交叉出行，两个维度具有部分相同的维度属性。 10.2 维度设计高级主题 10.2.1 维度整合 数据仓库是一个面向主题的、继承的、非易失的且随时间变化的数据集合，用来支持管理人员的决策。\n不同数据来源之前的具体差异：\n应用在编码、命名习惯、度量单位等方面会存在很大的差异。 应用出于性能和扩展性的考虑，或者随技术架构的演变，以及业务的发展，采用不同的屋里实现。 数据集成的具体体现：\n命名规范的统一。表明、字段名 字段类型的统一。相同和相似字段的字段类型统一。 公共代码及代码值的统一。 业务含义相同的表的统一。高内聚、低耦合，主要有以下几种集成方式： 采用主从表的设计方式，主表放基本信息，从表放从属信息。 直接合并，共有信息和个性信息都放在同一个表中。 不合并，因为源表的表结构及主键等差异很大。 表级别的整合：\n垂直整合，即不同的来源表包含相同的数据集，只是存储的信息不同。 水平整合，及不同的来源表包含不同的数据集，不同子集之间无交叉，也可以存在部分交叉。交叉或者超自然键 10.2.2 水平拆分 维度通常可以按照类别或类型进行细分。\n设计维度的两种方案：\n将维度的不同分类实例化为不同的属性，同时在主维度中保存公共属性 维护单一维度，包含所有可能的属性 设计维度的三个原则：\n扩展性。高内聚、低耦合 效能。在性能和存储成本方面取得平衡。 易用性。可理解度高则易用性低。 对维度进行水平拆分是的两个依据：\n维度的不同分类的属性差异情况。 业务的关联程度。 10.2.3 垂直拆分 维度属性的来源表产出的时间不同。维度属性使用频率不同。有的维度属性经常变化。\n基于扩展性、产出时间、易用性等方面的考虑，设计主从维度。主维表存放稳定、产出时间早、热度高的属性。\n10.2.4 历史归档 归档策略：\n同前台归档策略，在数仓中实现前台归档算法。适用于前台逻辑简单 同前台归档策略，但采用数据库变更日志的方式。不需要关注前台归档策略，简单易行 数仓自定义归档策略。原则是比前台应用晚归档，少归档。 如果技术条件允许，能够解析数据库binlog日志，建议使用归档策略2，规避前台归档算法。\n10.3 维度变化 10.3.1 缓慢变化维 反映历史变化，如何处理维度变化？\n缓慢变化维：维度的属性并不是静态的，会随着时间发生缓慢变化。与数据增长较为快速的事实表相比，维度变化相对缓慢。\n处理缓慢变化维的三种方式：\n重写维度值。不保留历史数据，始终最新 插入新的维度行。保留历史数据 添加维度列。 10.3.2 快照维度 每天保留一份全量快照数据。\n优点：\n简单而有效，开发和维护成本低。 使用方便，理解性好。 弊端：极大的存储浪费。\n10.3.3 极限存储 透明化：底层的数据还是历史拉链数据，但是上层做一个试图或者hook，吧对极限存储前的表转换成对极限存储表的查询。\n分月做历史拉链表。\n极限存储压缩了全量存储的成本，同时对下游用户透明。\n局限性：产出效率较低，对于变化频率高的数据并不能达到节约成本的效果。\n实际生产中的格外处理：\n在极限存储前做一个全量存储表，保留最近一段时间的全量分区数据。 对于部分变化频率频繁的字段需要过滤。 10.3.4 微型模型 通过将一些属性移到全新的维表中，可以解决维度的过度增长导致极限存储效果大打折扣的问题。通过垂直拆分和微型模型可以实现。\n阿里数仓不使用此技术的原因：\n微型维度的局限性。 ETL逻辑复杂。 破坏了维度的可浏览性。 10.4 特殊维度 10.4.1 递归层次 按照层级是否固定分为均衡层次结构和非均衡层次结构。\n由于很多数据仓库系统和商业只能工具不支持递归SQL，且递归的成本较高，所以需要对此层次结构进行处理。\n层次结构扁平化。（回填，将类目向下虚拟） 层次桥接表。灵活性高但是复杂。 10.4.2 行为维度 种类划分：\n另一个维度的过去行为。 快照事实行为维度。 分组事实行为维度。 复杂逻辑是时行为维度。 两种处理方式：\n将其冗余至现有的维度表中。 加工成单独的维度表。 两个原则：\n避免过快增长。 避免耦合度过高。 10.4.3 多值维度 三种常见的处理方式：\n降低事实表的粒度。 采用多字段。 采用较为通用的桥接表。 10.4.4 多值属性 维表中的某个属性字段同时有多个值成为多值属性。\n三种常见的处理方式：\n保持维度主键不变，将多值属性放在维度的一个属性字段中。（扩展性好，使用麻烦） 保持维度主键不变，将多值属性放在维度的多个属性字段中。 维度主键发生变化，一个维度值存放多条记录。（扩展性好，使用方便，但是数据急剧膨胀） 10.4.5 杂项维度 是由操作型系统种的指示符或者标志字段组合而成的，一般不再一致性维度之列。\n通常的做法是将这些字段建立到一个维表中，在事实表中只保存一个外键。\n第11章 事实表设计 11.1 事实表基础 11.1.1 事实表特性 事实表作为数据仓库维度建模的核心，紧紧围绕业务过程来设计，通过获取描述业务过程的度量来表达业务过程，包含了引用的维度和业务过程有关的度量。\n事实表中一条巨禄所表达的业务细节程度被称为粒度。\n粒度的两种表述方式：\n维度属性组合所表示的细节粒度； 所表示的具体业务含义。 作为度量业务过程的事实，一般位整形或浮点型的十进制数值，有可加性、半可加性和不可加性三种类型。\n可加性是指可以按照与事实表关联的任意维度进行汇总。\n半可加性事实只能按照特定维度汇总。\n比率型事实完全不具备可加性。对于不可加性事实可分解为可加的组件来实现聚集。\n相对维表来说，通常事实表要细长得多，行的增加速度也比维表快很多。\n事实表的三种类型：\n事物事实表：用来描述业务过程，跟踪空间或时间上某点的度量事件，保存的是最原子的数据，也成为原子事实表。 周期快照事实表：依据有规律性的、可预见的时间见过记录事实，时间间隔如每天、每月、每年等。 累积快照事实表：用来表述过程开始和结束之间的关键步骤事件，覆盖过程的整个生命周期，通常具有多个日期字段来记录关键时间点，当过程随着生命周期不断变化时，记录也会随着过程的变化而被修改。 11.1.2 事实表设计原则 尽可能包含所有与业务过程相关的事实 之选择与业务过程相关的事实 分解不可加性事实为客家的组件 在选择维度和事实之前必须先声明粒度 在同一个事实表中不能有多种不同力度的事实 事实的单位要保持一直 对事实的null值要处理 使用退化维度提高事实表的易用性 11.1.3 事实表设计方法 Kimball的四步设计：选择业务过程、声明粒度、确定维度、确定事实。\n改进后的设计方法：\n选择业务过程及确定事实表类型。\n声明粒度。粒度意味着精确定义事实表的没一行所表示的业务含义，粒度传递的是与事实表粒度有关的细节层次。明确粒度能确保事实表中行的意思的理解不会产生混淆，保证所有的事实按照同样的细节层次记录。\n确定维度。选择能够描述清楚业务过程所处的环境的维度信息。\n确定事实。\n冗余维度。提高使用侠侣，降低数据获取的复杂性，减少关联的表数量。\n11.2 事务事实表 11.2.1 设计过程 任何类型的事件都可以被理解为一种事务。\n（1）选择业务过程\n（2）确定粒度\n（3）确定维度\n（4）确定事实\n（5）冗余维度\n11.2.2 单事务事实表 即针对每个业务过程设计一个事实表。可以方便地对每个业务过程进行独立的分析研究。\n11.2.3 多事务事实表 将不同的事实放到同一个事实表中，即对同一个事实包含不同的业务过程。\n进行事实处理的两种方式：\n不同业务过程的事实使用不同的事实字段进行存放。 不同业务过程的事实使用同一个事实字段进行存放，但增加一个业务过程标签。 多事务事实表的选择：\n当不同业务过程的度量比较相似、差异不大时，可以使用同一个字段来表示度量数据。存在的问题：在同一个周期内会存在多条记录。 当不同业务过程的度量差异较大时，可以将不同业务过程度量使用使用不同字段冗余到表中，非当前业务过程则置零表示。存在的问题：度量字段零值较多。 11.2.4 两种事实表对比 业务过程 单事务事实表，一个业务过程建立一个事实表，只反映一个业务过程的事实；\n多事务事实表，在同一个事实表中反映过个业务过程。\n需要相似性和业务源系统，选择是否放到同一个事实表中。\n粒度和维度 当不同业务过程的粒度相同，同时拥有相似的维度时，此时就可以考虑采用多事务事实表。\n如果粒度不同，则必定是不同的事实表。\n事实 单事务事实表在处理事实上比较方便和灵活，仅仅体现同一个业务过程的事实即可 ；\n多事务事实表由于有多个业务过程，所以有很多的事实需要处理。\n下游业务使用 单事务事实表对于下游用户而言更容易理解，关注哪个业务过程就使用响应的事务事实表\n计算存储成本 多事务事实表成本更低。\n11.2.5 父子事实的处理方式 11.2.6 事实的设计准则 事实完整性 事实一致性 事实可加性 11.3 周期快照事实表 在确定的间隔内对尸体的度量进行臭氧，这样可以很容易地研究尸体的度量值，而不需要聚集长期的事务历史。\n11.3.1 特性 快照事实表的粒度通常以维度形式声明；事务事实表是系数的，但快照事实表是稠密的；事务事实表中的事实是完全可加的，但快照模型将至少包含一个用来展示半可加性质的事实。\n用快照采样状态 快照事实表以预定的间隔采样状态度量。这种间隔联合一个或多个维度，将被涌来定义快照事实表的粒度，每行都将包含记录所涉及状态的事实。\n快照粒度 快照事实表的粒度通常总是被多维声明，可以简单地理解无快照需要采样的周期以及什么将被采样。\n密度与稀疏性 快照事实表无论是否有业务过程发生都会记录一行。\n半可加性 快照事实表中收集到的状态度量都是半可加的。\n11.3.2 实例 快照事实表的设计步骤：\n确定快照事实表的快照粒度。 确定快照事实表采样的状态度量。 单维度的每天快照事实表 （1）确定粒度\n（2）确定状态度量\n混合维度的每天快照事实表 混合维度相对于单维度，只是在每天的采样周期上针对多个维度进行采样。\n","permalink":"http://121.199.2.5:6080/cffcc67d768246fd8624b3e5b98854f5/","summary":"序 大、快、多样性只是表象，大数据的真正价值在于生命性和生态性。（活数据）\n第1 章 总述 如果不能对数据进行有序、有结构地分类组织和存储，如果不能有效利用并发掘它，继而产生价值，那么它同时也成为一场“灾难”。无需、无结构的数据犹如堆积如山的垃圾，给企业带来的是有令人咋舌的高额成本。\n要求：\n如何建设高效的数据模型和体系，是数据易用，避免重复建设和数据不一致性； 如何提供高效易用的数据开发工具； 如何做好数据质量保障； 如何有效管理和控制日益增长的存储和计算消耗； 如何保证数据服务的稳定，保证其性能； 如何设计有效的数据产品高效赋能于外部客户和内部员工 数据采集层 日志采集体系方案包括两大体系：Aplus.JS是Web端日志采集技术方案；UserTrack是App端日记采集技术方案。\n在采集技术基础上面向各个场景的埋点规范。\n在传输方面采用TimeTunel（TT）,它既包括数据库的增量数据传输，也包括日志数据的传输；既支持实时流式计算，也知乎此各种时间窗口的批量计算。\n也通过数据同步工具（DataX和同步中心，其中同步中心是给予DataX易用性封装的）直连异构数据库（备库）来抽取各种时间窗口的数据。\n数据计算层 数据只有被整合和计算,才能被用于洞察商业规律,挖掘潜在信息，从而实现大数据价值,达到赋能于商业和创造价值的目的。\n阿里巴巴的数据计算层包括两大体系:数据存储及计算云平台（离线计算平台MaxCompute和实时计算平台StreamCompute）和数据整合及管理体系（内部称之为“ OneData ”） 。\nMaxCompute是阿里巴巴自主研发的离线大数据平台。\nStreamCompute是阿里巴巴自主研发的流式大数据平台。\nOneData是数据整合及管理的方法体系和工具。\n借助此体系，构建了数据公共层。\n从数据计算频率角度来看，阿里数据仓库可以分为离线数据仓库（传统的数据仓库概念）和实时数据仓库（典型应用：双11实时数据）。\n阿里数据仓库的数据加工链路也是遵循业界的分层理念，包括：\n操作数据层（Operational Data Store，ODS）; 明细数据层（Data WareHouse Detail，DWD）； 应用数据层（Application Data Store，ADS）。 通过数据仓库不同层次之间的加工过程实现从数据资产向信息资产的转化，并且对整个过程进行有效的元数据管理及数据质量处理。\n元数据模型整合及应用是一个重要的组成部分，主要包含：\n数据源元数据 数据仓库元数据 数据链路元数据 工具类元数据 数据质量类元数据 元数据应用主要面向数据发现、数据管理等，如用于存储、计算和成本管理。\n数据服务层 当数据已被整合和计算好之后，需要提供给产品和应用进行数据消费。\n针对不同的需求，数据服务层的数据源架构在多种数据库之上，如Mysql和HBase。\n数据服务层主要考虑性能、稳定性、扩展性。\nOneService（数据服务平台）一数据仓库整合计算好的数据作为数据源，对外通过接口的方式提供数据服务，主要提供简单数据查询服务、复杂数据查询服务（用户识别、用户画像等）和实时数据推送服务。\n数据应用层 第1篇 数据技术篇 第2章 日志采集 第2篇 数据模型篇 第8章 大数据领域建模综述 8.1 为什么需要数据建模 如何将数据进行有序、有结构地分类组织和存储？\n数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用角度合理存储数据。有了适合业务和基础数据存储环境的模型，那么大数据就能获得以下好处：\n性能：良好的数据模型能帮助我们快速查询所需要的数据，减少数据的I/O吞吐。 成本：良好的数据模型能极大地减少不必要的数据冗余，也能实现计算结果复用，极大地降低大数据系统中的存储和计算成本。 效率：良好的数据模型能极大地改善用户使用数据的体验，提高使用数据的效率。 质量：良好的数据模型能改善数据统计口径的不一致性，减少数据计算错误的可能性。 8.2 关系行数据库系统和数据仓库 大数据领域仍然使用关系型数据库，使用关系理论描述数据之间的关系，只是基于其数据存储的特点关系数据模型的范式上有了不同的选择。\n8.3 从OLTP和OLAP系统特别看模型方法论的选择 OLTP系统通常面向的主要数据操作是随即读写，主要采用满足3NF的实体关系模型存储数据，从而在事务处理中解决数据的冗余和一致性问题；而OLAP系统面向的主要数据操作时批量读写，事物处理中的一致性不是OLAP所关注的，其主要关注数据的集合，以及在一次性的复杂大数据查询和处理中的性能，因此它需要采用一些不同的数据建模方法。\n8.4 典型的数据仓库建模方法论 8.4.1 ER模型 数据仓库中的3NF和OLPT系统中的3NF的却别在于，它是站在企业角度面向主题的抽象，而不是针对某个具体业务流程的实体对象关系的抽象。其具有以下几个特点：\n需要全面了解企业业务和数据； 事实周期非常长； 对建模人员的能力要求非常高。 采用ER模型建设数据仓库魔性的出发点是整合数据将个系统中的数据以整个企业角度按主题进项相似性组合和合并，并进行一致性处理，为数据分析决策服务，但是并不能直接用于分析决策。\n其建模步骤分为三个阶段：\n高层模型：一个高度抽象的模型，描述主要的主题以及主题间的关系，用语描述企业的业务总体概况。 中层模型：在高层模型的基础上，细化主题的数据项。 物理模型（也叫底层模型）：在中层模型的基础上，考虑物理存储，同时基于性能和平台特点进行物理属性的设计，也可能作一些表的合并、分区的设计等。 实践典型：金融业务FS-LDM。\n8.4.2 维度模型 是数据仓库工程领域最流行的数据仓库建模的经典。\n维度建模从分析决策的需求出发构建模型，为分析需求服务，因此它重点关注用户如何更快速地完成需求分析，同时具有较好的大规模复杂查询的响应性能。其典型的代表是星形模型，以及在一些特殊场景下使用的雪花模型。其设计分为一下几个步骤：\n选择需要进行分析决策的业务过程。业务过程可以是单个业务事件，比如交易的支付、退款等；也可以时某个时间的状态，比如当前的账户余额等；还可以是一系列相关业务时间组成的业务流程，具体需要看我们分析的是某些事件发生情况，还是当前状态，或是事件流转效率； 选择粒度。在事件分析中，我们要预判所有分析需要细分的程度，从而决定选择的粒度。粒度是维度的一个组合； 识别维表。选择好粒度之后，就需要细雨此粒度设计维表，包括维度属性，用于分析时进行分组和筛选； 选择事实。确定分析需要衡量的指标。 8.4.3 Data Vault模型 ER模型的衍生。其设计的出发点也是为了实现数据的整合，但不能直接用于数据分析决策。","title":"序"},{"content":"第1章 Java的I/O演进之路 1.1 I/O基础入门 1.1.1 Linux网络I/O模型简介 UNIX提供了5种I/O模型：\n（1）阻塞I/O模型\n（2）非阻塞I/O模型\n（3）I/O复用模型\n（4）信号驱动I/O模型\n（5）异步I/O\n1.1.2 I/O多路复用技术 把多个I/O阻塞复用到同一个select的阻塞上，从而是的系统在单线程的情况下可以同时处理多个客户端请求。比多线程有性能优势，节约资源。\n支持I/O多路复用的系统调用select/pselect/poll/epoll。\nepoll的优点：\n支持一个进程打开的socket描述符（FD）不受限制（仅受限于操作系统的最大文件句柄数（内存））。 I/O效率不会随着FD数目的增加而线性下降。 使用mmap加速内核与用户空间的消息传递。 epoll的API更加简单。 1.2 Java的I/O演进 历史题，略。\n第2章 NIO入门 2.1 传统的BIO编程 C/S模型，客户端发起连接请求，三次握手，通过Socket进行通信。\n2.1.1 BIO通信模型图 一请求一应答模型：每次接收到连接请求都创建一个新的线程进行链路处理。处理完成后通过输出流返回应答给客户端，线程销毁。\n该模型最大的问题就是：缺乏弹性伸缩能力，当客户端并发访问量增加后，服务端的线程个数和客户端并发访问数呈1：1的正比关系。\n2.1.2 同步阻塞式I/O创建的TimeServer源码分析 import java.io.IOException; import java.net.ServerSocket; import java.net.Socket; /** * \u0026lt;p\u0026gt;Description: 同步阻塞式I/O创建的TimeServer\u0026lt;/p\u0026gt; * * @author 李宏博 * @version xxx * @create 2019/8/14 17:58 */ public class TimeServer { /** * * @param args */ public static void main(String[] args) throws IOException { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } ServerSocket server = null; try { server = new ServerSocket(port); System.out.println(\u0026#34;服务端端口开启：\u0026#34; + port); Socket socket = null; while (true) { socket = server.accept(); new Thread(new TimeServerHandle(socket)).start(); } } finally { if (server != null) { System.out.println(\u0026#34;服务端关闭\u0026#34;); server.close(); server = null; } } } } import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.io.PrintWriter; import java.net.Socket; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version xxx * @create 2019/8/14 18:05 */ public class TimeServerHandle implements Runnable { private Socket socket; public TimeServerHandle(Socket socket) { this.socket = socket; } @Override public void run() { BufferedReader in = null; PrintWriter out = null; try { in = new BufferedReader(new InputStreamReader(this.socket.getInputStream())); out = new PrintWriter(this.socket.getOutputStream(),true); String currentTime = null; String body = null; while (true) { body = in.readLine(); if (body == null) { break; } System.out.println(\u0026#34;接收到命令：\u0026#34; + body); currentTime = \u0026#34;time\u0026#34;.equalsIgnoreCase(body) ? new java.util.Date(System.currentTimeMillis()).toString() : \u0026#34;命令错误\u0026#34;; out.println(currentTime); } } catch (Exception e) { if (in != null) { try { in.close(); } catch (IOException ex) { ex.printStackTrace(); } } if (out != null) { out.close(); out = null; } if (this.socket != null) { try { this.socket.close(); } catch (IOException ex) { ex.printStackTrace(); } finally { this.socket = null; } } } } } 2.1.3 同步阻塞式I/O创建的TimeClient源码分析 import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.io.PrintWriter; import java.net.Socket; import java.net.UnknownHostException; /** * \u0026lt;p\u0026gt;Description: 同步阻塞式I/O创建的TimeClient\u0026lt;/p\u0026gt; * * @author 李宏博 * @version xxx * @create 2019/8/14 18:21 */ public class TimeClient { /** * * @param args */ public static void main(String[] args) { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } Socket socket = null; BufferedReader in = null; PrintWriter out = null; try { socket = new Socket(\u0026#34;127.0.0.1\u0026#34;,port); in = new BufferedReader(new InputStreamReader(socket.getInputStream())); out = new PrintWriter(socket.getOutputStream(),true); out.println(\u0026#34;time\u0026#34;); System.out.println(\u0026#34;命令发送成功\u0026#34;); String resp = in.readLine(); System.out.println(\u0026#34;现在时间：\u0026#34; + resp); } catch (UnknownHostException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } finally { if (out != null) { out.close(); out = null; } if (in != null) { try { in.close(); } catch (IOException e) { e.printStackTrace(); } finally { in = null; } } if (socket != null) { try { socket.close(); } catch (IOException e) { e.printStackTrace(); } finally { socket = null; } } } } } 2.2 伪异步I/O编程 引入线程池。\n2.2.1 伪异步I/O模型图 由于线程池可以设置消息队列的大小和最大线程数，因此，它的资源占用式可控的，无论多少个客户端并发访问，都不会导致资源的耗尽和宕机。\n2.2.2 伪异步I/O创建的TimeServer源码分析 import java.io.IOException; import java.net.ServerSocket; import java.net.Socket; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version xxx * @create 2019/8/14 18:37 */ public class TimeServer2 { /** * * @param args */ public static void main(String[] args) throws IOException { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } ServerSocket server = null; try { server = new ServerSocket(port); System.out.println(\u0026#34;服务端端口开启：\u0026#34; + port); Socket socket = null; TimeServerHandlerExecutePool singleExecutePool = new TimeServerHandlerExecutePool(50,10000); while (true) { socket = server.accept(); singleExecutePool.execute(new TimeServerHandle(socket)); } } finally { if (server != null) { System.out.println(\u0026#34;服务端关闭\u0026#34;); server.close(); server = null; } } } } import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ExecutorService; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; /** * \u0026lt;p\u0026gt;Description: 线程池\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/14 18:40 */ public class TimeServerHandlerExecutePool { private ExecutorService executor; public TimeServerHandlerExecutePool(int maxPoolSize,int queueSize) { executor = new ThreadPoolExecutor(Runtime.getRuntime().availableProcessors(),maxPoolSize,120L, TimeUnit.SECONDS,new ArrayBlockingQueue\u0026lt;Runnable\u0026gt;(queueSize)); } public void execute(Runnable task) { executor.execute(task); } } 由于底层的通信依然采用同步阻塞模型。因此无法从根本上解决问题。\n2.2.3 伪异步I/O弊端分析 操作Socket的输入流时，会一直阻塞，知道出现以下情况：\n有数据可读； 可用数据已经读取完毕； 发生空指针或者I/O异常。 输出流也会阻塞，TCP发送窗口跟接收窗口大小相关，流量控制。如果采用同步I/O会，wirte操作会被无限期阻塞。\n阻塞的时间取决于对方I/O线程的处理速度和网络I/O的传输速度。所以不能依赖对方的处理速度进行I/O。\n通信对方返回应答时间过程会引起的级联故障：\n（1）服务端处理缓慢，返回应答消息耗费60s，平时只需要10ms；\n（2）采用伪异步I/O的线程正在读取故障服务节点的响应，由于读取输入流时阻塞的，它将会被同步阻塞60s；\n（3）假如所有可用线程都被故障服务器阻塞，那后续所有的I/O消息都将在队列中排队。\n（4）由于线程池采用阻塞队列实现，当队列积满之后，后续入队列的操作将被阻塞。\n（5）由于前端只有一个Accptor线程接收客户端接入，它被阻塞在线程池的同步阻塞队列之后，新的客户端请求消息将被拒绝，客户端会发生大量的连接超时。\n（6）由于所有的连接都超时，调用者会认为系统已经崩溃，无法接收新的请求消息。\n2.3 NIO编程 本书使用的NIO都指的是非阻塞I/O。\n一般来说，低负载、低并发的应用程序可以选择同步I/O以降低编程复杂度；对于高负载、高并发的网络应用，需要使用NIO的非阻塞模式进行开发。\n2.3.1 NIO类库简介 缓冲区 Buffer Buffer是一个对象，包含一些要写入或者要读出的数据。在面向流的I/O种，可以将数据直接写入或者将数据直接读到Stream对象中。\n所有读写操作都通过缓冲区进行。\n实际上是一个数组。通常为ByteBuffer。\n通道 Channel 双向。流Stream是单向的。通道可以用于读、写或者二者同时进行。\n多路复用器 Selector Selector会不断地轮询注册在其上的Channel，如果某个Channel上面发生读或者写事件，这个Channe了就处于就绪状态，会被Selector轮询出来，然后通过SelectionKey可以获取就绪Channel的集合，进行后续的I/O操作。\n2.3.2 NIO服务端序列图 步骤一：打开ServerSocketChannel，用于监听客户端的连接，它是所有客户端连接的父管道；\nServerSocketChannel acceptorSvr = ServerSocketChannel.open(); 步骤二：绑定监听端口，设置连接为非阻塞模式；\nacceptorSvr.socket().bind(new InetSocketAddress(InetAddress.getByName(\u0026#34;IP\u0026#34;),port)); acceptorSvr.configureBlocking(false); 步骤三：创建Reactor线程，创建多路复用器并启动线程；\nSelector selector = Selector.open(); New Thread(new ReactorTask()).start(); 步骤四：将ServerSocketChannel注册到Reactor线程的多路复用器Selector上，监听ACCEPT事件；\nSelectionKey key = acceptorSvr.register(selector, SelectionKey.OP_ACCEPT, ioHandler); 步骤五：多路复用器在线程run方法的无限循环体内轮询准备就绪的Key；\nint num = Selector.select(); Set selectedKeys = selector.selectedKeys(); Iterator it = selectedKeys.iterator(); while (it.hasNext()) { SelectionKey key = (SelectionKey) it.next(); } 步骤六：多路复用监听器听到有新的客户端接入，处理新的接入请求，完成TCP三次握手，建立物理链路；\nSocketChannel channel = svrChannel.accept(); 步骤七：设置客户端链路为非阻塞模式；\nchannel.configureBlocking(false); channel.socket().setReuseAddress(true); 步骤八：将新接入的客户端连接注册到Reactor线程的多路复用器上，监听读操作，读取客户端发送的网络消息，示例代码如下；\nSelectionKey key = socketChannel.register(selector, SelectionKey.OP_READ,ioHandler); 步骤九：异步读取客户端请求消息到缓冲区；\nint readNumber = channel.read(receivedBuffer); 步骤十：对ByteBuffer进行编解码，如果有半包消息指针reset，继续读取后续的报文，将节码成功的消息封装成Task，投递到业务线程池中，进行业务逻辑编排；\nObject message = null; while(buffer.hasRemain()) { byteBuffer.mark(); Object message = decode(byteBuffer); if(message == null) { byteBuffer.reset(); break; } messageList.add(message); } if(!byteBuffer.hasRemain()) { byteBuffer.clear(); } else { byteBuffer.compact(); } .......................... 步骤十一：将POJO对象encode成Bytebuffer，调用SocketChannel的异步write接口，将消息异步发送到客户端；\nsocketChannel.write(buffer); 2.3.3 NIO创建的TimeServer源码分析 /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/15 11:40 */ public class TimeServer3 { public static void main(String[] args) { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } MultiplexerTimeServer timeServer = new MultiplexerTimeServer(port); new Thread(timeServer, \u0026#34;NIO-MultiplexerTimeServer-001\u0026#34;).start(); } } import java.io.IOException; import java.net.InetSocketAddress; import java.net.ServerSocket; import java.nio.ByteBuffer; import java.nio.channels.SelectionKey; import java.nio.channels.Selector; import java.nio.channels.ServerSocketChannel; import java.nio.channels.SocketChannel; import java.util.Iterator; import java.util.Set; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/15 11:42 */ public class MultiplexerTimeServer implements Runnable{ private Selector selector; private ServerSocketChannel serverSocketChannel; private volatile boolean stop; public MultiplexerTimeServer(int port) { try { selector = Selector.open(); serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.socket().bind(new InetSocketAddress(port),1024); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); System.out.println(\u0026#34;服务器启动，端口号：\u0026#34;+port); } catch (IOException e) { e.printStackTrace(); System.exit(1); } } public void stop() { this.stop = true; } public void run() { while (!stop) { try { selector.select(1000); Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; it = selectionKeys.iterator(); SelectionKey key = null; while (it.hasNext()) { key = it.next(); it.remove(); try { handleInput(key); } catch (Exception e) { if (key != null) { key.cancel(); if (key.channel() != null) { key.channel().close(); } } } } } catch (Throwable t) { t.printStackTrace(); } } if (selector != null) { try { selector.close(); } catch (IOException e) { e.printStackTrace(); } } } private void handleInput(SelectionKey key) throws IOException { if (key.isValid()) { if (key.isAcceptable()) { ServerSocketChannel ssc = (ServerSocketChannel) key.channel(); SocketChannel sc = ssc.accept(); sc.configureBlocking(false); sc.register(selector,SelectionKey.OP_READ); } if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); ByteBuffer readBuffer = ByteBuffer.allocate(1024); int readBytes = sc.read(readBuffer);//非阻塞 if (readBytes \u0026gt; 0) { readBuffer.flip(); byte[] bytes = new byte[readBuffer.remaining()]; readBuffer.get(bytes); String body = new String(bytes,\u0026#34;UTF-8\u0026#34;); System.out.println(\u0026#34;服务器接收到命令：\u0026#34;+body); String currentTime = \u0026#34;time\u0026#34;.equalsIgnoreCase(body) ? new java.util.Date(System.currentTimeMillis()).toString() : \u0026#34;命令错误\u0026#34;; doWrite(sc,currentTime); } else if (readBytes \u0026lt; 0) {//返回值等于-1，链路已经关闭 key.cancel(); sc.close(); } else {//返回值等于0 } } } } private void doWrite(SocketChannel channel, String response) throws IOException { if (response != null \u0026amp;\u0026amp; response.trim().length() \u0026gt; 0) {//由于SocketChannel是异步非阻塞的，会出现“写半包”问题 byte[] bytes = response.getBytes(); ByteBuffer writeBuffer = ByteBuffer.allocate(bytes.length); writeBuffer.put(bytes); writeBuffer.flip(); channel.write(writeBuffer); } } } 2.3.4 NIO客户端序列图 步骤一：打开SocketChannel，绑定客户端本地地址（可选，默认随机分配）；\nSocketChannel clientChannel = SocketChannel.open(); 步骤二：设置SocketChannel为非阻塞模式，同时设置客户端连接的TCP参数；\nclientChannel.configureBlocking(false); socket.setReuseAddress(true); socket.setReceiveBufferSize(BUFFER_SIZE); socket.setSendBufferSize(BUFFER_SIZE); 步骤三：异步连接服务端；\nboolean connected = clientChannel.connect(new InetSocketAddress(\u0026#34;ip\u0026#34;,port)); 步骤四：判断是否连接成功，如果连接成功，则直接注册读状态位到多路复用器中，如果当前没有连接成功（异步连接，返回false，说明客户端已经发送sync包，服务端没有返回ack包，物理链路还没有建立）；\nif(connectied) { clientChannel.register(selector,Selection.OP_READ,ioHandler); } else { clientChannel.register(selector,Selection.OP_CONNECT,ioHandler); } 步骤五：向Reactor线程的多路复用器注册OP_CONNECT状态位，监听服务端的TCPACK应答；\nclientChannel.register(selector, Selection.OP_CONNECT, ioHandler); 步骤六：创建Reactor线程，创建多路复用器并启动线程；\nSelector seletor = Selector.open(); new Thread(new ReactorTask()).start(); 步骤七：多路复用器在线程run方法的无限循环体内轮询准备就绪的Key；\nint num = selector.select(); Set selectedKeys = selector,selectedKeys(); Irerator it = selectedKeys.iterator(); while (it.hasNext()) { SelectionKey key = (SelectionKey) it.next(); } 步骤八：接收connect事件进行处理；\nif(key.isConnectable()) { .. } 步骤九：判断连接结果，如果连接成功，注册读事件到多路复用器；\nif(channel.finishConnection()) { registerRead(); } 步骤十：注册读事件到多路复用器；\nclientChannel.register(selector, SelectionKey.OP_READ, ioHandler); 步骤十一：异步读客户端请求消息到缓冲区；\nint readNumber = channel.read(receiveBuffer); 步骤十二：对ByteBuffer进行编解码，如果有半包消息接收缓冲区Reset，继续读取后续的报文，将解码成功的消息封装成Task，投递到业务线程池中，进行业务逻辑编排。\n略\n步骤十三：将POKO对象encode成ByteBuffer，调用SocketChannel的异步write接口，将消息异步发送给客户端。\nsocketChannel.write(buffer); 2.3.5 NIO创建的TimeClient源码分析 package NIO; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/15 14:13 */ public class TimeClient { public static void main(String[] args) { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } new Thread(new TimeClientHandle(\u0026#34;127.0.0.1\u0026#34;,port),\u0026#34;TimeClient-001\u0026#34;).start(); } } package NIO; import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.SelectionKey; import java.nio.channels.Selector; import java.nio.channels.SocketChannel; import java.util.Iterator; import java.util.Set; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/15 14:16 */ public class TimeClientHandle implements Runnable { private String host; private int port; private Selector selector; private SocketChannel socketChannel; private volatile boolean stop; public TimeClientHandle(String host, int port) { this.host = host == null ? \u0026#34;127.0.0.1\u0026#34; : host; this.port = port; try { selector = Selector.open(); socketChannel = SocketChannel.open(); socketChannel.configureBlocking(false); } catch (IOException e) { e.printStackTrace(); System.exit(1); } } @Override public void run() { try { doConnect(); } catch (IOException e) { e.printStackTrace(); System.exit(1); } while (!stop) { try { selector.select(1000); Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; it = selectionKeys.iterator(); SelectionKey key = null; while (it.hasNext()) { key = it.next(); it.remove(); try { handleInput(key); } catch (IOException e) { if (key != null) { key.cancel(); if (key.channel() != null) { key.channel().close(); } } } } } catch (Exception e) { e.printStackTrace(); System.exit(1); } } if (selector != null) { try { selector.close(); } catch (IOException e) { e.printStackTrace(); } } } private void handleInput(SelectionKey key) throws IOException { if (key.isValid()) { SocketChannel sc = (SocketChannel) key.channel(); if (key.isConnectable()) { if (sc.finishConnect()) { sc.register(selector, SelectionKey.OP_READ); doWrite(sc); } else { System.exit(1); } } if (key.isReadable()) { ByteBuffer readBuffer = ByteBuffer.allocate(1024); int readBytes = sc.read(readBuffer); if (readBytes \u0026gt; 0) { readBuffer.flip(); byte[] bytes = new byte[readBuffer.remaining()]; readBuffer.get(bytes); String body = new String(bytes,\u0026#34;UTF-8\u0026#34;); System.out.println(\u0026#34;现在时间：\u0026#34; + body); this.stop = true; } else if (readBytes \u0026lt; 0) { key.cancel(); sc.close(); } else {} } } } private void doConnect() throws IOException { if (socketChannel.connect(new InetSocketAddress(host,port))) { socketChannel.register(selector, SelectionKey.OP_READ); doWrite(socketChannel); } else { socketChannel.register(selector,SelectionKey.OP_CONNECT); } } private void doWrite(SocketChannel sc) throws IOException { byte[] req = \u0026#34;Time\u0026#34;.getBytes(); ByteBuffer writeBuffer = ByteBuffer.allocate(req.length); writeBuffer.put(req); writeBuffer.flip(); sc.write(writeBuffer); if (!writeBuffer.hasRemaining()) { System.out.println(\u0026#34;命令发送成功\u0026#34;); } } } NIO编程的难度大，而且还没有考虑”半包读“和”半包写“，加上会更复杂。\nNIO的优点：\n（1）客户端发起的连接操作是异步的，可以通过在多路复用器注册OP_CONNECT等待后续结果，不需要像之前的客户端那样被同步阻塞。\n（2）SocketChannel的读写操作都是异步的，如果没有可读写的数据它不会同步等待，直接返回，这样I/O通信线程就可以处理其他的链路，不需要同步等待这个链路可用。\n（3）线程模型的优化：在linux上的底层是epoll。性能不随客户端的增加先行下降。\n因此适合做高性能，高负载的网络服务器。\n2.4 AIO编程 异步通道通过以下两种方式获取操作结果：\njava.util.concurrent.Future类表示异步操作的结果。 在执行异步操作的时候传入一个java.nio.channels。 CompletionHandle接口的实现类作为操作完成的回调。\n不需要多路复用器（Selector），简化NIO的编程模型。\n2.4.1 AIO创建的TimeServer源码分析 package AIO; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 9:38 */ public class TimeServer { public static void main(String[] args) { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } AsyncTimeServerHandler timeserver = new AsyncTimeServerHandler(port); new Thread(timeserver,\u0026#34;AIO-\u0026#34; + \u0026#34;AsyncTimeServerHandler-001\u0026#34;).start(); } } package AIO; import java.io.IOException; import java.net.InetSocketAddress; import java.nio.channels.AsynchronousServerSocketChannel; import java.util.concurrent.CountDownLatch; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 9:41 */ public class AsyncTimeServerHandler implements Runnable{ private int port; CountDownLatch latch; AsynchronousServerSocketChannel asynchronousServerSocketChannel; public AsyncTimeServerHandler(int port) { this.port = port; try { asynchronousServerSocketChannel = AsynchronousServerSocketChannel.open(); asynchronousServerSocketChannel.bind(new InetSocketAddress(port)); } catch (IOException e) { e.printStackTrace(); } } @Override public void run() { latch = new CountDownLatch(1); doAccept(); try { latch.await(); } catch (InterruptedException e) { e.printStackTrace(); } } private void doAccept() { asynchronousServerSocketChannel.accept(this, new AcceptCompletionHandler()); } } package AIO; import java.nio.ByteBuffer; import java.nio.channels.CompletionHandler; import java.nio.channels.AsynchronousSocketChannel; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 9:54 */ public class AcceptCompletionHandler implements CompletionHandler\u0026lt;AsynchronousSocketChannel,AsyncTimeServerHandler\u0026gt; { @Override public void completed(AsynchronousSocketChannel result, AsyncTimeServerHandler attachment) { attachment.asynchronousServerSocketChannel.accept(attachment,this); ByteBuffer buffer = ByteBuffer.allocate(1024); result.read(buffer, buffer, new ReadCompletionHandler(result)); } @Override public void failed(Throwable exc, AsyncTimeServerHandler attachment) { exc.printStackTrace(); attachment.latch.countDown(); } } package AIO; import java.io.IOException; import java.io.UnsupportedEncodingException; import java.nio.ByteBuffer; import java.nio.channels.AsynchronousSocketChannel; import java.nio.channels.CompletionHandler; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 10:14 */ public class ReadCompletionHandler implements CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt; { private AsynchronousSocketChannel channel; public ReadCompletionHandler(AsynchronousSocketChannel channel) { if (this.channel == null) { this.channel = channel; } } @Override public void completed(Integer result, ByteBuffer attachment) { attachment.flip(); byte[] body = new byte[attachment.remaining()]; attachment.get(body); try { String req = new String(body, \u0026#34;UTF-8\u0026#34;); System.out.println(\u0026#34;服务器接收到命令：\u0026#34; + req); String currentTime = \u0026#34;time\u0026#34;.equalsIgnoreCase(req) ? new java.util.Date(System.currentTimeMillis()).toString() : \u0026#34;命令错误\u0026#34;; doWrite(currentTime); } catch (UnsupportedEncodingException e) { e.printStackTrace(); } } private void doWrite(String currentTime) { if (currentTime != null \u0026amp;\u0026amp; currentTime.trim().length() \u0026gt; 0) { byte[] bytes = (currentTime).getBytes(); ByteBuffer writeBuffer = ByteBuffer.allocate(bytes.length); writeBuffer.put(bytes); writeBuffer.flip(); channel.write(writeBuffer, writeBuffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override public void completed(Integer result, ByteBuffer buffer) { if (buffer.hasRemaining()) channel.write(buffer, buffer, this); } @Override public void failed(Throwable exc, ByteBuffer buffer) { try { channel.close(); } catch (IOException e) { e.printStackTrace(); } } }); } } @Override public void failed(Throwable exc, ByteBuffer attachment) { try { this.channel.close(); } catch (IOException e) { e.printStackTrace(); } } } 2.4.2 AIO创建的TimeClient源码分析 package AIO; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 10:32 */ public class TimeClient { public static void main(String[] args) { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } new Thread(new AsyncTimeClientHandler(\u0026#34;127.0.0.1\u0026#34;,port),\u0026#34;AIO-AsyncTimeClient-001\u0026#34;).start(); } } package AIO; import java.io.IOException; import java.io.UnsupportedEncodingException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.AsynchronousSocketChannel; import java.nio.channels.CompletionHandler; import java.util.concurrent.CountDownLatch; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 10:35 */ public class AsyncTimeClientHandler implements CompletionHandler\u0026lt;Void,AsyncTimeClientHandler\u0026gt;,Runnable { private AsynchronousSocketChannel client; private String host; private int port; private CountDownLatch latch; public AsyncTimeClientHandler(String host, int port) { this.host = host; this.port = port; try { client = AsynchronousSocketChannel.open(); } catch (IOException e) { e.printStackTrace(); } } @Override public void run() { latch = new CountDownLatch(1); client.connect(new InetSocketAddress(host, port), this, this); try { latch.await(); } catch (InterruptedException e) { e.printStackTrace(); } try { client.close(); } catch (IOException e) { e.printStackTrace(); } } @Override public void completed(Void result, AsyncTimeClientHandler attachment) { byte[] req = \u0026#34;time\u0026#34;.getBytes(); ByteBuffer writeBuffer = ByteBuffer.allocate(req.length); writeBuffer.put(req); writeBuffer.flip(); client.write(writeBuffer, writeBuffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override public void completed(Integer result, ByteBuffer buffer) { if (buffer.hasRemaining()) { client.write(buffer, buffer, this); } else { ByteBuffer readBuffer = ByteBuffer.allocate(1024); client.read(readBuffer, readBuffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override public void completed(Integer result, ByteBuffer buffer) { buffer.flip(); byte[] bytes = new byte[buffer.remaining()]; buffer.get(bytes); String body; try { body = new String(bytes,\u0026#34;UTF-8\u0026#34;); System.out.println(\u0026#34;现在时间：\u0026#34; + body); latch.countDown(); } catch (UnsupportedEncodingException e) { e.printStackTrace(); } } @Override public void failed(Throwable exc, ByteBuffer buffer) { try { client.close(); } catch (IOException e) { e.printStackTrace(); } } }); } } @Override public void failed(Throwable exc, ByteBuffer buffer) { try { client.close(); } catch (IOException e) { e.printStackTrace(); } } }); } @Override public void failed(Throwable exc, AsyncTimeClientHandler attachment) { exc.printStackTrace(); try { client.close(); latch.countDown(); } catch (IOException e) { e.printStackTrace(); } } } 2.5 4种I/O的对比 2.5.1 概念澄清 异步非阻塞I/O JDK1.4开始提供的NIO严格按照网络编程模型和JDK实现来分类，实际上只能被称为非阻塞I/O不能叫异步非阻塞I/O。1.5用epoll替换了select/poll，只是底层性能的优化，并没有替换I/O模型。\nJDK1.7提供的NIO2.0是真正的异步I/O。\n但习惯上NIO相对于IO还是被称为异步非阻塞IO或非阻塞I/O。\n多路复用器Selector\n伪异步I/O\n完全来源于实践，加入一个缓冲区（线程池）。不是官方概念。\n2.5.2 不同I/O模型的对比 同步阻塞I/O（BIO） 伪异步I/O 非阻塞I/O（NIO） 异步I/O（AIO） 客户端个数：I/O线程 1：1 M:N（其中M可以大于N） M：1（1个I/O线程除了多个客户端连接） M：0（不需要启动额外的I/O线程，被动回调） I/O类型（阻塞） 阻塞I/O 阻塞I/O 非阻塞I/O 非阻塞I/O I/O类型（同步） 同步I/O 同步I/O 同步I/O（I/O多路复用） 异步I/O API使用难度 简单 简单 非常复杂 复杂 可靠性 非常差 差 高 高 吞吐量 低 中 高 高 2.6 选择Netty的理由 2.6.1 不选择Java原生NIO的原因 （1）NIO的类库和API繁杂，使用麻烦，需要熟练掌握Selector、ServerSocketChannel、SocketChannel、ByteBuffer等；\n（2）涉及到Reactor模式，必须非常熟悉多线程和网络编程；\n（3）可靠性补齐的工作量和难度大；\n（4）BUG，epoll bug。\n2.6.2 为什么选择Netty 优点如下：\nAPI使用简单，开发门槛低。 功能强大，预置了多种编解码功能，支持多种主流协议。 。。。 2.7 总结 第3章 Netty入门应用 3.1 Netty开发环境的搭建 注意Netty版本号为 5.0.0.Alpha1。\n3.2 Netty 服务端开发 NIO开发步骤回顾：\n（1）创建ServerSocketChannel，并配置为非阻塞；\n（2）绑定监听，配置TCP参数，例如backlog大小；\n（3）创建一个独立的I/O线程，用于轮询多路复用器Selector；\n（4）创建Selector，将之前的ServerSocketChannel注册在Selector上，监听SelectionKey.ACCEPT；\n（5）启动I/O线程，在循环体中执行Selector.select()方法，轮询就绪的Channel；\n（6）轮循处于就绪状态的Channel，对其判断，如果是OP_ACCEPT，说明是新的客户端接入，则调用ServerSocketChannel.accept()方法接受新的客户端；\n（7）设置新接入的客户端链路SocketChannel为非阻塞模式，配置其他的一些TCP参数；\n（8）将SocketChannel注册到Selector，监听OP_READ操作位；\n（9）监听到后读取；\n（10）如果是OP_WRITE，则继续发送。\npackage Time; import io.netty.bootstrap.ServerBootstrap; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelInitializer; import io.netty.channel.ChannelOption; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioServerSocketChannel; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 15:05 */ public class TimeServer { public void bind(int port) throws Exception { //创建两个线程组的原因：一个用于服务端接受客户端的连接，另一个用于进行SocketChannel的网络读写 EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { //Netty用于启动NIO服务端的辅助启动类，目的是降低服务端的开发复杂度 ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup,workerGroup)//传入线程组 .channel(NioServerSocketChannel.class)//设置Channel，对应NIO中的ServerSocketChannel .option(ChannelOption.SO_BACKLOG, 1024)//配置TCP参数，设置backlog为1024 .childHandler(new ChildChannelHandler()); ChannelFuture f = b.bind(port).sync();//绑定并阻塞，完成后取消阻塞，返回一个ChannelFuture，类似于JDK的Future f.channel().closeFuture().sync();//阻塞，等待服务端链路关闭之后才退出main函数 } finally { //优雅退出 bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } private class ChildChannelHandler extends ChannelInitializer\u0026lt;SocketChannel\u0026gt; { @Override protected void initChannel(SocketChannel arg0) throws Exception { arg0.pipeline().addLast(new TimeServerHandle()); } } public static void main(String[] args) throws Exception { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } new TimeServer().bind(port); } } package Time; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; import java.io.UnsupportedEncodingException; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 15:28 */ public class TimeServerHandler extends ChannelHandlerAdapter { public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ByteBuf buf = (ByteBuf) msg;//类似ByteBuffer，更强大灵活 byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, \u0026#34;UTF-8\u0026#34;); String currentTime = \u0026#34;time\u0026#34;.equalsIgnoreCase(body) ? new java.util.Date(System.currentTimeMillis()).toString() : \u0026#34;命令错误\u0026#34;; ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); } public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush();//从性能角度考虑，为避免频繁唤醒Selector进行消息发送，Netty不直接写，而是先放到缓冲数组，再调用flush写入。 } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { ctx.close();//发生异常时释放。 } } 3.3 Netty客户端开发 package Time; import io.netty.bootstrap.Bootstrap; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelInitializer; import io.netty.channel.ChannelOption; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioSocketChannel; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 15:47 */ public class TimeClient { public void connect(int port, String host) throws Exception { EventLoopGroup group = new NioEventLoopGroup(); try { Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY,true) .handler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new TimeClientHandler());//将ClientHandler设置到ChannelPipeline中，用于处理网络I/O事件 } }); ChannelFuture f = b.connect(host, port).sync();//connect异步连接，sync同步等待连接 f.channel().closeFuture().sync(); } finally { group.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } new TimeClient().connect(port, \u0026#34;127.0.0.1\u0026#34;); } } package Time; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandler; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; import io.netty.util.concurrent.EventExecutorGroup; import java.io.UnsupportedEncodingException; import java.util.logging.Logger; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 15:59 */ public class TimeClientHandler extends ChannelHandlerAdapter { private static final Logger logger = Logger.getLogger(TimeClientHandler.class.getName()); private final ByteBuf firstMessage; public TimeClientHandler() { byte[] req = \u0026#34;time\u0026#34;.getBytes(); firstMessage = Unpooled.buffer(req.length); firstMessage.writeBytes(req); } @Override public void channelActive(ChannelHandlerContext ctx) { ctx.writeAndFlush(firstMessage); } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, \u0026#34;UTF-8\u0026#34;); System.out.println(\u0026#34;现在时间：\u0026#34; + body); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { logger.warning(\u0026#34;出错：\u0026#34; + cause.getMessage()); ctx.close(); } } 3.4 运行和调试 略\n3.5 总结 第4章 TCP粘包/拆包问题的解决之道 4.1 TCP粘包/拆包 TCP不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分。\n4.1.1 TCP粘包/拆包问题说明 4种情况（假设客户端发包）：\n（1）服务端收到两个独立数据包，无粘包/拆包；\n（2）服务端收到两个粘合在一起的包，粘包；\n（3）服务端分两次读取到一个包，拆包；\n（4）同3；\n（5）接收窗口过小，出现多次拆包；\n4.1.2 TCP粘包/拆包发生的原因 3个原因：\n（1）应用程序write写入的字节大小大于套接口发送大小；\n（2）进行MSS（最大报文长度）大小的TCP分段；\n（3）以太网帧的payload大于MTU进行IP分片；\n4.1.3 粘包问题的解决策略 由于底层的TCP不能理解业务数据，只能通过上层的应用协议栈设计来解决，4个策略：\n（1）消息定长（例如固定每个报文段的大小为固定长度，不够则补空格）；\n（2）在包尾增加回车换行符进行分割，例如FTP协议；\n（3）消息头消息体，消息头中设立一个总长度字段；\n（4）更复杂的应用层协议。\n4.2 未考虑TCP粘包导致功能异常案例 4.2.1 TimeServer的改造 package _4_; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/16 15:28 */ public class TimeServerHandler extends ChannelHandlerAdapter { private int counter; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ByteBuf buf = (ByteBuf) msg;//类似ByteBuffer，更强大灵活 byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, \u0026#34;UTF-8\u0026#34;).substring(0, req.length - System.getProperty(\u0026#34;line.separator\u0026#34;).length()); System.out.println(\u0026#34;服务端接收到命令：\u0026#34; + body + \u0026#34;; 计数：\u0026#34; + ++counter); String currentTime = \u0026#34;time\u0026#34;.equalsIgnoreCase(body) ? new java.util.Date(System.currentTimeMillis()).toString() : \u0026#34;命令错误\u0026#34;; ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.writeAndFlush(resp); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { ctx.close();//发生异常时释放。 } } 4.2.2 TimeClient的改造 package _4_; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; import java.util.logging.Logger; /** * */ public class TimeClientHandler extends ChannelHandlerAdapter { private static final Logger logger = Logger.getLogger(TimeClientHandler.class.getName()); private int counter; private byte[] req; public TimeClientHandler() { req = (\u0026#34;time\u0026#34; + System.getProperty(\u0026#34;line.separator\u0026#34;)).getBytes(); } @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { ByteBuf message = null; for (int i = 0; i \u0026lt; 100; i++) {//建立连接后循环发送100条消息，保证每条都被写入到Channel message = Unpooled.buffer(req.length); message.writeBytes(req); ctx.writeAndFlush(message); } } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, \u0026#34;UTF-8\u0026#34;); System.out.println(\u0026#34;现在时间：\u0026#34; + body + \u0026#34;; 计数：\u0026#34; + ++counter); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { logger.warning(\u0026#34;出错：\u0026#34; + cause.getMessage()); ctx.close(); } } 4.2.3 运行结果 出现TCP粘包\n4.3 利用LineBasedFrameDecoder解决TCP粘包问题 代码不贴了，就是将自己在ByteBuf中的操作省去，然后加入两个解码器LineBasedFrameDecoder和StringDecoder。\n4.3.3 运行TCP粘包的时间服务器程序 运行成功，符合预期。成功解决了TCP粘包导致的读半包问题。\n4.3.4 LineBasedFrameDecoder和StringDecoder的原理分析 LineBasedFrameDecoder的工作原理：依次遍历，有“\\n”或者“\\r\\n”则结束。以换行符为结束标志的解码器，支持携带结束符或者不携带结束符两种解码方式，同时支持配置单行的最大长度。如果连续读取到最大长度后仍然没有发现换行符，就会抛出异常，同时忽略掉之前督导的异常码流。\nStringDecoder的功能非常简单，就是将接收到的对象转换成字符串，然后继续调用后面的Handler。\nLineBasedFrameDecoder+StringDecoder组合就是按行切换的文本解码器，被设计用来支持TCP的粘包和拆包。\n4.4 总结 第5章 分隔符和定长解码器的应用 区分消息的四种方式：\n（1）消息定长\n（2）回车换行符作为消息结束符\n（3）特殊符号作为结束标志，回车为一种特殊实现\n（4）通常在消息头中定义长度字段来标识消息的总长度\n5.1 DelimiterBasedFrameDecoder应用开发 以分隔符作为码流结束标识的消息的解码。以“$_”作为分隔符。\n5.1.3 DelimiterBasedFrameDecoder服务端开发 package _5_1_; import io.netty.bootstrap.ServerBootstrap; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelInitializer; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioServerSocketChannel; import io.netty.handler.codec.DelimiterBasedFrameDecoder; import io.netty.handler.codec.string.StringDecoder; import io.netty.handler.logging.LogLevel; import io.netty.handler.logging.LoggingHandler; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 11:03 */ public class EchoServer { public void bind(int port) throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ByteBuf delimiter = Unpooled.copiedBuffer(\u0026#34;$_\u0026#34;.getBytes());//创建分隔符缓冲对象，以$_为分隔符。 ch.pipeline().addLast(new DelimiterBasedFrameDecoder(1024,delimiter));//设置单条消息最大长度，加入分隔符对象 ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new EchoServerHandler()); } }); ChannelFuture f = b.bind(port).sync(); f.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } new EchoServer().bind(port); } } package _5_1_; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandler; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; import io.netty.channel.ChannelHandlerInvoker; import io.netty.util.concurrent.EventExecutorGroup; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 11:12 */ public class EchoServerHandler extends ChannelHandlerAdapter { int counter = 0; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { String body = (String) msg; System.out.println(\u0026#34;第\u0026#34; + ++counter +\u0026#34;次接收来自客户端的消息：[\u0026#34;+body+\u0026#34;]\u0026#34;);//打印接收到的消息 body += \u0026#34;$_\u0026#34;; ByteBuf echo = Unpooled.copiedBuffer(body.getBytes()); ctx.writeAndFlush(echo); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); } } 5.1.2 DelimiterBasedFrameDecoder客户端开发 package _5_1_; import io.netty.bootstrap.Bootstrap; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelInitializer; import io.netty.channel.ChannelOption; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioSocketChannel; import io.netty.handler.codec.DelimiterBasedFrameDecoder; import io.netty.handler.codec.string.StringDecoder; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 11:18 */ public class EchoClient { public void connect(int port, String host) throws Exception { EventLoopGroup group = new NioEventLoopGroup(); try { Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY,true) .handler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ByteBuf delimiter = Unpooled.copiedBuffer(\u0026#34;$_\u0026#34;.getBytes()); ch.pipeline().addLast(new DelimiterBasedFrameDecoder(1024,delimiter)); ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new EchoClientHandler()); } }); ChannelFuture f = b.connect(host, port).sync(); f.channel().closeFuture().sync(); } finally { group.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } new EchoClient().connect(port, \u0026#34;127.0.0.1\u0026#34;); } } package _5_1_; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 11:27 */ public class EchoClientHandler extends ChannelHandlerAdapter { private int counter; static final String ECHO_REQ = \u0026#34;hello,world.$_\u0026#34;; public EchoClientHandler() { } @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { for (int i = 0; i \u0026lt; 10; i++) { ctx.writeAndFlush(Unpooled.copiedBuffer(ECHO_REQ.getBytes())); } } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { System.out.println(\u0026#34;第\u0026#34; + ++counter +\u0026#34;次接收来自服务端的返回：[\u0026#34;+msg+\u0026#34;]\u0026#34;); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush(); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); } } 5.2 FixedLengthFrameDecoder应用开发 FixedLengthFrameDecoder固定长度解码器。\n5.2.1 FixedLengthFrameDecoder服务端开发 利用FixedLengthFrameDecoder解码器，无论一次接收到多少数据报，它都会按照构造函数中设置的固定长度进行解码，如果是半包消息，FixedLengthFrameDecoder会缓存半包消息并等待下个包到达后进行拼包，知道读取到一个完整的包。\npackage _5_2_; import io.netty.bootstrap.ServerBootstrap; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelInitializer; import io.netty.channel.ChannelOption; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioServerSocketChannel; import io.netty.handler.codec.DelimiterBasedFrameDecoder; import io.netty.handler.codec.FixedLengthFrameDecoder; import io.netty.handler.codec.string.StringDecoder; import io.netty.handler.logging.LogLevel; import io.netty.handler.logging.LoggingHandler; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 11:03 */ public class EchoServer { public void bind(int port) throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 100)//加入一个属性 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new FixedLengthFrameDecoder(20));//设置单条消息最大长度，加入分隔符对象 ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new EchoServerHandler()); } }); ChannelFuture f = b.bind(port).sync(); f.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } new EchoServer().bind(port); } } package _5_2_; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 11:12 */ public class EchoServerHandler extends ChannelHandlerAdapter { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { System.out.println(\u0026#34;接收到来自客户端的消息：\u0026#34;+\u0026#34;[\u0026#34;+msg+\u0026#34;]\u0026#34;); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); } } 5.2.2 利用telnet命令行测试EchoServer服务端 telnet命令相关操作此处不赘述。\n以20字节长度进行截取。\n5.3 总结 DelimiterBasedFrameDecoder用于对使用分隔符结尾的消息进行自动解码。 FixLengthFrameDecoder用于对固定长度的消息进行自动解码。 第6章 编解码技术 Java序列化的目的：\n网络传输 对象持久化 Java对象编解码技术：在进行远程跨进程服务调用时，需要把被传输的Java对象编码为字节数组或者ByteBuffer对象。而当远程服务读取到ByteBuffer对象或者字节数组时，需要将其解码为发送时的Java对象。\nJava序列化仅仅是Java编解码技术的一种，有缺陷。\n6.1 Java序列化的缺点 RPC很少使用Java序列化进行消息的编解码和传输。原因如下：\n6.1.1 无法跨语言 Java序列化技术时Java语言内部的私有协议，其他语言并不支持，对于用户来说完全是黑盒。对于Java序列化后的字节数组，别的语言无法进行反序列化。\n6.1.2 序列化后的码流太大 序列化测试：\npackage _6_1_2_; import java.io.Serializable; import java.nio.ByteBuffer; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 12:52 */ public class UserInfo implements Serializable { private static final long serialVersionID = 1L; private String userName; private int userID; public UserInfo(String userName, int userID) { this.userName = userName; this.userID = userID; } public String getUserName() { return userName; } public void setUserName(String userName) { this.userName = userName; } public int getUserID() { return userID; } public void setUserID(int userID) { this.userID = userID; } public byte[] codeC() { ByteBuffer buffer = ByteBuffer.allocate(1024); byte[] value = this.userName.getBytes(); buffer.putInt(value.length); buffer.put(value); buffer.putInt(this.userID); buffer.flip(); value = null; byte[] result = new byte[buffer.remaining()]; buffer.get(result); return result; } } package _6_1_2_; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.ObjectOutputStream; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 12:59 */ public class TestUserInfo { public static void main(String[] args) throws IOException { UserInfo info = new UserInfo(\u0026#34;Welcome to Netty\u0026#34;, 100); ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream os = new ObjectOutputStream(bos); os.writeObject(info); os.flush(); os.close(); byte[] b = bos.toByteArray(); System.out.println(\u0026#34;JDK序列化长度：\u0026#34; + b.length); bos.close(); System.out.println(\u0026#34;-------------------------\u0026#34;); System.out.println(\u0026#34;byte数组序列化长度：\u0026#34; + info.codeC().length); } } 测试结果：JDK序列化机制编码后的二进制数组大小是二进制编码的5倍多。\n评判编解码框架的优劣时，考虑如下因素：\n是否支持跨语言，支持的语言种类是否丰富； 编码后的码流大小； 编解码的性能； 类库是否小巧，API使用是否方便； 使用者需要手工开发的工作量和难度。 编码后的字节数组大，存储时占用空间大，硬件成本大，网络传输时更占用带宽，导致系统的吞吐量降低。\n6.1.3 序列化性能太低 性能测试：\npackage _6_1_3_; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.io.ObjectOutputStream; import java.nio.ByteBuffer; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 13:43 */ public class PerformTestUserInfo { public static void main(String[] args) throws IOException { UserInfo info = new UserInfo(\u0026#34;Welcome to Netty\u0026#34;, 100); int loop = 1000000; ByteArrayOutputStream bos = null; ObjectOutputStream os = null; long startTime = System.currentTimeMillis(); for (int i = 0; i \u0026lt; loop; i++) { bos = new ByteArrayOutputStream(); os = new ObjectOutputStream(bos); os.flush(); os.close(); byte[] b = bos.toByteArray(); bos.close(); } long endTime = System.currentTimeMillis(); System.out.println(\u0026#34;JDK序列化用时：\u0026#34;+(endTime - startTime)+\u0026#34; ms\u0026#34;); System.out.println(\u0026#34;=========================================\u0026#34;); ByteBuffer buffer = ByteBuffer.allocate(1024); startTime = System.currentTimeMillis(); for (int i = 0; i \u0026lt; loop; i++) { byte[] b = info.codeC(buffer); } endTime = System.currentTimeMillis(); System.out.println(\u0026#34;byte数组序列化用时：\u0026#34;+(endTime - startTime)+\u0026#34; ms\u0026#34;); } } Java序列化性能只有二进制编码的6%左右，性能很低。\n6.2 业界主流的编解码框架 略\n6.3 总结 第7章 MessagePack编解码 MessagePack是一个高效的二进制序列化框架。像JSON一样支持不同语言间的数据交换，但是性能更快，序列化之后的码流也更小。\n7.1 MessagePack介绍 特点如下：\n编解码高效，性能高； 序列化之后的码流小； 支持跨语言。 7.1.1 MessagePack多语言支持 几乎都支持，不例举。\n7.1.2 MessagePack Java API介绍 略\n7.2 MessagePack编码器和解码器开发 Netty的编解码框架可以非常方便的继承第三方序列化框架。\n7.2.1 MessagePack编码器开发 package _7_2_1_; import io.netty.buffer.ByteBuf; import io.netty.channel.ChannelHandlerContext; import io.netty.handler.codec.MessageToByteEncoder; import org.msgpack.MessagePack; import java.util.List; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 14:05 */ public class MsgpackEncoder extends MessageToByteEncoder\u0026lt;Object\u0026gt; { @Override protected void encode(ChannelHandlerContext atg0, Object arg1, ByteBuf arg2) throws Exception { MessagePack msgpack = new MessagePack(); byte[] raw = msgpack.write(arg1); arg2.writeBytes(raw); } } MsgpackEncoder继承MessageToByteEncoder，负责将Object类型的POJO对象编码为byte数组，然后写入到ByteBuf中。\n7.2.2 MessagePack解码器开发 package _7_2_1_; import io.netty.buffer.ByteBuf; import io.netty.channel.ChannelHandlerContext; import io.netty.handler.codec.MessageToMessageDecoder; import org.msgpack.MessagePack; import java.util.List; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/19 14:39 */ public class MsgpackDecoder extends MessageToMessageDecoder\u0026lt;ByteBuf\u0026gt; { @Override protected void decode(ChannelHandlerContext arg0, ByteBuf arg1, List\u0026lt;Object\u0026gt; arg2) throws Exception { final byte[] array; final int length = arg1.readableBytes(); array = new byte[length]; arg1.getBytes(arg1.readerIndex(), array, 0, length); MessagePack msgpack = new MessagePack(); arg2.add(msgpack.read(array));//解码 } } 7.2.3 功能测试 暂未通过；\n第8章 Google Protobuf编解码 略\n第9章 JBoss Marshalling编解码 第10章 HTTP协议开发应用 HTTP是一个属于应用层的面向对象的协议。Netty的HTTP协议也是异步非阻塞的。\n10.1 HTTP协议介绍 主要特点：\n支持C/S模式； 简单——客户端向服务端请求服务，秩序指定服务URL，携带必要的请求参数或消息体； 灵活——HTTP允许传输任意类型的数据对象，传输的内容类型由HTTP消息头中的Content-Type加以标记； 无状态——对事务处理没有记忆能力。 10.1.1 HTTP协议的URL http://host[\u0026#34;:\u0026#34;port][abs_path] 详细介绍不赘述。\n10.1.2 HTTP请求消息（HttpRequest） 三部分：\nHTTP请求行； HTTP消息头； HTTP请求正文。 GET和POST的区别：\n（1）根据HTTP规范，GET用于信息获取，应该是安全的和幂等的；POST则表示可能改变服务器上的资源的请求。\n（2）GET提交，请求的数据会附在URL之后，就是把数据放置在请求行中，以“?”分隔URL和传输数据，多个参数用“\u0026amp;”连接；而POST提交会把提交的数据放置再HTTP消息的包体中，数据不会在地址栏中显示出来。\n（3）传输数据的大小不同，GET受具体浏览器长度的限制。POST理论上长度不受限制。\n（4）安全性。\n10.1.3 HTTP响应消息（HttpResponse） 三部分：\n状态行； 消息报头； 响应正文。 10.2 Netty HTTP服务端入门开发 相比于传统的Tomcat、Jetty等Web容器，它更加轻量和小巧，灵活性和定制性也更好。\n10.2.1 HTTP服务端例程场景描述 例程场景如下：文件服务器使用HTTP协议对外提供服务，当客户端通过浏览器访问文件服务器时，对访问路径进行检查，检查失败时返回HTTP403错误，该页无法访问；如果校验通过，以连接的方式打开当前文件目录，每个目录或者文件都是个超链接，可以递归访问。\n如果是目录，可以继续递归访问它下面的子目录或者文件，如果时文件且可读，则可以在浏览器端直接打开，或者通过【目标另存为】下载该文件。\n10.2.2 HTTP服务端开发 package HTTP; import io.netty.bootstrap.ServerBootstrap; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelInitializer; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioServerSocketChannel; import io.netty.handler.codec.http.HttpObjectAggregator; import io.netty.handler.codec.http.HttpRequestDecoder; import io.netty.handler.codec.http.HttpResponseEncoder; import io.netty.handler.stream.ChunkedWriteHandler; import java.net.InetAddress; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/21 10:44 */ public class HttpFileServer { private static final String DEFAULT_URL = \u0026#34;/src\u0026#34;; public void run(final int port, final String url) throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(\u0026#34;http-decoder\u0026#34;, new HttpRequestDecoder());//向ChannelPipeLine中添加HTTP请求消息解码器 ch.pipeline().addLast(\u0026#34;http-aggregator\u0026#34;, new HttpObjectAggregator(65536));//添加HttpObjectAggregator解码器 ch.pipeline().addLast(\u0026#34;http-encoder\u0026#34;, new HttpResponseEncoder());//HTTP响应编码器 ch.pipeline().addLast(\u0026#34;http-chunked\u0026#34;, new ChunkedWriteHandler());//支持异步发送大的码流（例如大文件传输，但不占用过多内存，防止Java内存溢出错误） ch.pipeline().addLast(\u0026#34;fileServerHandler\u0026#34;, new HttpFileServerHandler(url));// } }); ChannelFuture f = b.bind(InetAddress.getLocalHost().getHostAddress(), port).sync(); System.out.println(\u0026#34;Http文件目录服务器启动，网址是：\u0026#34; + InetAddress.getLocalHost().getHostAddress() +\u0026#34;:\u0026#34; + port + url); f.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception{ int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } String url = DEFAULT_URL; if (args.length \u0026gt; 1) url = args[1]; new HttpFileServer().run(port, url); } } package HTTP; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.*; import io.netty.handler.codec.http.*; import io.netty.handler.stream.ChunkedFile; import javax.activation.MimetypesFileTypeMap; import java.io.File; import java.io.FileNotFoundException; import java.io.RandomAccessFile; import java.io.UnsupportedEncodingException; import java.net.URLDecoder; import java.util.regex.Pattern; import static io.netty.handler.codec.http.HttpHeaders.Names.*; import static io.netty.handler.codec.http.HttpHeaders.isKeepAlive; import static io.netty.handler.codec.http.HttpHeaders.setContentLength; import static io.netty.handler.codec.http.HttpMethod.GET; import static io.netty.handler.codec.http.HttpResponseStatus.*; import static io.netty.handler.codec.http.HttpVersion.HTTP_1_1; import static io.netty.util.CharsetUtil.UTF_8; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/21 11:02 */ public class HttpFileServerHandler extends SimpleChannelInboundHandler\u0026lt;FullHttpRequest\u0026gt; { private final String url; public HttpFileServerHandler(String url) { this.url = url; } @Override protected void messageReceived(ChannelHandlerContext ctx, FullHttpRequest request) throws Exception { //对Http请求消息的解码结果进行判断，如果解码失败，直接构造HTTP400错误返回。 if (!request.getDecoderResult().isSuccess()) { sendError(ctx, BAD_REQUEST); return; } //对请求行中的方法进行判断，如果不是从浏览器或者表单设置为GET发起的请求（例如POST），则构造HTTP405错误返回。 if (request.getMethod() != GET) { sendError(ctx, METHOD_NOT_ALLOWED); return; } final String uri = request.getUri(); //对URL进行包装 final String path = sanitizeUri(uri); //如果构造的URI不合法，则返回403错误 if (path == null) { sendError(ctx, FORBIDDEN); return; } //使用新组装的URI路径构造File对象。 File file = new File(path); //如果文件不存在或是系统隐藏文件，则构造Http404异常返回 if (file.isHidden() || !file.exists()) { sendError(ctx, NOT_FOUND); return; } //如果文件是目录，则发送目录的链接给客户端连接 if (file.isDirectory()) { if (uri.endsWith(\u0026#34;/\u0026#34;)) { sendListing(ctx, file); } else { sendRedirect(ctx, uri + \u0026#34;/\u0026#34;); } return; } //点击或下载文件，校验文件合法性。 if (!file.isFile()) { sendError(ctx, FORBIDDEN); return; } //使用随机文件读写类以制度的方式打开文件，如果文件打开失败，则返回404错误。 RandomAccessFile randomAccessFile = null; try { randomAccessFile = new RandomAccessFile(file, \u0026#34;r\u0026#34;);//以只读方式打开文件 } catch (FileNotFoundException fnfe) { sendError(ctx, NOT_FOUND); return; } //获取文件的长度，构造成功的Http应答信息 long fileLength = randomAccessFile.length(); HttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, OK); setContentLength(response, fileLength); setContentTypeHeader(response, file); //判断连接是否KeepAlive，如果是，则设置 if (isKeepAlive(request)) { response.headers().set(CONNECTION, HttpHeaders.Values.KEEP_ALIVE); } ctx.write(response); ChannelFuture sendFileFuture; //通过Netty的ChunkedFile对象直接将文件写入到发送缓冲区。 sendFileFuture = ctx.write(new ChunkedFile(randomAccessFile, 0, fileLength, 8192), ctx.newProgressivePromise()); sendFileFuture.addListener(new ChannelProgressiveFutureListener() { @Override public void operationProgressed(ChannelProgressiveFuture future, long progress, long total) throws Exception { if (total \u0026lt; 0) System.err.println(\u0026#34;传输出错：\u0026#34; + progress); else System.err.println(\u0026#34;传输进度：\u0026#34; + progress+\u0026#34;/\u0026#34;+total); } @Override public void operationComplete(ChannelProgressiveFuture future) throws Exception { System.out.println(\u0026#34;传输完成。\u0026#34;); } }); //使用chunked编码，最后需要发送一个编码结束的空消息体，将LastHttpContent.EMPTY_LAST_CONTENT发送到缓冲区，标识所有消息体都发送完成。 ChannelFuture lastContentFuture = ctx.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT); if (!isKeepAlive(request)) { lastContentFuture.addListener(ChannelFutureListener.CLOSE); } } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); if (ctx.channel().isActive()) { sendError(ctx, INTERNAL_SERVER_ERROR); } } private static final Pattern INSECURE_URI = Pattern.compile(\u0026#34;.*[\u0026lt;\u0026gt;\u0026amp;\\\u0026#34;].*\u0026#34;); private String sanitizeUri(String uri) { try { //使用java.net.URLDecoder对URL进行解码，使用UTF-8字符集。 uri = URLDecoder.decode(uri, \u0026#34;UTF-8\u0026#34;); } catch (UnsupportedEncodingException e) { try { uri = URLDecoder.decode(uri, \u0026#34;ISO-8859-1\u0026#34;); } catch (UnsupportedEncodingException ex) { throw new Error(); } } //对URI进行合法性判断，如果URI与允许访问的URI一致或者是其子目录（文件），则校验通过，否则返回空。 if (!uri.startsWith(url)) return null; if (!uri.startsWith(\u0026#34;/\u0026#34;)) return null; //将硬编码的文件路径替换为本地操作系统的文件路径分隔符。 uri = uri.replace(\u0026#39;/\u0026#39;, File.separatorChar); //对新的URI进行二次合法性校验。 if(uri.contains(File.separator + \u0026#39;.\u0026#39;) || uri.contains(\u0026#39;.\u0026#39; + File.separator) || uri.startsWith(\u0026#34;.\u0026#34;) || uri.endsWith(\u0026#34;.\u0026#34;) || INSECURE_URI.matcher(uri).matches()){ return null; } //校验完成，使用当前运行程序所在的工作目录+URI构造绝对路径返回。 return System.getProperty(\u0026#34;user.dir\u0026#34;) + File.separator + uri; } private static final Pattern ALLOWED_FILE_NAME = Pattern.compile(\u0026#34;[A-Za-z0-9][-_A-Za-z0-9\\\\.]*\u0026#34;); private void sendListing(ChannelHandlerContext ctx, File dir) { //创建成功的Http响应消息，并设置消息头 FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, OK); response.headers().set(CONTENT_TYPE, \u0026#34;text/html;charset=UTF-8\u0026#34;); //消息体 String dirPath = dir.getPath(); StringBuilder buf = new StringBuilder(); buf.append(\u0026#34;\u0026lt;!DOCTYPE html\u0026gt;\\r\\n\u0026#34;); buf.append(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;\u0026#34;); buf.append(dirPath); buf.append(\u0026#34;目录:\u0026#34;); buf.append(\u0026#34;\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;body\u0026gt;\\r\\n\u0026#34;); buf.append(\u0026#34;\u0026lt;h3\u0026gt;\u0026#34;); buf.append(dirPath).append(\u0026#34; 目录：\u0026#34;); buf.append(\u0026#34;\u0026lt;/h3\u0026gt;\\r\\n\u0026#34;); buf.append(\u0026#34;\u0026lt;ul\u0026gt;\u0026#34;); buf.append(\u0026#34;\u0026lt;li\u0026gt;链接：\u0026lt;a href=\\\u0026#34; ../\\\u0026#34;\u0026gt;..\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\\r\\n\u0026#34;); //展示所有文件和文件夹，同时使用超链接来标识 for (File f : dir.listFiles()) { if(f.isHidden() || !f.canRead()) { continue; } String name = f.getName(); if (!ALLOWED_FILE_NAME.matcher(name).matches()) { continue; } buf.append(\u0026#34;\u0026lt;li\u0026gt;链接：\u0026lt;a href=\\\u0026#34;\u0026#34;); buf.append(name); buf.append(\u0026#34;\\\u0026#34;\u0026gt;\u0026#34;); buf.append(name); buf.append(\u0026#34;\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\\r\\n\u0026#34;); } buf.append(\u0026#34;\u0026lt;/ul\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\\r\\n\u0026#34;); //分配对应消息的缓冲对象。 ByteBuf buffer = Unpooled.copiedBuffer(buf, UTF_8); //将缓冲区中的响应消息存放到Http应答消息中，然后释放缓冲区 response.content().writeBytes(buffer); buffer.release(); ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); } private void sendRedirect(ChannelHandlerContext ctx, String newUri) { FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, FOUND); response.headers().set(LOCATION,newUri); ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); } private void sendError(ChannelHandlerContext ctx, HttpResponseStatus status) { FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, status, Unpooled.copiedBuffer(\u0026#34;Failure: \u0026#34; + status.toString() + \u0026#34;\\r\\n\u0026#34;, UTF_8)); response.headers().set(CONTENT_TYPE, \u0026#34;text/html;charset=UTF-8\u0026#34;); ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); } private void setContentTypeHeader(HttpResponse response, File file) { MimetypesFileTypeMap mimetypesFileTypeMap = new MimetypesFileTypeMap(); response.headers().set(CONTENT_TYPE, mimetypesFileTypeMap.getContentType(file.getPath())); } } 10.3 Netty HTTP+XML协议栈开发 由于HTTP协议的通用性，很多异构系统间的通信交互采用HTTP协议，通过HTTP协议承载业务数据进行消息交互。例如非常流行的HTTP+XML或者Restful+JSON。\n很多基于HTTP的应用都是后台应用，HTTP仅仅是承载数据交换的一个通道，是一个载体而不是Web容器。所以在这种场景下，一般不需要Tomcat这样的重量行Web容器。\n且Tomcat会有很多安全漏洞。所以一个轻量级的HTTP协议栈是个更好的选择。\n10.3.1 开发场景介绍 模拟一个简单的用户订购系统。客户端填写订单，通过HTTP客户端向服务端发送订购请求，请求消息放在HTTP消息体中，以XML承载，即采用HTTP+XML的方式进行通信。\n双方采用HTTP1.1协议，连接类型为CLOSE方式，即双方交互完成，由HTTP服务端主动关闭链路，随后客户端也关闭链路并退出。\n订购请求消息定义如表所示：\n字段名称 类型 备注 订购数量 Int64 订购的商品数量 客户信息 Customer 客户信息，负责POJO对象 账单地址 Address 账单的地址 寄送方式 Shippling 枚举类型如下：普通快递宅急送国际配送国内快递国际快递 送货地址 Address 总价 float 客户信息定义：\n字段名称 类型 备注 客户ID Int64 客户ID，厂整型 姓 String 客户姓氏，字符串 名 String 客户名字，字符串 全名 List\u0026lt;String\u0026gt; 客户全程，字符列表 地址信息定义：\n字段名称 类型 备注 街道1 String 街道2 String 城市 String 省份 String 邮政编码 String 国家 String 邮递方式定义：\n字段名称 类型 备注 普通邮递 枚举类型 宅急送 枚举类型 国际邮递 枚举类型 国内快递 枚举类型 国际快递 枚举类型 10.3.2 HTTP+XML协议栈设计 步骤：\n构造订购请求消息，将请求消息编码为HTTP+XML格式； HTTP客户端发起连接，通过HTTP协议栈发送HTTP请求消息； HTTP服务端对HTTP+XML请求消息进行解码，解码成请求POJO； 服务端构造应答消息并编码，通过HTTP+XML方式返回给客户端； 客户端对HTTP+XML响应消息进行解码，解码成响应POJO。 流程分析：\n步骤1，需要自定义HTTP+XML格式的请求消息编码器；\n步骤2，可重用Netty的能力；\n步骤3，Netty可解析HTTP请求消息，但是消息体为XML，Netty无法将其解码为POJO，需要在Netty协议栈的基础上扩展实现。\n步骤4，需定制将POJO以XML方式发送\n步骤5，需定制解码。\n设计思路：\n（1）需要一套通用、高性能的XML序列化框架，它能够灵活的实现POJO-XML的互相转换，最好能够通过工具自动生成绑定关系，或者通过XML的方式配置双方的映射关系；\n（2）作为通用的HTTP+XML协议栈，XML-POJO的映射关系应该非常灵活，支持命名空间和自定义标签。\n（3）一系列HTTP+XML消息编解码器\n（4）编解码过程应该对协议栈使用者透明，对上层业务零侵入。\n10.3.3 高效的XML绑定框架JiBx JiBX入门 专门为Java语言设计的XML数据绑定框架JiBx。\n优点：转换效率高、配置绑定文件简单、不需要操作xpath文件、不需要写输行的get/set方法、对象属性名与XML文件element名可以不同等。\n绑定XML与Java对象的两个步骤：第一步是绑定XML文件，也就是映射XML文件与Java对象之间的对应关系；第二步是在运行时，实现XML文件与Java势力之间的互相转换。\n在运行程序之前，需要先配置绑定文件并进行绑定，在绑定过沉重它将会动态地修改程序中相应地class文件，主要是生成对应对象实例地方法和添加呗绑定标记地输行JiBX_bindingList等。它使用的技术是BCEL(Byte Code Engineering Library)，BCEL是Apache Software Foundation的Jakarta项目的一部分，也是目前Java classworking最广泛使用的一种框架，它可以让你深入JVM汇编语言进行类操作。在JiBX运行时，它使用了目前比较流行的一个技术XPP（Xml Pull Parsing），这也是JiBX如此高效的原因。\nXPP：将整个文档写入内存，然后进行DOM操作，也不是使用基于事件流的SAX。XPP使用饿是不断增加的数据流处理方式，同时允许在解析XML文件时中断。\n因书上的项目为Ant打包，与我使用的Maven有冲突，所以开发过程略。\n第11章 WebSocket协议开发 WebSocket解决的问题：由于HTTP协议的开销，导致它们不合适低延迟应用。\nWebSocket将网络套接字引入到了客户端和服务端，浏览器和服务器之间可以通过套接字建立持久的连接，双方随时都可以互发数据给对方，而不是之前由客户端控制的一请求一应答模式。\n11.1 HTTP协议的弊端 主要弊端如下：\n（1）HTTP协议为半双工。可以双向但不能同时。\n（2）HTTP消息冗长而繁琐。HTTP消息包含消息头、消息体、换行符等。通常以文本传输，相比于其他二进制通信协议，冗长而繁琐。\n（3）针对服务器推送的黑客攻击。例如长时间轮询。\n11.2 WebSocket入门 H5提供的一种浏览器与服务器间进行全双工通信的网络技术。\n在WebSocketAPI中，浏览器和服务器只需要一个握手的动作，然后浏览器和服务器直接就形成了一条快速通道，两者就可以直接互相传送数据了。基于TCP双向全双工进行消息传递。\nWebSocket的特点：\n单一的TCP连接，采用全双工模式通信； 对代理、防火墙和路由器透明； 无头部信息、Cookie和身份验证； 无安全开销； 通过“ping/pong”帧保持链路激活； 服务器可以主动传递消息给客户端，不再需要客户端轮询。 11.2.1 WebSocket背景 取代轮询和Comet技术，是客户端浏览器具备像C/S架构下桌面系统一样的实时通信能力。\n在流量和负载增大的情况下，WebSocket方案相比传统的AJAX轮询方案有极大的性能优势。\n11.2.2 WebSocket连接建立 发送一个HTTP请求，与平常的不同，包含一些附加头信息。\n返回的也包含附加信息。\n11.2.3 WebSocket生命周期 握手成功后，就能通过“messages”的方式进行通信了。一个消息由一个或多个帧组成。可以被分割成多个帧或者被合并。\n11.2.4 WebSocket连接关闭 为关闭WebSocket连接，客户端与服务端需要通过一个安全的方法关闭底层TCP连接以及TLS会话。如果合适，丢弃任何可能已经接收的字节，必要时（比如受到攻击）可以通过任何可用的手段关闭连接。\n底层的TCP连接，在正常情况下，应该首先由服务器关闭。在异常情况下（例如在一个合理的时间周期后没有接收到服务器的TCP Close），客户端可以发起TCP Close。因此，当服务器被指示关闭WebSocket连接时，它应该立即发起一个TCP Close操作；客户端应该等待服务器的TCP Close。\nWebSocket的握手关闭消息带有一个状态码和一个可选的关闭原因，它必须按照协议要求发送一个Close控制帧，当对端接收到关闭控制帧指令时，需要主动关闭WebSocket连接。\n11.3 Netty WebSocket协议开发 11.3.1 WebSocket服务端功能介绍 支持WebSocket的浏览器通过WebSocket协议发送请求消息给服务端，服务端对请求消息进行判断，如果时合法的WebSocket请求，则获取请求消息体（文本），并在后面追加字符串“欢迎使用Netty WebSocket服务，现在时刻：系统时间”。\n客户端HTML通过内嵌的JS脚本创建WebSocket连接，如果握手成功，在文本框中打印“打开WebSocket服务正常。浏览器支持WebSocket！”。\n11.3.2 WebSocket服务端开发 package WebSocket.server; import io.netty.bootstrap.ServerBootstrap; import io.netty.channel.Channel; import io.netty.channel.ChannelInitializer; import io.netty.channel.ChannelPipeline; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioServerSocketChannel; import io.netty.handler.codec.http.HttpObjectAggregator; import io.netty.handler.codec.http.HttpServerCodec; import io.netty.handler.stream.ChunkedWriteHandler; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 13:12 */ public class WebSocketServer { public void run(int port) throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(\u0026#34;http-codec\u0026#34;, new HttpServerCodec());//添加HttpServerCodec，将请求和应答消息编码或者解码为HTTP消息； pipeline.addLast(\u0026#34;aggregator\u0026#34;, new HttpObjectAggregator(65536));//添加HttpObjectAggregator，将HTTP消息的多个部分组合成一条完整的HTTP消息 ch.pipeline().addLast(\u0026#34;http-chunked\u0026#34;, new ChunkedWriteHandler());//向客户端发送H5文件，主要用于支持浏览器和服务端进行WebSocket通信 pipeline.addLast(\u0026#34;handler\u0026#34;, new WebSocketServerHandler()); } }); Channel ch = b.bind(port).sync().channel(); System.out.println(\u0026#34;Websocket服务器启动，端口：\u0026#34; + port + \u0026#39;。\u0026#39;); System.out.println(\u0026#34;浏览器访问\u0026#34;+\u0026#34;http://localhost:\u0026#34;+ port + \u0026#39;/\u0026#39;); ch.closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port = 8080; if (args.length \u0026gt; 0 ) { try { port = Integer.parseInt(args[0]); } catch (Exception e) { e.printStackTrace(); } } new WebSocketServer().run(port); } } package WebSocket.server; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelFutureListener; import io.netty.channel.ChannelHandlerContext; import io.netty.channel.SimpleChannelInboundHandler; import io.netty.handler.codec.http.DefaultFullHttpResponse; import io.netty.handler.codec.http.FullHttpRequest; import io.netty.handler.codec.http.websocketx.*; import java.util.logging.Level; import java.util.logging.Logger; import static io.netty.handler.codec.http.HttpHeaders.isKeepAlive; import static io.netty.handler.codec.http.HttpHeaders.setContentLength; import static io.netty.handler.codec.http.HttpResponseStatus.BAD_REQUEST; import static io.netty.handler.codec.http.HttpVersion.HTTP_1_1; import static io.netty.util.CharsetUtil.UTF_8; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 13:33 */ public class WebSocketServerHandler extends SimpleChannelInboundHandler\u0026lt;Object\u0026gt; { private static final Logger logger = Logger.getLogger(WebSocketServerHandler.class.getName()); private WebSocketServerHandshaker handshaker; @Override protected void messageReceived(ChannelHandlerContext ctx, Object msg) throws Exception { if (msg instanceof FullHttpRequest) {//握手请求 handleHttpRequest(ctx, (FullHttpRequest) msg); } else if (msg instanceof WebSocketFrame) { handleWebSocketFrame(ctx, (WebSocketFrame) msg); } } private void handleHttpRequest(ChannelHandlerContext ctx, FullHttpRequest req) { if (!req.getDecoderResult().isSuccess() || (!\u0026#34;websocket\u0026#34;.equals(req.headers().get(\u0026#34;Upgrade\u0026#34;)))) { sendHttpResponse(ctx, req, new DefaultFullHttpResponse(HTTP_1_1, BAD_REQUEST));//如果不是握手请求就返回400； return; } //构造握手工厂 WebSocketServerHandshakerFactory wsFactory = new WebSocketServerHandshakerFactory(\u0026#34;ws://localhost:8080/socket\u0026#34;, null, false); //握手处理类 handshaker = wsFactory.newHandshaker(req); if (handshaker == null) { WebSocketServerHandshakerFactory.sendUnsupportedWebSocketVersionResponse(ctx.channel()); } else { handshaker.handshake(ctx.channel(), req); } } private void sendHttpResponse(ChannelHandlerContext ctx, FullHttpRequest req, DefaultFullHttpResponse resp) { if (resp.getStatus().code() != 200) { ByteBuf buf = Unpooled.copiedBuffer(resp.getStatus().toString(), UTF_8); resp.content().writeBytes(buf); buf.release(); setContentLength(resp, resp.content().readableBytes()); } ChannelFuture f =ctx.channel().writeAndFlush(resp); if (!isKeepAlive(req) || resp.getStatus().code() != 200) { f.addListener(ChannelFutureListener.CLOSE); } } private void handleWebSocketFrame(ChannelHandlerContext ctx, WebSocketFrame frame) { if (frame instanceof CloseWebSocketFrame) { handshaker.close(ctx.channel(), ((CloseWebSocketFrame) frame).retain()); return; } if (frame instanceof PingWebSocketFrame) { ctx.channel().write(new PongWebSocketFrame(frame.content().retain())); return; } if (!(frame instanceof TextWebSocketFrame)) { throw new UnsupportedOperationException(String.format(\u0026#34;%s frame types not supported\u0026#34;, frame.getClass().getName())); } String request = ((TextWebSocketFrame) frame).text(); logger.info(request); if (logger.isLoggable(Level.FINE)) { logger.fine(String.format(\u0026#34;%s received %s\u0026#34;, ctx.channel(), request)); } ctx.channel().write( new TextWebSocketFrame(request + \u0026#34; , 欢迎使用Netty WebSocket服务，现在时刻：\u0026#34; + new java.util.Date().toString())); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush(); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); } } WebSocket本身非常复杂，可以通过多种形式（文本方式，二进制方式）承载信息。\n第12章 私有协议栈开发 私有协议灵活，在公司内部或组织内部使用。\n绝大多数私有协议传输层采用TCP/IP。\n12.1 私有协议介绍 除非授权，不然其他厂商无法使用。私有协议也称非标准协议。\n传统的Java应用中，通常使用以下4中方式进行跨节点通信：\n（1）通过RMI进行远程服务调用；\n（2）通过Java的Socket+Java序列化的方式进行跨节点调用；\n（3）利用一些开源的RPC框架进行远程服务嗲用，如Facebook的Thrift、Apache的Avro等；\n（4）利用标准的公有协议进行跨节点调用，例如HTTP+XML、RESTful+JSON或者WebService。\n私有协议：链路层的物理连接，对请求和响应消息进行编码，在请求和应答消息本身以外，也需要携带一些其他控制和管理类指令，例如链路建立的握手请求和响应消息、链路检测的心跳消息等。\n并没有标准的定义，只要能够用于跨进程、跨主机数据交换的非标准协议，都可以称为私有协议。\n12.2 Netty协议栈功能设计 用于内部各模块之间的通信， 它基于TCP/IP协议栈，是一个类HTTP协议的应用层协议栈。\n12.1.1 网络拓扑图 无固定客户端，服务端。谁发起谁就是客户端，接收时服务端。\n12.2.2 协议栈功能描述 （1）基于Netty的NIO通信框架，提供高性能饿异步通信能力；\n（2）提供消息的编解码框架，可以实现POJO的序列化和反序列化；\n（3）提供基于IP地址的白名单接入认证机制；\n（4）链路的有效性检验机制；\n（5）链路的断连重连机制。\n12.2.3 通信模型 具体步骤：\n（1）Netty协议栈客户端发送握手请求消息，携带节点ID等有效身份认证信息；\n（2）Netty协议栈服务端对握手请求消息进行合法性校验，包括节点ID有效性校验、节点重复登录校验和IP地址合法性校验，校验通过后，返回登录成功的握手应答消息；\n（3）链路建立成功之后，客户端发送业务；\n（4）链路成功之后，服务端发送心跳信息；\n（5）链路建立成功之后，客户端法统心跳信息；\n（6）链路建立成功之后，服务端发送业务消息；\n（7）服务端退出时，服务端关闭连接，客户端感知对方关闭连接后，被动关闭客户端连接。\n12.2.4 消息定义 两部分：\n消息头； 消息体。 协议详细描述部分不赘述，因为是私有协议，内容不重要，开发过程才是重点，之后看Dubbo在着重看协议内容。\n12.3 Netty协议栈开发 12.3.1 数据结构定义 package PrivateProtocol.struct; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 14:49 */ public class NettyMessage { private Header header;//消息头 private Object body;//消息体 public Header getHeader() { return header; } public void setHeader(Header header) { this.header = header; } public Object getBody() { return body; } public void setBody(Object body) { this.body = body; } @Override public String toString() { return \u0026#34;NettyMessage{\u0026#34; + \u0026#34;header=\u0026#34; + header + \u0026#34;, body=\u0026#34; + body + \u0026#39;}\u0026#39;; } } package PrivateProtocol.struct; import java.util.HashMap; import java.util.Map; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 14:50 */ public class Header { private int crcCode = 0xabef0101; private int length;//消息长度 private long sessionID;//会话ID private byte type;//消息类型 private byte priority;//消息优先级 private Map\u0026lt;String, Object\u0026gt; attachment = new HashMap\u0026lt;String, Object\u0026gt;();//附件 public int getCrcCode() { return crcCode; } public void setCrcCode(int crcCode) { this.crcCode = crcCode; } public int getLength() { return length; } public void setLength(int length) { this.length = length; } public long getSessionID() { return sessionID; } public void setSessionID(long sessionID) { this.sessionID = sessionID; } public byte getType() { return type; } public void setType(byte type) { this.type = type; } public byte getPriority() { return priority; } public void setPriority(byte priority) { this.priority = priority; } public Map\u0026lt;String, Object\u0026gt; getAttachment() { return attachment; } public void setAttachment(Map\u0026lt;String, Object\u0026gt; attachment) { this.attachment = attachment; } @Override public String toString() { return \u0026#34;Header{\u0026#34; + \u0026#34;crcCode=\u0026#34; + crcCode + \u0026#34;, length=\u0026#34; + length + \u0026#34;, sessionID=\u0026#34; + sessionID + \u0026#34;, type=\u0026#34; + type + \u0026#34;, priority=\u0026#34; + priority + \u0026#34;, attachment=\u0026#34; + attachment + \u0026#39;}\u0026#39;; } } 由于心跳消息、握手请求和握手应答消息都可以统一由NettyMessage承载，所以不需要为这几类控制消息做单独的数据结构定义。\n12.3.2 消息编解码 package PrivateProtocol.codec; import PrivateProtocol.struct.NettyMessage; import io.netty.buffer.ByteBuf; import io.netty.channel.ChannelHandlerContext; import io.netty.handler.codec.MessageToByteEncoder; import java.io.IOException; import java.util.Map; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 14:59 */ public class NettyMessageEncoder extends MessageToByteEncoder\u0026lt;NettyMessage\u0026gt; { MarshallingEncoder marshallingEncoder; public NettyMessageEncoder() throws IOException{ this.marshallingEncoder = new MarshallingEncoder(); } @Override protected void encode(ChannelHandlerContext ctx, NettyMessage msg, ByteBuf sendBuf) throws Exception { if (msg == null || msg.getHeader() == null) { throw new Exception(\u0026#34;The encode message is null\u0026#34;); } sendBuf.writeInt(msg.getHeader().getCrcCode()); sendBuf.writeInt(msg.getHeader().getLength()); sendBuf.writeLong(msg.getHeader().getSessionID()); sendBuf.writeByte(msg.getHeader().getType()); sendBuf.writeByte(msg.getHeader().getPriority()); sendBuf.writeInt(msg.getHeader().getAttachment().size()); String key = null; byte[] keyArray = null; Object value = null; for (Map.Entry\u0026lt;String, Object\u0026gt; param : msg.getHeader().getAttachment().entrySet()) { key = param.getKey(); keyArray = key.getBytes(\u0026#34;UTF-8\u0026#34;); sendBuf.writeInt(keyArray.length); sendBuf.writeBytes(keyArray); value = param.getValue(); marshallingEncoder.encode(value, sendBuf); } key = null; keyArray = null; value = null; if (msg.getBody() != null) { marshallingEncoder.encode(msg.getBody(), sendBuf); } else sendBuf.writeInt(0); sendBuf.setInt(4, sendBuf.readableBytes() - 8); } } package PrivateProtocol.codec; import io.netty.buffer.ByteBuf; import org.jboss.marshalling.Marshaller; import java.io.IOException; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 15:06 */ public class MarshallingEncoder { private static final byte[] LENGTH_PLACEHOLDER = new byte[4]; Marshaller marshaller; public MarshallingEncoder() throws IOException { marshaller = MarshallingCodecFactory.buildMarshalling(); } public void encode(Object msg, ByteBuf out) throws IOException { try { int lengthPos = out.writerIndex(); out.writeBytes(LENGTH_PLACEHOLDER); ChannelBufferByteOutput output = new ChannelBufferByteOutput(out); marshaller.start(output); marshaller.writeObject(msg); marshaller.finish(); out.setInt(lengthPos, out.writerIndex() - lengthPos - 4); } finally { marshaller.close(); } } } package PrivateProtocol.codec; import PrivateProtocol.struct.Header; import PrivateProtocol.struct.NettyMessage; import io.netty.buffer.ByteBuf; import io.netty.channel.ChannelHandlerContext; import io.netty.handler.codec.LengthFieldBasedFrameDecoder; import java.io.IOException; import java.nio.ByteOrder; import java.util.HashMap; import java.util.Map; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 15:36 */ /* *Netty的LengthFieldBasedFrameDecoder解码器，它支持自动的TCP粘包和半包处理， *只需要给出标识消息长度饿字段偏移量和消息长度自身所占的字节数，Netty就能自动实现对半包的处理。 */ public class NettyMessageDecoder extends LengthFieldBasedFrameDecoder { MarshallingDecoder marshallingDecoder; public NettyMessageDecoder(int maxFrameLength, int lengthFieldOffset, int lengthFieldLength) throws IOException { super(maxFrameLength, lengthFieldOffset, lengthFieldLength); marshallingDecoder = new MarshallingDecoder(); } @Override protected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception { ByteBuf frame = (ByteBuf) super.decode(ctx, in); if (frame == null) { return null; } NettyMessage message = new NettyMessage(); Header header = new Header(); header.setCrcCode(frame.readInt()); header.setLength(frame.readInt()); header.setSessionID(frame.readLong()); header.setType(frame.readByte()); header.setPriority(frame.readByte()); int size = frame.readInt(); if (size \u0026gt; 0) { Map\u0026lt;String, Object\u0026gt; attch = new HashMap\u0026lt;String, Object\u0026gt;(size); int keySize = 0; byte[] keyArray = null; String key = null; for (int i = 0; i \u0026lt; size; i++) { keySize = frame.readInt(); keyArray = new byte[keySize]; frame.readBytes(keyArray); key = new String(keyArray, \u0026#34;UTF-8\u0026#34;); attch.put(key, marshallingDecoder.decode(frame)); } keyArray = null; key = null; header.setAttachment(attch); } if (frame.readableBytes() \u0026gt; 4) { message.setBody(marshallingDecoder.decode(frame)); } message.setHeader(header); return message; } } package PrivateProtocol.codec; import io.netty.buffer.ByteBuf; import org.jboss.marshalling.ByteInput; import org.jboss.marshalling.Unmarshaller; import java.io.IOException; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 15:47 */ public class MarshallingDecoder { private final Unmarshaller unmarshaller; public MarshallingDecoder() throws IOException { unmarshaller = MarshallingCodecFactory.buildUnMarshalling(); } protected Object decode(ByteBuf in) throws Exception { int objectSize = in.readInt(); ByteBuf buf = in.slice(in.readerIndex(), objectSize); ByteInput input = new ChannelBufferByteInput(buf); try { unmarshaller.start(input); Object obj = unmarshaller.readObject(); unmarshaller.finish(); in.readerIndex(in.readerIndex() + objectSize); return obj; } finally { unmarshaller.close(); } } } 12.3.3 握手和安全验证 package PrivateProtocol.client; import PrivateProtocol.struct.Header; import PrivateProtocol.struct.NettyMessage; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import java.awt.*; import static PrivateProtocol.common.MessageType.LOGIN_REQ; import static PrivateProtocol.common.MessageType.LOGIN_RESP; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 16:08 */ public class LoginAuthReqHandler extends ChannelHandlerAdapter { private static final Log LOG = LogFactory.getLog(LoginAuthReqHandler.class); //TCP连接三次握手成功之后又客户端构造握手请求消息发送给服务端，由于采用白名单认证机制，不需要携带消息体。 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { ctx.writeAndFlush(buildLoginReq()); } //对我收应答消息进行处理 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { NettyMessage message = (NettyMessage) msg; if (message.getHeader() != null \u0026amp;\u0026amp; message.getHeader().getType() == LOGIN_RESP.value()) { byte loginResult = (byte) message.getBody(); //非0则认证失败，关闭链路 if (loginResult != (byte) 0) { ctx.close(); } else { LOG.info(\u0026#34;Login is ok：\u0026#34; + message); ctx.fireChannelRead(msg); } } else { //不是握手应答消息，传递给后面的ChannelHandler处理 ctx.fireChannelRead(msg); } } private NettyMessage buildLoginReq() { NettyMessage message = new NettyMessage(); Header header = new Header(); header.setType(LOGIN_REQ.value()); message.setHeader(header); return message; } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { ctx.fireExceptionCaught(cause); } } package PrivateProtocol.server; import PrivateProtocol.struct.Header; import PrivateProtocol.struct.NettyMessage; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import java.net.InetAddress; import java.net.InetSocketAddress; import java.net.UnknownHostException; import java.util.Map; import java.util.concurrent.ConcurrentHashMap; import static PrivateProtocol.common.MessageType.LOGIN_REQ; import static PrivateProtocol.common.MessageType.LOGIN_RESP; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/23 15:56 */ public class LoginAuthRespHandler extends ChannelHandlerAdapter { public final static Log LOG = LogFactory.getLog(LoginAuthRespHandler.class); //重复登录保护 private Map\u0026lt;String, Boolean\u0026gt; nodeCheck = new ConcurrentHashMap\u0026lt;String, Boolean\u0026gt;(); //白名单 private String[] whitekList = {\u0026#34;127.0.0.1\u0026#34;, InetAddress.getLocalHost().getHostAddress()}; public LoginAuthRespHandler() throws UnknownHostException { } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { NettyMessage message = (NettyMessage) msg; if (message.getHeader() != null \u0026amp;\u0026amp; message.getHeader().getType() == LOGIN_REQ.value()) { String nodeIndex = ctx.channel().remoteAddress().toString(); NettyMessage loginResp = null; if (nodeCheck.containsKey(nodeIndex)) { loginResp = buildResponse((byte)-1); } else { InetSocketAddress address = (InetSocketAddress) ctx.channel().remoteAddress(); String ip = address.getAddress().getHostAddress(); boolean isOK = false; for (String WIP : whitekList) { if (WIP.equals(ip)) { isOK = true; break; } } loginResp = isOK ? buildResponse((byte)0) : buildResponse((byte)-1); if (isOK) { nodeCheck.put(nodeIndex, true); } LOG.info(\u0026#34;The login response is : \u0026#34; + loginResp + \u0026#34; body [\u0026#34; + loginResp.getBody() + \u0026#34;]\u0026#34;); ctx.writeAndFlush(loginResp); } } else { ctx.fireChannelRead(msg); } } private NettyMessage buildResponse(byte result) { NettyMessage message = new NettyMessage(); Header header = new Header(); header.setType(LOGIN_RESP.value()); message.setHeader(header); message.setBody(result); return message; } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); //出现异常时需要去注册，保证后续客户端可以重连成功 nodeCheck.remove(ctx.channel().remoteAddress().toString()); ctx.close(); ctx.fireExceptionCaught(cause); } } 12.3.4 心跳检测机制 package PrivateProtocol.client; import PrivateProtocol.struct.Header; import PrivateProtocol.struct.NettyMessage; import io.netty.channel.ChannelHandlerAdapter; import io.netty.channel.ChannelHandlerContext; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import java.util.concurrent.ScheduledFuture; import java.util.concurrent.TimeUnit; import static PrivateProtocol.common.MessageType.*; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/26 10:36 */ public class HeartBeatReqHandler extends ChannelHandlerAdapter { private static final Log LOG = LogFactory.getLog(HeartBeatReqHandler.class); private volatile ScheduledFuture\u0026lt;?\u0026gt; heartBeat; //握手成功后启动无限循环定时器用于定期发送心跳消息 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { NettyMessage message = (NettyMessage) msg; if (message.getHeader() != null \u0026amp;\u0026amp; message.getHeader().getType() == LOGIN_RESP.value()) { //每5秒发送一条心跳消息 heartBeat = ctx.executor().scheduleAtFixedRate( new HeartBeatTask(ctx), 0, 5000, TimeUnit.MILLISECONDS ); //处理服务端发送的心跳应答消息 } else if (message.getHeader() != null \u0026amp;\u0026amp; message.getHeader().getType() == HEARTBEAT_RESP.value()) { LOG.info(\u0026#34;Client receive server heart beat message : ---\u0026gt; \u0026#34; + message); } else { ctx.fireChannelRead(msg); } } private class HeartBeatTask implements Runnable { private final ChannelHandlerContext ctx; private HeartBeatTask(ChannelHandlerContext ctx) { this.ctx = ctx; } @Override public void run() { NettyMessage heartBeat = buildHeartBeat(); LOG.info(\u0026#34;Client send heart beat messsage to server : ---\u0026gt; \u0026#34; + heartBeat); ctx.writeAndFlush(heartBeat); } private NettyMessage buildHeartBeat() { NettyMessage message = new NettyMessage(); Header header = new Header(); header.setType(HEARTBEAT_REQ.value()); message.setHeader(header); return message; } } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); if (heartBeat != null) { heartBeat.cancel(true); heartBeat = null; } ctx.fireExceptionCaught(cause); } } package PrivateProtocol.server; import PrivateProtocol.struct.Header; import PrivateProtocol.struct.NettyMessage; import io.netty.channel.ChannelHandlerAppender; import io.netty.channel.ChannelHandlerContext; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import static PrivateProtocol.common.MessageType.HEARTBEAT_REQ; import static PrivateProtocol.common.MessageType.HEARTBEAT_RESP; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/26 10:55 */ public class HeartBeatRespHandler extends ChannelHandlerAppender { private static final Log LOG = LogFactory.getLog(HeartBeatRespHandler.class); @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { NettyMessage message = (NettyMessage) msg; if (message.getHeader() != null \u0026amp;\u0026amp; message.getHeader().getType() == HEARTBEAT_REQ.value()) { LOG.info(\u0026#34;Receive client heart beat message : ---\u0026gt; \u0026#34; + message); NettyMessage heartBeat = buildHeatBeat(); LOG.info(\u0026#34;Send heart beat response message to client : ---\u0026gt; \u0026#34; + heartBeat); ctx.writeAndFlush(heartBeat); } else { ctx.fireChannelRead(msg); } } private NettyMessage buildHeatBeat() { NettyMessage message = new NettyMessage(); Header header = new Header(); header.setType(HEARTBEAT_RESP.value()); message.setHeader(header); return message; } } 12.3.5 断线重连 package PrivateProtocol.client; import PrivateProtocol.codec.NettyMessageDecoder; import PrivateProtocol.codec.NettyMessageEncoder; import PrivateProtocol.common.NettyConstant; import io.netty.bootstrap.Bootstrap; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelInitializer; import io.netty.channel.ChannelOption; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioSocketChannel; import io.netty.handler.timeout.ReadTimeoutHandler; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import java.net.InetSocketAddress; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; import static PrivateProtocol.common.NettyConstant.*; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/26 11:04 */ public class NettyClient { private static final Log LOG = LogFactory.getLog(NettyClient.class); private ScheduledExecutorService executor = Executors.newScheduledThreadPool(1); EventLoopGroup group = new NioEventLoopGroup(); public void connect(int port, String host) throws Exception { try { Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { //消息解码，并设置了最大长度上限 ch.pipeline().addLast(new NettyMessageDecoder(1024 * 1024, 4, 4)); //消息编码 ch.pipeline().addLast(\u0026#34;MessageEncoder\u0026#34;, new NettyMessageEncoder()); //读超时 ch.pipeline().addLast(\u0026#34;readTimeoutHandler\u0026#34;, new ReadTimeoutHandler(50)); //握手请求 ch.pipeline().addLast(\u0026#34;LoginAuthHandler\u0026#34;, new LoginAuthReqHandler()); //心跳消息 ch.pipeline().addLast(\u0026#34;HeartBeatHandler\u0026#34;, new HeartBeatReqHandler()); //类似于AOP但比AOP性能更高。 } }); //ChannelFuture f = b.connect(new InetSocketAddress(host, port), //new InetSocketAddress(LOCALIP, LOCAL_PORT)).sync(); ChannelFuture f = b.connect(\u0026#34;127.0.0.1\u0026#34;, 8080).sync(); f.channel().closeFuture().sync(); } finally { executor.execute(new Runnable() { @Override public void run() { try { TimeUnit.SECONDS.sleep(1); try { connect(PORT, REMOTEIP);//发起重连操作 } catch (Exception e) { e.printStackTrace(); } } catch (InterruptedException e) { e.printStackTrace(); } } }); } } public static void main(String[] args) throws Exception { new NettyClient().connect(PORT, REMOTEIP); } } package PrivateProtocol.server; import PrivateProtocol.codec.NettyMessageDecoder; import PrivateProtocol.codec.NettyMessageEncoder; import io.netty.bootstrap.ServerBootstrap; import io.netty.channel.ChannelInitializer; import io.netty.channel.ChannelOption; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioServerSocketChannel; import io.netty.handler.logging.LogLevel; import io.netty.handler.logging.LoggingHandler; import io.netty.handler.timeout.ReadTimeoutHandler; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import static PrivateProtocol.common.NettyConstant.PORT; import static PrivateProtocol.common.NettyConstant.REMOTEIP; /** * \u0026lt;p\u0026gt;Description: xx\u0026lt;/p\u0026gt; * * @author 李宏博 * @version 1.0 * @create 2019/8/26 11:23 */ public class NettyServer { private static final Log LOG = LogFactory.getLog(NettyServer.class); public void bind() throws Exception { EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ch.pipeline().addLast(new NettyMessageDecoder(1024 * 1024, 4, 4)); ch.pipeline().addLast(new NettyMessageEncoder()); ch.pipeline().addLast(new ReadTimeoutHandler(50)); ch.pipeline().addLast(new LoginAuthRespHandler()); ch.pipeline().addLast(new HeartBeatRespHandler()); } }); b.bind(REMOTEIP, PORT).sync(); LOG.info(\u0026#34;Netty server start ok : \u0026#34; + (REMOTEIP + \u0026#34; : \u0026#34; + PORT)); } public static void main(String[] args) throws Exception { new NettyServer().bind(); } } 12.5 总结 协议栈还有很多缺陷，比如断线后，发送的消息保存在哪里？\n之后的章节 源码就不记录了，详细看这个https://www.javadoop.com/post/netty-part-1\n","permalink":"http://121.199.2.5:6080/e15796699f1a419ba85731d159eba322/","summary":"第1章 Java的I/O演进之路 1.1 I/O基础入门 1.1.1 Linux网络I/O模型简介 UNIX提供了5种I/O模型：\n（1）阻塞I/O模型\n（2）非阻塞I/O模型\n（3）I/O复用模型\n（4）信号驱动I/O模型\n（5）异步I/O\n1.1.2 I/O多路复用技术 把多个I/O阻塞复用到同一个select的阻塞上，从而是的系统在单线程的情况下可以同时处理多个客户端请求。比多线程有性能优势，节约资源。\n支持I/O多路复用的系统调用select/pselect/poll/epoll。\nepoll的优点：\n支持一个进程打开的socket描述符（FD）不受限制（仅受限于操作系统的最大文件句柄数（内存））。 I/O效率不会随着FD数目的增加而线性下降。 使用mmap加速内核与用户空间的消息传递。 epoll的API更加简单。 1.2 Java的I/O演进 历史题，略。\n第2章 NIO入门 2.1 传统的BIO编程 C/S模型，客户端发起连接请求，三次握手，通过Socket进行通信。\n2.1.1 BIO通信模型图 一请求一应答模型：每次接收到连接请求都创建一个新的线程进行链路处理。处理完成后通过输出流返回应答给客户端，线程销毁。\n该模型最大的问题就是：缺乏弹性伸缩能力，当客户端并发访问量增加后，服务端的线程个数和客户端并发访问数呈1：1的正比关系。\n2.1.2 同步阻塞式I/O创建的TimeServer源码分析 import java.io.IOException; import java.net.ServerSocket; import java.net.Socket; /** * \u0026lt;p\u0026gt;Description: 同步阻塞式I/O创建的TimeServer\u0026lt;/p\u0026gt; * * @author 李宏博 * @version xxx * @create 2019/8/14 17:58 */ public class TimeServer { /** * * @param args */ public static void main(String[] args) throws IOException { int port = 8080; if (args != null \u0026amp;\u0026amp; args.length \u0026gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } ServerSocket server = null; try { server = new ServerSocket(port); System.","title":"第1章 Java的I/O演进之路"},{"content":"第1章 Spring框架的由来 1.1 Spring之崛起 1.2 Spring框架概述 基于POJO（Plain Old Java Object，简单Java对象）的轻量级开发理念。\nSpring总体架构：\n1.3 Spring大观园 1.4 小结 第2章 Spring的IOC容器 2.1 我们的理念是：让别人为你服务 2.2 手语，呼喊，还是心有灵犀 2.2.1 构造方法注入 IoC Service Provider会检查被注入对象的构造方法，取得它所需要的依赖对象列表，进而为其注 入相应的对象。同一个对象是不可能被构造两次的，因此，被注入对象的构造乃至其整个生命周期， 应该是由IoC Service Provider来管理的。\n2.2.2 setter方法注入 setter方法注入虽不像构造方法注入那样，让对象构造完成后即可使用，但相对来说更宽松一些， 可以在对象构造完成后再注入。\n2.2.3 接口注入 对于前两种注入方式来说，接口注入没有那么简单明了。被注入对象如果想要IoC Service Provider为其注入依赖对象，就必须实现某个接口。这个接口提供一个方法，用来为其注入依赖对象。 IoC Service Provider最终通过这些接口来了解应该为被注入对象注入什么依赖对象。\n示例：\n2.2.4 三种注入方式的比较 接口注入。不提倡，带有侵入性 构造方法注入。这种注入方式的优点就是，对象在构造完成之后，即已进入就绪状态，可以马上使用。缺点就是，当依赖对象比较多的时候，构造方法的参数列表会比较长。而通过反射构造对象的时候，对相同类型的参数的处理会比较困难，维护和使用上也比较麻烦。而且在Java中，构造方法无法被继承，无法设置默认值。对于非必须的依赖处理，可能需要引入多个构造方法，而参数数量的变动可能造成维护上的不便。 setter方法注入。因为方法可以命名， 所以setter方法注入在描述性上要比构造方法注入好一些。另外， setter方法可以被继承，允许设置默认值，而且有良好的IDE支持。缺点当然就是对象无法在构造完成后马上进入就绪状态。 2.3 IOC的附加值 ","permalink":"http://121.199.2.5:6080/567c6d054f1d4d53a29e9d24f213165a/","summary":"第1章 Spring框架的由来 1.1 Spring之崛起 1.2 Spring框架概述 基于POJO（Plain Old Java Object，简单Java对象）的轻量级开发理念。\nSpring总体架构：\n1.3 Spring大观园 1.4 小结 第2章 Spring的IOC容器 2.1 我们的理念是：让别人为你服务 2.2 手语，呼喊，还是心有灵犀 2.2.1 构造方法注入 IoC Service Provider会检查被注入对象的构造方法，取得它所需要的依赖对象列表，进而为其注 入相应的对象。同一个对象是不可能被构造两次的，因此，被注入对象的构造乃至其整个生命周期， 应该是由IoC Service Provider来管理的。\n2.2.2 setter方法注入 setter方法注入虽不像构造方法注入那样，让对象构造完成后即可使用，但相对来说更宽松一些， 可以在对象构造完成后再注入。\n2.2.3 接口注入 对于前两种注入方式来说，接口注入没有那么简单明了。被注入对象如果想要IoC Service Provider为其注入依赖对象，就必须实现某个接口。这个接口提供一个方法，用来为其注入依赖对象。 IoC Service Provider最终通过这些接口来了解应该为被注入对象注入什么依赖对象。\n示例：\n2.2.4 三种注入方式的比较 接口注入。不提倡，带有侵入性 构造方法注入。这种注入方式的优点就是，对象在构造完成之后，即已进入就绪状态，可以马上使用。缺点就是，当依赖对象比较多的时候，构造方法的参数列表会比较长。而通过反射构造对象的时候，对相同类型的参数的处理会比较困难，维护和使用上也比较麻烦。而且在Java中，构造方法无法被继承，无法设置默认值。对于非必须的依赖处理，可能需要引入多个构造方法，而参数数量的变动可能造成维护上的不便。 setter方法注入。因为方法可以命名， 所以setter方法注入在描述性上要比构造方法注入好一些。另外， setter方法可以被继承，允许设置默认值，而且有良好的IDE支持。缺点当然就是对象无法在构造完成后马上进入就绪状态。 2.3 IOC的附加值 ","title":"第1章 Spring框架的由来"},{"content":"第1章 概述 互联网公司的分布式两个特点：规模大、成本低。\n1.1 分布式存储概念 定义：分布式存储系统是大量普通PC服务器通过Internet互联，对外作为一个整体提供存储服务。\n特性：\n可扩展。集群，性能随规模线性增长。 低成本。有自动容错，自动负载均衡机制。所以可以构建在普通PC机器。 高性能。 易用。提供易用的对外接口。 主要挑战：数据、状态信息的持久化，要求在自动迁移、自动容错、并发读写的过程中保证数据的一致性。\n主要技术来自于分布式系统和数据库：\n数据分布：如何保证均匀？如何实现跨服务器读写？ 一致性：如何复制？如何保证出现异常也一致？ 容错：如何检测？出错如何迁移？ 负载均衡：如何实现？如何做到迁移时不影响其他业务？ 事务与并发控制：如何实现分布式事务？如何实现MVCC？ 易用性：如何易用？如何方便运维？ 压缩/解压缩：如何根据数据特点设计压缩/解压缩算法？如何平衡节省的存储空间和消耗的CPU资源浪费？ 1.2 分布式存储分类 数据大致分为三类：\n非结构化数据：包括所有格式的办公文档、文本、图片、图像、音频和视频信息等。 结构化数据：一般存储在关系数据库中，可用二位关系表结构来表示。 半结构化数据：与结构化的区别在于，半结构化数据的模式结构和内容混在一起，没有明显的区分，也不需要预先定义数据的模式结构。 存储系统分为四类：\n分布式文件系统 图片、照片、视频等非结构化数据对象，以对象组织，对象之间没有关联，称为Blob（Binary Large Object，二进制大对象）数据。\n总体上看：分布式文件系统存储三种类型的数据：Blob对象、定长块以及大文件。在系统层面，分布式文件系统内部按照数据块来组织数据，每个数据块的大小大致相同，每个数据块可以包含多个Blob对象或者定长块，一个大文件也可以拆分成多个数据块。\n分布式键值系统 用于存储关系简单的半结构化数据，它只提供基于主键的CRUD功能，即根据主键创建、读取、更新或者删除一条键值记录。\n与传统哈希表相似，但是支持将数据分布到多个存储节点。\n分布式键值系统是分布式表格系统的一种简化实现，一般用作缓存。\n分布式表格系统 用于存储较为复杂的半结构化数据。不仅仅支持简单的CRUD操作，而且支持扫面某个主键范围。\n借鉴了许多关系型数据库的技术，例如支持某种程度上的事务，比如单行事务，某个实体组下的多行事务。\n与分布式数据库先比，仅支持针对单张表格的操作，不支持复杂操作。\n分布式数据库 一般从单机数据库扩展而来，用于存储结构化数据。\n第2章 单机存储系统 单机存储引擎就是哈希表、B树等数据结构在机械磁盘、SSD等持久化介质上的实现。\n2.1 硬件基础 2.1.1 CPU架构 经典的多CPU架构为对称多处理结构（Symmetric Multi-Processing，SMP），即在一个计算机上汇集了一组处理器，它们之间对称工作，无主次或从属关系，共享相同的物理内存及总线。\n2.1.2 IO总线 存储系统的性能瓶颈一般在与IO。\n2.1.3 网络拓扑 传统的数据中心网络拓扑，分三层。最下面是接入层，中间是汇聚层，上面是汇聚层。存在的问题：大量下层接入，导致同一个接入层下的服务器之间的带宽减小。\n2.1.4 性能参数 2.1.5 存储层次架构 存储系统的性能主要包括两个维度：吞吐量以及访问延时。\n2.2 单机存储引擎 2.2.1 哈希存储引擎 Bitcask是一个基于哈希表结构的键值寸尺系统，它仅支持追加操作（Append-only），即所有的写操作只追加而不修改老的数据。\n在Bitcask系统中，每个文件有一定的大小限制，当文件增加到相应的大小时，就会产生一个新的文件，老的文件只读不写。在任意时刻，只有一个文件市可写的，用于数据追加，称为活跃数据文件。而其他已经达到大小限制的文件，称为老数据文件。\n数据结构 一条一条写入，每条记录的数据项分别为：主键（key），value内容（value），主键长度（key_sz），value长度（value_sz），时间戳（timetamp）以及crc校验值。（删除不会删除旧的条目，而是将value设定为一个特殊的之作标识）。内存中的结构是hash表。\n定期合并 为解决垃圾文件问题。将所有老数据文件中的数据扫描一遍生成一个新的数据文件。对用一个key的多个操作以保留最新的一个原则进行删除。\n快速恢复 每次合并时，将内存中的哈希索引表转储到磁盘中，生成一个索引文件。这个索引文件只存储value的位置。\n2.2.2 B树存储引擎 不仅支持随机读取，还支持范围扫描。\n数据结构 InnoDB按照页面（Page）来组织数据，每个页面对用B+树的一个节点。\nB+树的根节点是常驻内存的。修改操作首先需要记录提交日志，接着修改内存中的B+树。\n缓冲区管理 LRU、LIRS\n2.2.3 LSM树存储引擎 将对数据的修改增量保持在内存中，达到指定的大小先之后将这些修改操作批量写入磁盘，读取时需要合并磁盘中的历史数据和内存中最近的修改操作。\n","permalink":"http://121.199.2.5:6080/b221a7f05b774f7ab4305cff0f244d52/","summary":"第1章 概述 互联网公司的分布式两个特点：规模大、成本低。\n1.1 分布式存储概念 定义：分布式存储系统是大量普通PC服务器通过Internet互联，对外作为一个整体提供存储服务。\n特性：\n可扩展。集群，性能随规模线性增长。 低成本。有自动容错，自动负载均衡机制。所以可以构建在普通PC机器。 高性能。 易用。提供易用的对外接口。 主要挑战：数据、状态信息的持久化，要求在自动迁移、自动容错、并发读写的过程中保证数据的一致性。\n主要技术来自于分布式系统和数据库：\n数据分布：如何保证均匀？如何实现跨服务器读写？ 一致性：如何复制？如何保证出现异常也一致？ 容错：如何检测？出错如何迁移？ 负载均衡：如何实现？如何做到迁移时不影响其他业务？ 事务与并发控制：如何实现分布式事务？如何实现MVCC？ 易用性：如何易用？如何方便运维？ 压缩/解压缩：如何根据数据特点设计压缩/解压缩算法？如何平衡节省的存储空间和消耗的CPU资源浪费？ 1.2 分布式存储分类 数据大致分为三类：\n非结构化数据：包括所有格式的办公文档、文本、图片、图像、音频和视频信息等。 结构化数据：一般存储在关系数据库中，可用二位关系表结构来表示。 半结构化数据：与结构化的区别在于，半结构化数据的模式结构和内容混在一起，没有明显的区分，也不需要预先定义数据的模式结构。 存储系统分为四类：\n分布式文件系统 图片、照片、视频等非结构化数据对象，以对象组织，对象之间没有关联，称为Blob（Binary Large Object，二进制大对象）数据。\n总体上看：分布式文件系统存储三种类型的数据：Blob对象、定长块以及大文件。在系统层面，分布式文件系统内部按照数据块来组织数据，每个数据块的大小大致相同，每个数据块可以包含多个Blob对象或者定长块，一个大文件也可以拆分成多个数据块。\n分布式键值系统 用于存储关系简单的半结构化数据，它只提供基于主键的CRUD功能，即根据主键创建、读取、更新或者删除一条键值记录。\n与传统哈希表相似，但是支持将数据分布到多个存储节点。\n分布式键值系统是分布式表格系统的一种简化实现，一般用作缓存。\n分布式表格系统 用于存储较为复杂的半结构化数据。不仅仅支持简单的CRUD操作，而且支持扫面某个主键范围。\n借鉴了许多关系型数据库的技术，例如支持某种程度上的事务，比如单行事务，某个实体组下的多行事务。\n与分布式数据库先比，仅支持针对单张表格的操作，不支持复杂操作。\n分布式数据库 一般从单机数据库扩展而来，用于存储结构化数据。\n第2章 单机存储系统 单机存储引擎就是哈希表、B树等数据结构在机械磁盘、SSD等持久化介质上的实现。\n2.1 硬件基础 2.1.1 CPU架构 经典的多CPU架构为对称多处理结构（Symmetric Multi-Processing，SMP），即在一个计算机上汇集了一组处理器，它们之间对称工作，无主次或从属关系，共享相同的物理内存及总线。\n2.1.2 IO总线 存储系统的性能瓶颈一般在与IO。\n2.1.3 网络拓扑 传统的数据中心网络拓扑，分三层。最下面是接入层，中间是汇聚层，上面是汇聚层。存在的问题：大量下层接入，导致同一个接入层下的服务器之间的带宽减小。\n2.1.4 性能参数 2.1.5 存储层次架构 存储系统的性能主要包括两个维度：吞吐量以及访问延时。\n2.2 单机存储引擎 2.2.1 哈希存储引擎 Bitcask是一个基于哈希表结构的键值寸尺系统，它仅支持追加操作（Append-only），即所有的写操作只追加而不修改老的数据。\n在Bitcask系统中，每个文件有一定的大小限制，当文件增加到相应的大小时，就会产生一个新的文件，老的文件只读不写。在任意时刻，只有一个文件市可写的，用于数据追加，称为活跃数据文件。而其他已经达到大小限制的文件，称为老数据文件。\n数据结构 一条一条写入，每条记录的数据项分别为：主键（key），value内容（value），主键长度（key_sz），value长度（value_sz），时间戳（timetamp）以及crc校验值。（删除不会删除旧的条目，而是将value设定为一个特殊的之作标识）。内存中的结构是hash表。\n定期合并 为解决垃圾文件问题。将所有老数据文件中的数据扫描一遍生成一个新的数据文件。对用一个key的多个操作以保留最新的一个原则进行删除。\n快速恢复 每次合并时，将内存中的哈希索引表转储到磁盘中，生成一个索引文件。这个索引文件只存储value的位置。\n2.2.2 B树存储引擎 不仅支持随机读取，还支持范围扫描。\n数据结构 InnoDB按照页面（Page）来组织数据，每个页面对用B+树的一个节点。\nB+树的根节点是常驻内存的。修改操作首先需要记录提交日志，接着修改内存中的B+树。\n缓冲区管理 LRU、LIRS\n2.2.3 LSM树存储引擎 将对数据的修改增量保持在内存中，达到指定的大小先之后将这些修改操作批量写入磁盘，读取时需要合并磁盘中的历史数据和内存中最近的修改操作。","title":"第1章 概述"},{"content":"第2章 InnoDB存储引擎 事务安全。\n2.1 InnoDB存储引擎概述 Mysql5.5开始是默认的表存储引擎（之前尽在Window下是）。第一个完整支持ACID事务的MySQL存储引擎。其特点是行锁设计、支持MVCC、支持外键、提供一致性非锁定读，同时被设计用来最有效地利用以及使用内存和CUP。\n高性能、高可用、高扩展。\n2.2 InnoDB存储引擎的版本 略\n2.3 InnoDB体系结构 后台线程的主要作用是负责刷新内存池中的数据，保证缓冲池中的内存缓存的是最近的数据。\n此外将已修改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下InnoDB能恢复到正常运行状态。\n2.3.1 后台线程 多线程模型。不同的后台线程处理不同的任务。\nMaster Thread 将缓冲池的数据异步刷新到磁盘，保证数据一致性，包括脏页的刷新、合并插入缓冲（INSERT BUFFER）、UNDO页的回收。\nIO Thread 大量使用AIO（Async IO），极大提高性能。IO Thread主要负责这些IO请求的回调处理。\n第3章 文件 参数文件：告诉MySQL启动时查找文件的地址，指定初始化参数； 日志文件：错误日志文件、二进制文件日志文件、慢查询日志文件、查询日志文件； socket：当用UNIX域套接字方式进行连接时需要的文件 pid文件：MySQL实例的进程ID文件 MySQL表结构文件：用来存放MySQL表结构定义文件 存储引擎文件：每个存储引擎都会有自己的文件来保存各种数据，这些存储引擎真正存储了记录和索引等数据。 3.1 参数文件 MySQL实例也可以不需要参数文件，这是编译MySQL时指定的默认值。但是如果在默认的数据库目录下找不到mysql架构，则会启动失败。\n3.1.1 什么是参数 数据库中的参数是键值对。MySQL中无类似Oracle中的隐藏参数。\n3.1.2 参数类型 动态参数：运行时可修改 静态参数：在整个实例的生命周期都不能修改。 3.2 日志文件 日志文件记录了MySQL数据库的各种类型活动。\n3.2.1 错误日志 对MySQL的启动、运行、关闭过程进行了记录。当出现MySQL数据库不能正常启动时，第一个必须查找的文件应该就是错误日志文件。\n3.2.2 慢查询日志 帮助DBA定位可能存在问题的SOL语句，从而进行SQL语句层面的优化。设置一个阈值（最小精度是微秒），超过（必须是大于，等于不行）时将该SQL语句记录到慢查询日志中。\n默认情况下并不启动慢查询日志。\n如果没有使用索引也会被记录。\n3.2.3 查询日志 记录了所有对MySQL数据库请求的信息，无论这些请求是否得到了正确的执行。\n3.2.4 二进制文件 记录了对MySQL数据库执行更改的所有操作，但是不包括SELEC和SHOW这类操作。\n作用：\n恢复：某些数据的恢复需要二进制日志。 复制：主从复制 审计：通过审计二进制文件，判断是否有数据库进行注入的攻击。 二进制日志文件在默认情况下并没有启动，需要手动指定参数来启动。会影响性能，但仅仅1%。\n缓冲写宕机问题，无缓冲写宕机问题\n3.3 套接字文件 3.4 pid文件 3.5 表结构定义文件 记录该存储引擎对应的表结构\n3.6 InnoDB存储引擎文件 InnoDB独有的文件\n3.6.1 表空间文件 InnoDB采用将存储的数据按表空间进行存放的设计。\n还可设置独立表空间，用户不用将所有数据都存放于默认的表空间中。\n单独的表空间仅存储表的数据、索引和插入缓冲BITMAP等信息，其余的信息存放到默认表空间。\n3.6.2 重做日志文件 宕机问题。\n重做日志文件与二进制文件的区别：\n记录范围，二进制日志会记录所有与MySQL数据库有关的日志记录，而InnoDB仅记录该存储引擎的事务日志。 记录内容，二进制日志记录具体操作（逻辑日志），而InnoDB的重做日志文件是关于每个Page的更改的物理情况 写入时间，二进制日志文件只在事务提交前进行一次写入，而重做日志文件在事务的进行过程中，一直会被写入。 3.7 小结 重做日志文件使得InnoDB存储引擎可以提供可靠的事务。\n第4章 表 4.1 索引组织表 在InnoDB存储引擎中，表都是根据主键顺序组织存放。\n在InnoDB存储引擎中，每张表都有个主键，如果没有显示定义，则按如下方式创建：\n首先判断表中是否含有唯一非空索引（Unique NOT NULL），如果有，则该列即为主键。（按定义索引的顺序选择） 如果不符合上述条件，InnoDB存储引擎自动创建一个6字节大小的指针。 _rowid仅适用于单个列为主键的情况。\n4.2 InnoDB逻辑存储结构 表空间由段（segment），区（extent），页（page）组成。\n4.2.1 表空间 独立表空间：存放数据、索引和插入缓冲Bitmap页。\n共享表空间：存放其他类的数据，如回滚（undo）信息，插入缓冲索引页、系统事务信息，二次写缓冲。\n回滚不会导致共享表空间缩小，只是会把不需要的undo信息标记为可用空间。\n4.2.2 段 段有InnoDB存储引擎自身完成，DBA不能也没有必要控制段。常见的有数据段、索引段、回滚段等。\n数据段为B+树的叶子节点。\n索引段为B+树的非叶子节点。\n4.2.3 区 区是由连续页组成的空间，每个区大小都是1MB。\n默认页大小为16KB，一个区中有64个连续的页。\n不论页的大小如何变，区的大小都是1M。\n4.2.4 页 InnoDB管理磁盘的最小单位。\n在InnoDB存储引擎中，常见的页有：\n数据页 undo页 系统页 事务数据页 插入缓冲位图页 插入缓冲空闲列表页 未压缩的二进制大对象页 压缩的二进制大对象页 4.2.5 行 InnoDB存储引擎是面向列的，也就是说数据是按行进行存放的。\n每个页最多允许存放16KB/2-200行的记录，也就是7992行记录。\n4.3 InnoDB 行记录格式 新版本都使用Compact格式。旧版本使用Redundant格式。\n数据库实例的作用之一就是读取页中存放的行记录。\n4.3.1 Compact行记录格式 NULL值不占用空间。\n4.3.2 Redundant行记录格式 4.3.3 行溢出数据 InnoDB存储引擎可以将一条记录中的某些数据存储在真正的数据页面之外。\nVARCHAR最大长度65535字节（建表时填写的是字符数）。\n每个页中至少应该有两条行记录（否则失去了B+Tree的意义，变成了链表）。\n第6章 锁 一个开发难点：利用数据库的并发访问，同时又要确保一致性读与写。数据库系统区别于文件系统的关进特性。\n人们认为行级锁总会增加开销，实际上，只有当实现本身会增加开销时，行级锁才会增加开销。InnoDB不需要锁升级，因为一个锁和多个锁的开销时相同的。\n6.1 什么是锁？ 锁机制用于管理对共享资源的并发访问。不只是行级锁，同时会在内部其他地方使用锁。例如操作缓冲池LRU列表。\n数据库系统使用锁是为了支持对共享资源进行并发访问，提供数据的完整性和一致性。\n不同数据库实现对于锁的实现不同。\nMyISAM引擎是表锁设计。并发情况下读没有问题，但是并发插入的性能差一些。\nInnoDB存储引擎，提供一致性的非锁定读、行级锁支持。行级锁没有额外的开销，并可以同时得到并发性和一致性。\n6.2 lock和latch 都可以被称为“锁”，但含义不同。这里主要说lock。\nlatch分为mutex（互斥锁）和rwlock（读写锁）。其目的是用来保证并发线程操作资源的正确性，没有死锁检测机制。\nlock的对象是事务，锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务commit和rollback后进行释放（不同隔离级别释放时间不同）。有死锁机制。\n查看latch的命令：SHOW ENGINE INNODE MUTEX;\n6.3 InnoDB存储引擎中的锁 6.3.1 锁的类型 两种标准类型的行级锁：\n共享锁（S Lock） 排他锁（X Lock） X锁与任何的锁都不兼容，S锁仅与S锁兼容。\nInnoDB存储引擎支持多粒度锁定，允许行级锁和表级锁同时存在。为了支持不同粒度上加锁，InnoDB支持了一种额外的锁方式，意向锁（Intention Lock）。\n意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望再更细力度上进行加锁。\n即先用意向锁锁粗粒度，最后用行级锁锁住行。\n意向共享锁（IS Lock），事务想要获得一张表中某几行的共享锁。 意向排他锁（IX Lock），事务想要获得一张表中某几行的排他锁。 InnoDB支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫描以外的任何请求。\nlock_data不可信。当用户运行一个范围查找时，lock_data只返回第一行的主键值。\n6.3.2 一致性非锁定读 一致性非锁定读是指InnoDB通过多版本控制的方式来读取当前执行时间数据库中行的数据。\n如果读取的行正在执行DELETE或UPDATE操作，这是读取操作不会因此去等待行上锁的释放。而是去读取行的一个快照数据。\n非锁定是因为不需要等待X锁地释放。快照通过undo实现，没有额外开销。而且读快照不需要上锁。\nRC和RR下，InnoDB使用非锁定地一致性读。但对快照地定义不同。\nRC违反了隔离性。\n6.3.3 一致性锁定读 两种一致性锁定读操作：\nSELECT \u0026hellip; FOR UPDATE; SELECT \u0026hellip; LOCK IN SHARE MODE; SELECT \u0026hellip; FOR UPDATE;对读取地行记录加一个X锁，其他事务不能对已锁定地行加上任何锁。\nSELECT \u0026hellip; LOCK IN SHARE MODE;对读取的行记录加一个S锁，其他事务可以向被锁定的行加S锁，但是如果加X会被阻塞。\n事务提交时锁就释放了。\n6.3.4 自增长与锁 再InnoDB的内存结构中，对每个含有自增长值得表都有一个自增长计数器。当对含有自增长技术器得表进行插入操作时，这个计数器会被初始化，执行如下得语句来得到计数器的值：\nSELECT MAX(auto_inc_col) FROM t FOR UPDATE;\n插入操作将其加1。这个实现风湿称作AUTO-INC Locking。这种锁其实是采用一种特殊得表锁机制，为了提高插入得性能，锁不在事务完成后释放，而是完成对自增长得SQL语句后立即释放。\n虽然AUTO-INC Locking从一定程度上提高了并发插入得效率，但还是有性能问题。事务必须等待前一个插入的完成。其次对于INSERT\u0026hellip;SELECT 的大数量的插入会影响插入的性能，因为另一个事务中的插入会被阻塞。\n=========\n5.1.22之后对这部分进行了一个优化：提供了一种轻量级互斥量的自增长实现机制，这种机制大大提高了自增长插入的性能。\n并且从该版本开始，InnoDB存储引擎提供了一个参数innodb_autonic_lock_mode来控制自增长，默认值为1。\n自增长的插入分类图：\ninnodb_autoinc_lock_mode各个设置下对自增的影响：\n特别注意：InnoDB的自增长实现与与MyISAM不同，MyISAM存储引擎是表锁设计，自增长不用考虑并发插入的问题。\n在InnoDB中，自增长值得列必须是索引，同时必须是索引得第一个列。\n6.3.5 外键和锁 6.4 锁的算法 6.4.1 行锁的3种算法 Record Lock：单个行记录上的锁 GapLock：间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock：Record Lock + GapLock，锁定一个范围并且锁定记录本身。 Record Lock总是会去锁住索引记录，如果建表时没有设置索引，则会使用隐式的主键来进行锁定。\nNext-Key Lock锁定的区间为左开右闭区间，例如一个索引有10，11，13，20，那么可以锁定的区间有：\n采用Next-Key Lock的锁定技术称为Next-Key Locking。其设计目的时为了解决幻读。而利用这种锁定技术，锁定的时范围。\n如果事务T1已经通过Next-Key Locking锁定了如果下范围： 那么插入新的记录12时，锁定的范围会变成： 特别注意三种情况：\n就是说当查询的索引含有唯一属性，Next-Key Lock会被降级为Record Lock，即仅锁住索引本身，而不是范围。\nNext-Key Lock仅在查询的列时唯一索引的情况下。若是辅助索引，则还是使用传统的Next-Key Locking技术加锁，因为有两个索引，则需要分别锁定。对于聚集索引，则加Record Lock。而且辅助索引的下一索引加上gap lock。\n若唯一索引由多个列组成，而查询仅是查找多个唯一索引的其中一个，还是使用Next-Key Lock。\n显示的关闭Gap Lock：\n将事务的隔离级别设置为RC； 将参数innodb_locks_unsafe_for_binlog设置为1 使用INSERT INTO SELECT进行复制表时，若查询的范围被锁定则插入操作会被阻塞。\n6.4.2 解决幻象问题（幻读与不可重复读） 索引1，2，5的情况，执行SELECT * FROM t WHERE a \u0026gt; 2 FOR UPDATE;\n锁住的是 。\n如果用户通过索引查询一个值，并对该行加上了一个S Lock，那么即使查询的值不存在，其锁定的也是一个范围，因此若没有返回任何行，那么新插入的值一定是唯一的。\n若此时有并发情况则会死锁，所以并不会改变唯一性：\n6.5 锁问题 6.5.1 脏读 脏数据事务对缓冲池中行记录的修改，并且还没有被提交。\n脏读即一个事务可以读到另外一个事务中未提交的数据，违反了隔离性。\nRU下会产生脏读。主从复制的从节点的查询不需要特别精确的返回值，所以可以利用脏读。\n6.5.2 不可重复读 RC下会有，且允许不可重复读。\n6.5.3 丢失更新 当前所有的隔离级别下都不会导致丢失更行。因为对于行的DML操作都会对行或其他粗粒度级别的对象加锁。\n银行转账问题，或者超卖问题。通过加排他锁（悲观锁思想）解决。（我在秒杀系统中解决超卖使用的是乐观锁思想）\n6.6 阻塞 阻塞会导致超时，InnoDB默认不会回滚超时引发的异常，其实InnoDB在大部分情况下都不会对异常进行回滚，这个情况非常危险！！\n6.7 死锁 6.7.1 死锁的概念 死锁是指两个或两个以上的事务在执行过程中，因争夺资源而造成的一种互相等待的现象。\n超时机制，超时就回滚。\n等待图（类似操作系统中的资源分配图），主动检测死锁。深度优先遍历图，并且在后来的版本中将递归改为非递归。\n6.7.2 死锁概率 系统中事务的数量越多发生死锁的概率越大； 每个事务操作的数量越多，发生死锁的概率越大； 操作数据的集合越小发生死锁的概率越大。 死锁异常会回滚！\n6.8 锁升级 锁升级会导致锁粒度将低而导致并发性能降低。\nInnoDB不存在锁升级。InnoDB不是根据每条记录来产生行锁的，相反，其根据每个事务访问的每个页对锁进行管理，采用位图的方式。因此不管一个事务锁住页中一个记录还是多个记录，开销是一致的。\nInnoDB的加锁开销小。\n","permalink":"http://121.199.2.5:6080/232f08ac38d0478dab889420f809c20e/","summary":"第2章 InnoDB存储引擎 事务安全。\n2.1 InnoDB存储引擎概述 Mysql5.5开始是默认的表存储引擎（之前尽在Window下是）。第一个完整支持ACID事务的MySQL存储引擎。其特点是行锁设计、支持MVCC、支持外键、提供一致性非锁定读，同时被设计用来最有效地利用以及使用内存和CUP。\n高性能、高可用、高扩展。\n2.2 InnoDB存储引擎的版本 略\n2.3 InnoDB体系结构 后台线程的主要作用是负责刷新内存池中的数据，保证缓冲池中的内存缓存的是最近的数据。\n此外将已修改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下InnoDB能恢复到正常运行状态。\n2.3.1 后台线程 多线程模型。不同的后台线程处理不同的任务。\nMaster Thread 将缓冲池的数据异步刷新到磁盘，保证数据一致性，包括脏页的刷新、合并插入缓冲（INSERT BUFFER）、UNDO页的回收。\nIO Thread 大量使用AIO（Async IO），极大提高性能。IO Thread主要负责这些IO请求的回调处理。\n第3章 文件 参数文件：告诉MySQL启动时查找文件的地址，指定初始化参数； 日志文件：错误日志文件、二进制文件日志文件、慢查询日志文件、查询日志文件； socket：当用UNIX域套接字方式进行连接时需要的文件 pid文件：MySQL实例的进程ID文件 MySQL表结构文件：用来存放MySQL表结构定义文件 存储引擎文件：每个存储引擎都会有自己的文件来保存各种数据，这些存储引擎真正存储了记录和索引等数据。 3.1 参数文件 MySQL实例也可以不需要参数文件，这是编译MySQL时指定的默认值。但是如果在默认的数据库目录下找不到mysql架构，则会启动失败。\n3.1.1 什么是参数 数据库中的参数是键值对。MySQL中无类似Oracle中的隐藏参数。\n3.1.2 参数类型 动态参数：运行时可修改 静态参数：在整个实例的生命周期都不能修改。 3.2 日志文件 日志文件记录了MySQL数据库的各种类型活动。\n3.2.1 错误日志 对MySQL的启动、运行、关闭过程进行了记录。当出现MySQL数据库不能正常启动时，第一个必须查找的文件应该就是错误日志文件。\n3.2.2 慢查询日志 帮助DBA定位可能存在问题的SOL语句，从而进行SQL语句层面的优化。设置一个阈值（最小精度是微秒），超过（必须是大于，等于不行）时将该SQL语句记录到慢查询日志中。\n默认情况下并不启动慢查询日志。\n如果没有使用索引也会被记录。\n3.2.3 查询日志 记录了所有对MySQL数据库请求的信息，无论这些请求是否得到了正确的执行。\n3.2.4 二进制文件 记录了对MySQL数据库执行更改的所有操作，但是不包括SELEC和SHOW这类操作。\n作用：\n恢复：某些数据的恢复需要二进制日志。 复制：主从复制 审计：通过审计二进制文件，判断是否有数据库进行注入的攻击。 二进制日志文件在默认情况下并没有启动，需要手动指定参数来启动。会影响性能，但仅仅1%。\n缓冲写宕机问题，无缓冲写宕机问题\n3.3 套接字文件 3.4 pid文件 3.5 表结构定义文件 记录该存储引擎对应的表结构\n3.6 InnoDB存储引擎文件 InnoDB独有的文件\n3.6.1 表空间文件 InnoDB采用将存储的数据按表空间进行存放的设计。\n还可设置独立表空间，用户不用将所有数据都存放于默认的表空间中。\n单独的表空间仅存储表的数据、索引和插入缓冲BITMAP等信息，其余的信息存放到默认表空间。\n3.6.2 重做日志文件 宕机问题。\n重做日志文件与二进制文件的区别：\n记录范围，二进制日志会记录所有与MySQL数据库有关的日志记录，而InnoDB仅记录该存储引擎的事务日志。 记录内容，二进制日志记录具体操作（逻辑日志），而InnoDB的重做日志文件是关于每个Page的更改的物理情况 写入时间，二进制日志文件只在事务提交前进行一次写入，而重做日志文件在事务的进行过程中，一直会被写入。 3.7 小结 重做日志文件使得InnoDB存储引擎可以提供可靠的事务。\n第4章 表 4.1 索引组织表 在InnoDB存储引擎中，表都是根据主键顺序组织存放。\n在InnoDB存储引擎中，每张表都有个主键，如果没有显示定义，则按如下方式创建：\n首先判断表中是否含有唯一非空索引（Unique NOT NULL），如果有，则该列即为主键。（按定义索引的顺序选择） 如果不符合上述条件，InnoDB存储引擎自动创建一个6字节大小的指针。 _rowid仅适用于单个列为主键的情况。","title":"第2章 InnoDB存储引擎"},{"content":"第2章 Tomcat总体架构 系统设计及中间件设计时的参考：生命周期管理、可扩展的容器组件设计、类加载方式。\n2.1 总体设计 如何设计一个应用服务器？\n2.1.1 Server 最基本的功能：接收请求，业务处理，返回响应。\n两个方法：\nstart()：启动服务器，打开Socket链接，监听端口，负责接收请求，处理及返回。 stop()：停止服务器并释放网络资源。 作为嵌入在应用系统中的远程请求处理方案，且访问量低时可行。但作为应用服务器不可行。\n2.1.2 Connector和Container 请求监听与请求处理放到一起扩展性差。\nConnector负责监听，返回。\nContainer负责处理请求。\n均分别拥有自己的start()和stop()方法来加载和释放自己维护的资源。\n明显的缺陷：如何让Connector与Container对应？可以维护一个复杂关系映射，但是并不必需。Container设计足够灵活。\n引入Service，负责维护多个Connector和一个Container。\n在Tomcat中，Container是一个更加通用的概念。为了与Tomcat中的组件命名一致，所以重新命名为Engine，用以表示整个Servlet引擎。\nEngine表示整个Servlet引擎。Server表示整个Servlet容器。\n2.1.3 Container设计 应用服务器是用来部署并运行Web应用的，是一个运行环境，而不是独立的业务处理系统。因此需要在Engine容器中支持管理Web应用，当接收到Connector的处理请求时，Engine容器能够找到一个合适的Web应用来处理。\n使用一个Context来表示一个Web应用，并且一个Engine可以包含多个Context。\n虚拟主机，加入Host。一个Host可以包含多个Context。\nTomcat的设计中Engine可以包含Host也可以包含Context，这是由具体的Engine实现确定的。Tomcat提供的默认实现StandardEngine只能包含Host。\n一个Web应用可以包含多个Servlet实例。在Tomcat中，Servlet定义被称为Wrapper。\n“容器”的作用都是处理请求并返回响应数据。所以引入一个Container接口：addchild()添加子容器，backgroundProcess()实现文件变更的扫描。\n2.1.4 Lifecycle 所有组件均存在启动、停止这两个生命周期方法，可在此基础上扩展生命周期管理的方法，即对于生命周期管理进行一次接口抽象。\n将Server接口替换为Lifecycle接口：\nInit()：初始化组件 start()：启动组件 stop()：停止组件 destory()：销毁组件 addLifecycleListener：添加事件监听器（用于监听组件的状态变化） removeLifecycleListener：删除 Tomcat核心组件的默认实现均继承自LifecycleBeanBase抽象类，该类不但负责组件各个状态的转换和事件处理，还将组件自身注册为MBean，以便通过Tomcat的管理工具进行动态维护。\n2.1.5 Pipeline和Valve 以上设计以保证核心架构的了可伸缩性和可扩展性。但是还要考虑各个组件的灵活性，使其同样可扩展。\n责任链模式是一种比较好的选择。Tomcat即采用该模式来实现客户端请求的处理。在Tomcat中每个Container组件通过执行一个责任链来完成具体的请求处理。\nPipeline（管道）用于构造责任链，Valve（阀）代表责任链上的每个处理器。Pipeline中维护了一个基础的Valve（位于末端，最后执行）。\nTomcat的每个层级的容器（Engine、Host、Context、Wrapper）均有对应的基础Valve实现，同时维护一个Pipeline实例。即任何层级的容器都可以对请求处理进行可扩展。\n2.1.6 Connector设计 基本功能：\n监听服务器端口，读取来自客户端的请求。 将请求数据按照指定协议进行解析。 根据请求地址匹配正确的容器进行处理。 将响应返回客户端。 Tomcat支持多协议，默认支持HTTP和AJP。同时支持多种I/O方式，包括BIO（8.5之后移除）、NIO、APR。而且在Tomcat8之后新增了对NIO2和HTTP/2协议的支持。因此对协议和I/O进行抽象和建模时需要关注的重点。\n在Tomcat中，ProtocolHandler表示一个协议处理器，其包含一个Endpoint（无此接口，仅有AbstractEndpoint抽象类）用于启动Socket监听，还包含一个Processor用于按照指定协议读取数据，并将请求交由容器处理。\n在Connector启动时，Endpoint会启动线程来监听，并在接收到请求后调用Processor进行数据读取。\n当Processor读取客户端请求后，需要按照请求地址映射到具体的容器进行处理，这个过程即为请求映射。由于Tomcat各个组件采用通用的生命周期管理，而且可以通过管理工具进行状态变更，因此请求映射除考虑映射规则的实现外，还要考虑容器组件的注册与销毁。\nTomcat通过Mapper和MapperListener两个类实现上述功能。前者用于维护容器映射信息，同时按照映射规则（Servlet规范）查找容器。后者实现了ContainerListener和LifecycleListener，用于在容器组件状态发生变更时，注册或者取消对应的容器映射信息。为了实现上述功能，MapperListener实现了Lifecycle接口，当其启动时（在Service启动时启动），会自动作为监听器注册到各个容器组件上，同时将已创建的容器注册到Mapper。\nTomcat通过适配器模式（Adapter）实现了Connector与Mapper、Container的解耦。实现自己的Adapter可以脱离Servlet容器又使用Tomcat链接器。\n2.1.7 Excutor 并发问题的解决方案。采用线程池（默认采用JDK5的线程池，继承自Lifecycle，当作通用组件进行管理）对线程进行统一管理。\n在Tomcat中Excutor由Service维护，因此同一个Service中的组件可以共享一个线程池。\n如果没有定义任何线程池，相关组件（Endpoint）会自动创建线程池，此时线程池不再共享。\n在Tomcat中，Endpoint会启动一组线程来监听Socket端口，当接收到客户端请求后，会创建请求处理对象，并交由线程池处理，由此支持并发处理客户端请求。\n2.1.8 Bootstrap和Catalina 除开前面的核心组件外，还需要提供一套配置环境来支持系统的可配置性，便于通过修改配置来优化应用。\n集群、安全等组件同样重要，但不属于通用概念。\nTomcat通过类Catalina提供了一个Shell程序，用于解析server.xml创建各种组件，同时，负责启动、停止应用服务器（只需要启动Tomcat顶层组件Server）。\nTomcat使用Digester解析XML文件，包括server.xml以及web.xml等。\n最后，Tomcat提供了Bootstrap作为应用服务器启动入口。Bootstrap负责创建Catalina实例，根据执行参数调用Catalina相关方法完成针对应用服务器的操作（启动、停止）。\nBootstrap与Tomcat应用服务器完全松耦合（通过反射调用Catalina实例），它可以直接依赖JRE运行并为Tomcat应用服务器创建共享类加载器，用于构造Catalina实例以及整个Tomcat服务器。\n上述是Tomcat标准的启动方式。但是Server及其子组件代表了应用服务器本身，那么我们可以不通过Bootstrap和Catalina来启动服务器。\nTomcat组件说明：\n组件名称 说明 Server 表示整个Servlet容器，因此Tomcat运行环境中只有唯一一个Server实例 Service Service表示一个或者多个Connector的集合，这些Connector共享同一个Container来处理其请求。在同一个Tomcat实例内可以包含任意多个Service实例，它们彼此独立 Connector 即Tomcat链接器，用于监听并转化Socket请求，同时将读取的Socket请求交由Container处理，支持不同协议以及不同的I/O方式 Container Container表示能够执行客户端请求并返回响应的一类对象。在Tomcat中存在不同级别的容器：Engine、Host、Context、Warpper Engine Engine表示整个Servlet引擎。在Tomcat中，Engine为最高层级的容器对象。尽管Engine不是直接处理请求的容器，确实获取目标容器的入口 Host Hostz作为一类容器，表示Servlet容器中的虚拟机，与一个服务器的网络名有关，如域名等。客户端可以使用这个网络名连接服务器，这个名称必须在DNS服务器上注册。 Context Context作为一类容器，用于表示ServletContext，在Servlet规范中，一个ServletContext即表示一个独立的Web应用 Wrapper Wrapper作为一类容器，用于表示Web应用中动议的Servlet Executor 表示Tomcat组件可以共享的线程池 2.2 Tomcat启动 Tomcat默认实现在相关概念的基础上结合生命周期管理监听器完成了大量的启动工作。\nTomcat的启动流程非常标准化，统一按照生命周期管理接口Lifecycle的定义进行启动。首先调用init()方法进行组件的逐级初始化，然后再调用start()方法进行启动。\n每次调用均伴随着生命周期状态变更事件的触发。\n2.3 请求处理 从本质上讲，应用服务器的请求处理开始于监听的Socket端口接收到数据，结束与讲服务器处理结果写入Socket输入流。\n2.4 类加载器 2.4.1 J2SE标准了类加载器 Bootstrap Ext System 应用程序在不自己构造类加载器的情况下，使用System作为默认的类加载器。如果构造也是以System作为父类加载器。\nJVM还提供了Ensdorsed Standerds Override Mechanism机制用于允许替换JCP之外生成的API。应用程序可以提供新版本的API来覆盖JVM的实现。\n2.4.2 Tomcat加载器 应用服务器通常会自行创建类加载器以实现更灵活的控制，这一方面是对规范的实现（Servlet规范要求每个Web应用都有一个独立的类加载器实例），另一方面也有架构层面的考虑。\n隔离性：Web应用库相互隔离，避免依赖库或者应用包相互影响。 灵活性：类加载器相互独立可以实现只对容器中的某一个Web应用进行重新部署。 性能：搜索Jar包的范围小，性能高。 除了每个Web应用的类加载器外，Tomcat也提供了3个基础的类加载器和Web应用类加载器，而且这3个类加载器指向的路径和包列表均可由catalina.properties配置。\nCommon：以System为父类加载器，Tomcat中最顶层的公关类加载器。路径为common.loader，默认指向$CATALINA_HOME$/lib下的包。 Catalina：以Common为父类加载器，用于加载Tomcat应用服务器的类加载器。路径为server.loader，默认为空（此时Tomcat使用Common类加载器加载应用服务器）。 Shared：以Common为父类加载器，是所有Web应用的父类加载器。路径为shared.loader，默认为空（此时Tomcat使用Common类加载器加载应用服务器）。 Web应用：以Shared为父类加载器，加载/WEB-INF/classes目录下的未压缩的Class和资源文件以及/WEB-INF/lib目录下的jar包。该类加载器只对当前Web应用可见，对其他Web应用不可见。 默认情况下三个类加载器为同一个（Common），但可以通过配置创建三个不同的类加载器。\nCommon类加载器负责加载Tomcat应用服务器内部和Web应用均可见的类，例如Servlet规范和一些通用的包。\nCatalina类加载器负责加载只有Tomcat应用服务器内部可见的类，这些类对Web应用不可见。如Tomcat的具体实现类，因为我们的Web应用最好与服务器松耦合，故不应该以来应用服务器的内部类。\nShared类加载器服负责加载Web应用共享的类，这些类Tomcat服务器不会依赖。\n举个使用这个特性的例子：分布式Session。需要用到第三方包时，但是我们不希望这些包对Web应用可见（可能存在包版本冲突之类的问题，也可能根本不需要这些包）。此时我们可以配置server.loader，创建独立的Catalina类加载器。\n$CATALINA_HOME/bin$目录下的包作为启动入口由System类加载器加载。简化了应用服务器的启动，同时增加了灵活性。\n从架构层面分析Tomcat的类加载器方案（补充）：\n共享。通过分层实现了Jar包在各个层面及子层面的共享，同时确保了不会引入太多的包。 隔离性。区别于前者，这服务器与Web应用的隔离。理论上，出去Servlet规范定义的接口外，Web应用不应该依赖服务器的任何实现类（有利于Web应用的可移植性）。正因如此Tomcat支持通过Catalina类加载器加载服务器依赖的包，以便应用服务器与Web应用更好地隔离。 默认情况下Tomcat通过JVM安全策略许可，实现在同一个类加载器下，禁止Web应用使用服务器的相关实现类的。\n2.4.3 Web应用类加载器 双亲委派过程：\n从缓存中加载 如果缓存中没有，则从父类加载器中加载 如果父类加载器没有，则从当前类加载器加载 如果没有则抛出异常 Tomcat的委派模型与此的不同之处：当进行类加载时，除JVM基础的类库外，它会首先尝试通过当前类加载器加载，然后在进行委派。\n所以Web应用类加载器默认加载顺序如下：\n从缓存中加载 若无，JVM的Bootstrap类加载器加载 若无，则从当前类加载器加载（按照WEB-INF/classes、WEB-INF/lib的顺序） 若无，从父类加载器加载（双亲委派），加载顺序为System、Common、Shared。 Tomcat提供了delegate属性用于控制是否启用Java委派，默认false（不启用）。当配置为true时，Tomcat将使用Java默认的委派模式，则将上述3与4交换顺序。\nTomcat还可以通过packageTriggersDeny属性只让某些包路径采用Java的委派模式，Web应用类加载器对于符合packageTriggersDeny指定包路径的类强制采用Java的委派模式。\n2.5 小结 第3章 Catalina 3.1 什么是Catalina Catalina包含了前面讲到的所有容器组件，以及后续章节将会涉及的安全、会话、集群、部署、管理等Servlet容器架构的各个方面。它通过松耦合的方式集成Coyote，以及完成按照请求协议进行数据读写。同时，它还包括启动入口、Shell程序。\n3.2 Digester Catalina使用Digester解析XML（server.xml）配置文件并创建应用服务器。\nTomcat在Catalina的创建过程中通过Digester结合LifecycleListener做了大量的初始化工作。\nDigester及SAX的事件驱动，简而言之，就是通过流读取XML文件，当识别出特定XML节点后便会执行特定的动作，或者创建Java对象，或者执行对象的某个方法。因此Digester的核心是匹配模式和处理规则。\n对象栈机制用于构造Java对象。Digester是非线程安全的。\n3.2.1 对象栈 Digester的对象栈主要在匹配模式满足时，由处理规则进行操作。\nDigester的设计模式是指，在文件读取过程中，如果遇到一个XML节点的开始部分，则会出发处理规则事件创建Java对象，并将其放入栈。当处理该节点的子节点时，该对象都将维护在栈中。当遇到该节点的结束部分时，该对象将会从栈中取出并清除。\n需要解决的问题：\n如何在创建的对象之间建立关联？最终得到的结果应该是一个Java对象树。Digester提供了一个处理规则实现（SetNextRule），该规则会调用位于栈顶部对象之后对象（即父对象）得某个方法，同时将顶部对象（子对象）作为参数传入。通过此种方式不管是一对一还是一对多得关系都可创建。 如何持有创建的首个对象，即XML得传入结果？Digester对于曾经放入栈中的第一个对象将会持有一个引用，同时作为parse()方法得返回值。还有一个方式（哨兵）。Tomcat采用哨兵的方式实现。 3.2.2 匹配模式 在需要确定当读取到某个约定的XML节点时需要执行何种操作，Digester通过匹配模式指定相关约定。\n匹配类似于简单的正则，还可以使用“*”进行模糊匹配。\n当匹配模式指定多个处理规则，或者多个匹配规则匹配同一个节点时，均会出现一个节点执行多个处理规则的情况。此时，Digester的处理方式：开始读取节点时按照注册顺序执行处理规则，而完成读取时按照反向顺序执行，即先进后出的规则。\n3.2.4 处理规则 处理规则需要实现接口org.apache.commoms.digester.Rule，该接口定义了模式匹配时触发的事件方法。\n3.2.4 示例程序 由于Digester已经提供了常见处理规则的工厂方法，因此，直接嗲用相关方法即可。整个处理过程都不需要手动为何对象输行和对象间关系，不需要解析XML Dom。\n3.3 创建Server 3.3.1 Server的解析 从此处开始涉及到源码。已搭建好源码阅读环境，粗略看了一下内容过多，暂不深入。\n","permalink":"http://121.199.2.5:6080/a9dcb2a7a21d49dea965eedc7e169c79/","summary":"第2章 Tomcat总体架构 系统设计及中间件设计时的参考：生命周期管理、可扩展的容器组件设计、类加载方式。\n2.1 总体设计 如何设计一个应用服务器？\n2.1.1 Server 最基本的功能：接收请求，业务处理，返回响应。\n两个方法：\nstart()：启动服务器，打开Socket链接，监听端口，负责接收请求，处理及返回。 stop()：停止服务器并释放网络资源。 作为嵌入在应用系统中的远程请求处理方案，且访问量低时可行。但作为应用服务器不可行。\n2.1.2 Connector和Container 请求监听与请求处理放到一起扩展性差。\nConnector负责监听，返回。\nContainer负责处理请求。\n均分别拥有自己的start()和stop()方法来加载和释放自己维护的资源。\n明显的缺陷：如何让Connector与Container对应？可以维护一个复杂关系映射，但是并不必需。Container设计足够灵活。\n引入Service，负责维护多个Connector和一个Container。\n在Tomcat中，Container是一个更加通用的概念。为了与Tomcat中的组件命名一致，所以重新命名为Engine，用以表示整个Servlet引擎。\nEngine表示整个Servlet引擎。Server表示整个Servlet容器。\n2.1.3 Container设计 应用服务器是用来部署并运行Web应用的，是一个运行环境，而不是独立的业务处理系统。因此需要在Engine容器中支持管理Web应用，当接收到Connector的处理请求时，Engine容器能够找到一个合适的Web应用来处理。\n使用一个Context来表示一个Web应用，并且一个Engine可以包含多个Context。\n虚拟主机，加入Host。一个Host可以包含多个Context。\nTomcat的设计中Engine可以包含Host也可以包含Context，这是由具体的Engine实现确定的。Tomcat提供的默认实现StandardEngine只能包含Host。\n一个Web应用可以包含多个Servlet实例。在Tomcat中，Servlet定义被称为Wrapper。\n“容器”的作用都是处理请求并返回响应数据。所以引入一个Container接口：addchild()添加子容器，backgroundProcess()实现文件变更的扫描。\n2.1.4 Lifecycle 所有组件均存在启动、停止这两个生命周期方法，可在此基础上扩展生命周期管理的方法，即对于生命周期管理进行一次接口抽象。\n将Server接口替换为Lifecycle接口：\nInit()：初始化组件 start()：启动组件 stop()：停止组件 destory()：销毁组件 addLifecycleListener：添加事件监听器（用于监听组件的状态变化） removeLifecycleListener：删除 Tomcat核心组件的默认实现均继承自LifecycleBeanBase抽象类，该类不但负责组件各个状态的转换和事件处理，还将组件自身注册为MBean，以便通过Tomcat的管理工具进行动态维护。\n2.1.5 Pipeline和Valve 以上设计以保证核心架构的了可伸缩性和可扩展性。但是还要考虑各个组件的灵活性，使其同样可扩展。\n责任链模式是一种比较好的选择。Tomcat即采用该模式来实现客户端请求的处理。在Tomcat中每个Container组件通过执行一个责任链来完成具体的请求处理。\nPipeline（管道）用于构造责任链，Valve（阀）代表责任链上的每个处理器。Pipeline中维护了一个基础的Valve（位于末端，最后执行）。\nTomcat的每个层级的容器（Engine、Host、Context、Wrapper）均有对应的基础Valve实现，同时维护一个Pipeline实例。即任何层级的容器都可以对请求处理进行可扩展。\n2.1.6 Connector设计 基本功能：\n监听服务器端口，读取来自客户端的请求。 将请求数据按照指定协议进行解析。 根据请求地址匹配正确的容器进行处理。 将响应返回客户端。 Tomcat支持多协议，默认支持HTTP和AJP。同时支持多种I/O方式，包括BIO（8.5之后移除）、NIO、APR。而且在Tomcat8之后新增了对NIO2和HTTP/2协议的支持。因此对协议和I/O进行抽象和建模时需要关注的重点。\n在Tomcat中，ProtocolHandler表示一个协议处理器，其包含一个Endpoint（无此接口，仅有AbstractEndpoint抽象类）用于启动Socket监听，还包含一个Processor用于按照指定协议读取数据，并将请求交由容器处理。\n在Connector启动时，Endpoint会启动线程来监听，并在接收到请求后调用Processor进行数据读取。\n当Processor读取客户端请求后，需要按照请求地址映射到具体的容器进行处理，这个过程即为请求映射。由于Tomcat各个组件采用通用的生命周期管理，而且可以通过管理工具进行状态变更，因此请求映射除考虑映射规则的实现外，还要考虑容器组件的注册与销毁。\nTomcat通过Mapper和MapperListener两个类实现上述功能。前者用于维护容器映射信息，同时按照映射规则（Servlet规范）查找容器。后者实现了ContainerListener和LifecycleListener，用于在容器组件状态发生变更时，注册或者取消对应的容器映射信息。为了实现上述功能，MapperListener实现了Lifecycle接口，当其启动时（在Service启动时启动），会自动作为监听器注册到各个容器组件上，同时将已创建的容器注册到Mapper。\nTomcat通过适配器模式（Adapter）实现了Connector与Mapper、Container的解耦。实现自己的Adapter可以脱离Servlet容器又使用Tomcat链接器。\n2.1.7 Excutor 并发问题的解决方案。采用线程池（默认采用JDK5的线程池，继承自Lifecycle，当作通用组件进行管理）对线程进行统一管理。\n在Tomcat中Excutor由Service维护，因此同一个Service中的组件可以共享一个线程池。\n如果没有定义任何线程池，相关组件（Endpoint）会自动创建线程池，此时线程池不再共享。\n在Tomcat中，Endpoint会启动一组线程来监听Socket端口，当接收到客户端请求后，会创建请求处理对象，并交由线程池处理，由此支持并发处理客户端请求。\n2.1.8 Bootstrap和Catalina 除开前面的核心组件外，还需要提供一套配置环境来支持系统的可配置性，便于通过修改配置来优化应用。\n集群、安全等组件同样重要，但不属于通用概念。\nTomcat通过类Catalina提供了一个Shell程序，用于解析server.xml创建各种组件，同时，负责启动、停止应用服务器（只需要启动Tomcat顶层组件Server）。\nTomcat使用Digester解析XML文件，包括server.xml以及web.xml等。\n最后，Tomcat提供了Bootstrap作为应用服务器启动入口。Bootstrap负责创建Catalina实例，根据执行参数调用Catalina相关方法完成针对应用服务器的操作（启动、停止）。\nBootstrap与Tomcat应用服务器完全松耦合（通过反射调用Catalina实例），它可以直接依赖JRE运行并为Tomcat应用服务器创建共享类加载器，用于构造Catalina实例以及整个Tomcat服务器。\n上述是Tomcat标准的启动方式。但是Server及其子组件代表了应用服务器本身，那么我们可以不通过Bootstrap和Catalina来启动服务器。\nTomcat组件说明：\n组件名称 说明 Server 表示整个Servlet容器，因此Tomcat运行环境中只有唯一一个Server实例 Service Service表示一个或者多个Connector的集合，这些Connector共享同一个Container来处理其请求。在同一个Tomcat实例内可以包含任意多个Service实例，它们彼此独立 Connector 即Tomcat链接器，用于监听并转化Socket请求，同时将读取的Socket请求交由Container处理，支持不同协议以及不同的I/O方式 Container Container表示能够执行客户端请求并返回响应的一类对象。在Tomcat中存在不同级别的容器：Engine、Host、Context、Warpper Engine Engine表示整个Servlet引擎。在Tomcat中，Engine为最高层级的容器对象。尽管Engine不是直接处理请求的容器，确实获取目标容器的入口 Host Hostz作为一类容器，表示Servlet容器中的虚拟机，与一个服务器的网络名有关，如域名等。客户端可以使用这个网络名连接服务器，这个名称必须在DNS服务器上注册。 Context Context作为一类容器，用于表示ServletContext，在Servlet规范中，一个ServletContext即表示一个独立的Web应用 Wrapper Wrapper作为一类容器，用于表示Web应用中动议的Servlet Executor 表示Tomcat组件可以共享的线程池 2.2 Tomcat启动 Tomcat默认实现在相关概念的基础上结合生命周期管理监听器完成了大量的启动工作。","title":"第2章 Tomcat总体架构"},{"content":"第6章 深入分析ClassLoader工作机制 三个作用：\n将Class加载到JVM中； 审查每个类应该由谁加载（双亲委派）； 将CLass字节码重新解析成JVM统一要求的格式。 6.1 ClassLoader类结构解析 defineClass方法：将byte字节流解析成JVM能够识别的Class对象。意味着不仅仅可以通过class文件实例化对象。调用此方法生成的Class对象还没有resolve，resolve会在对象真正实例化时才进行。\nfindClass方法：通过直接覆盖ClassLoader父类的findClass方法来实现类的加载规则，从而取得要加载的字节码。然后调用defineClass方法生成类的Class对象。如果想在类被加载到JVM的时候就被链接（Link），那么可以接着调用另外一个resolveClass方法。\n如果想实现自己的ClassLader，一般都会继承URLClassLoader。\n6.2 ClassLoader的等级加载机制 双亲委派。\n（1）Bootstrap ClassLoader，主要加载JVM自身工作需要的类，完全由JVM自己控制。（既没有更高一级的父加载器，也没有子加载器）。\n（2）ExtClassLoader，并不是JVM亲自实现，加载System.getProperty(“java.ext.dirs”)目录下的类。\n（3）AppClassLoader，父类是ExtClassLoader。加载System.getProperty(“java.class.path”)目录下的类都可以被其加载。\n实现自己的类加载器，都必须最终调用getSystemClassLoader()作为父加载器。而此方法获取到的就是AppClassLoader。\n注意Bootstrap ClassLoader并不是如其他文章所说，而是其并无子类也无父类。ExtClassLoader并没有父类加载器。\nExtClassLoader和AppClassLoader都继承了URLClassloader类，而URLClassLoader又实现了抽象类ClassLoader，在创建Launcher对象时会首先创建ExtClassLoader，然后讲ExtClassLoader对象作为父加载器创建AppClassLoader对象。所以如果在Java应用中没有定义其他的ClassLoader，那么除了System.getProperty(“java.ext.dirs”)目录下的类是由ExtClassloader加载，其他类都是由AppClassLoader加载。\n加载class文件到内存的两种方式：隐式，显式。\n6.3 如何加载class文件 加载、验证、准备、解析、初始化。\n第13章 Spring框架的设计理念与设计模式分析 13.1 Spring的骨骼架构 三个核心组件：Core、Context和Bean。\n13.1.1 Spring的设计理念 最核心：Bean。（面向Bean编程）\n解决了一个关键问题：把对象之间的依赖关系转而用配置文件来管理（依赖注入）。\nSpring通过把对象包装在Bean中，从而达到管理这些对象及做一系列额外操作的目的。\n这种设计策略完全类似于OOP的设计理念。构建一个数据结构，然后根据这个数据结构设计它的生存环境，并让它在这个环境中按照一定的规律不停地运动，在它们地不停运动中设计一个系列于环境或者与其他各地完成信息交换。（同时也是大多数框架地设计理念）\n13.1.12 核心组件如何协同工作 Context负责发现每个Bean之间的关系，建立关系并且维护关系。所以Context就是一个Bean关系的集合，也叫Ioc容器。\nCore就是发现、建立和维护每个Bean之间的关系所需要的一系列工具。（也就是一些Util）\n13.2 核心组件详解 13.2.1 Bean组件 包：org.springframework.beans。这个包下的类主要解决三件事：Bean的定义、Bean的创建及对Bean的解析。（使用者只需关心创建）\nSpring是典型的工厂模式，工厂的继承层次关系图如下：\n顶级接口BeanFactory有3个子接口：ListableBeanFactory、HierarchicalBeanFantory和AutowireCapableBeanFactory。\nDefaultListableBeanFactory实现了所有的结构。\n定义多接口的原因：每个接口有不同的使用场景，主要是为了区分Spring内部对象的传递和转化过程中，对对象的数据访问所作的限制。例如，ListableBeanFactory接口标识这些Bean是可列表的，HierarchicalBeanFactory表示这些Bean是有继承关系的，AutowireCapableBeanFactory接口定义Bean的自动装配规则。4个接口共同定义了Bean的集合、Bean之间的关系和Bean的行为。\nBean的定义主要有Beandefinition描述：\nBean是配置文件信息中\u0026lt;bean/\u0026gt;节点信息的转化。Spring解析完成后，内部就是一个BeanDefinition对象。\nBean的解析过程过于繁琐，不赘述。\n13.2.2 Context组件 ApplicationContext继承了BeanFactory。\nApplicationContext的子类主要包含两个方面：\nConfigurableApplicationContext表示该Context是可修改的，也就是构建Context中，用户可以动态添加或修改已有的配置信息。 WebApplication，用于Web，可以直接访问Servletcontext。 ApplicationContext必须完成的事情：\n标识一个应用环境 利用BeanFactory创建Bean对象 保存对象关系表 能够捕获各种事件 13.2.3 Core组件 其中有很多关键类，一个重要的组成部分就是定义了资源的访问方式。\nResource类相关：封装了各种可能的资源类型，也就是说对使用者来说屏蔽了文件类型的不同。通过继承InputStreamSource接口，在这个接口中有个getInputStream方法，返回InputStream类，所有资源都可以通过InputStream来获取，及屏蔽了资源的提供者。\nContext把资源的加载、解析和描述工作委托给了ResourcePatternResolver类来完成。\n13.2.4 Ioc容器如何工作 如何创建BeanFactory工厂 refresh方法。源码已阅就不贴了。步骤如下：\n（1）构建BeanFactory\n（2）注册可能感兴趣的事件\n（3）创建Bean实例对象\n（4）出发被监听的事件\n如何创建Bean实例并构建Bean的关系网 详见源码。\nIoc容器的扩展点 BeanFactoryPostProcessor和BeanPostProcessor，分别在构建BeanFactory和构建Be\u0026rsquo;an对象时调用。还有就是InitPostProcessor和DisposableBean，它们分别在Bean实例创建和销毁时被调用。用户可以实现在这些接口中定义的方法，Spring会在适当的时候调用他们。还有一个是FactoryBean。（会扩展是精通Spring的第一步）\nIoc容器如何为我所用 扩展点。通过扩展点来改变Spring的通用行为。（AOP是一个例子，可以作为参考）\n13.3 Spring中AOP的特性详解 13.3.1 动态代理的实现原理 java.lang.reflect.Proxy。\n重点看公有方法。\n阅读源码部分略。\n13.2.2 Spring AOP如何实现 13.4 设计模式解析之代理模式 给某一个对象创建一个代理对象，有代理对象控制对原对象的引用，而创建代理对象之后可以再调用时增加一些额外的操作。\n13.5 设计模式解析之策略模式 CGLIB与JDK动态代理的选择，就是策略模式的一种实现。\n","permalink":"http://121.199.2.5:6080/eec901917e884002a43f917c02f47dd9/","summary":"第6章 深入分析ClassLoader工作机制 三个作用：\n将Class加载到JVM中； 审查每个类应该由谁加载（双亲委派）； 将CLass字节码重新解析成JVM统一要求的格式。 6.1 ClassLoader类结构解析 defineClass方法：将byte字节流解析成JVM能够识别的Class对象。意味着不仅仅可以通过class文件实例化对象。调用此方法生成的Class对象还没有resolve，resolve会在对象真正实例化时才进行。\nfindClass方法：通过直接覆盖ClassLoader父类的findClass方法来实现类的加载规则，从而取得要加载的字节码。然后调用defineClass方法生成类的Class对象。如果想在类被加载到JVM的时候就被链接（Link），那么可以接着调用另外一个resolveClass方法。\n如果想实现自己的ClassLader，一般都会继承URLClassLoader。\n6.2 ClassLoader的等级加载机制 双亲委派。\n（1）Bootstrap ClassLoader，主要加载JVM自身工作需要的类，完全由JVM自己控制。（既没有更高一级的父加载器，也没有子加载器）。\n（2）ExtClassLoader，并不是JVM亲自实现，加载System.getProperty(“java.ext.dirs”)目录下的类。\n（3）AppClassLoader，父类是ExtClassLoader。加载System.getProperty(“java.class.path”)目录下的类都可以被其加载。\n实现自己的类加载器，都必须最终调用getSystemClassLoader()作为父加载器。而此方法获取到的就是AppClassLoader。\n注意Bootstrap ClassLoader并不是如其他文章所说，而是其并无子类也无父类。ExtClassLoader并没有父类加载器。\nExtClassLoader和AppClassLoader都继承了URLClassloader类，而URLClassLoader又实现了抽象类ClassLoader，在创建Launcher对象时会首先创建ExtClassLoader，然后讲ExtClassLoader对象作为父加载器创建AppClassLoader对象。所以如果在Java应用中没有定义其他的ClassLoader，那么除了System.getProperty(“java.ext.dirs”)目录下的类是由ExtClassloader加载，其他类都是由AppClassLoader加载。\n加载class文件到内存的两种方式：隐式，显式。\n6.3 如何加载class文件 加载、验证、准备、解析、初始化。\n第13章 Spring框架的设计理念与设计模式分析 13.1 Spring的骨骼架构 三个核心组件：Core、Context和Bean。\n13.1.1 Spring的设计理念 最核心：Bean。（面向Bean编程）\n解决了一个关键问题：把对象之间的依赖关系转而用配置文件来管理（依赖注入）。\nSpring通过把对象包装在Bean中，从而达到管理这些对象及做一系列额外操作的目的。\n这种设计策略完全类似于OOP的设计理念。构建一个数据结构，然后根据这个数据结构设计它的生存环境，并让它在这个环境中按照一定的规律不停地运动，在它们地不停运动中设计一个系列于环境或者与其他各地完成信息交换。（同时也是大多数框架地设计理念）\n13.1.12 核心组件如何协同工作 Context负责发现每个Bean之间的关系，建立关系并且维护关系。所以Context就是一个Bean关系的集合，也叫Ioc容器。\nCore就是发现、建立和维护每个Bean之间的关系所需要的一系列工具。（也就是一些Util）\n13.2 核心组件详解 13.2.1 Bean组件 包：org.springframework.beans。这个包下的类主要解决三件事：Bean的定义、Bean的创建及对Bean的解析。（使用者只需关心创建）\nSpring是典型的工厂模式，工厂的继承层次关系图如下：\n顶级接口BeanFactory有3个子接口：ListableBeanFactory、HierarchicalBeanFantory和AutowireCapableBeanFactory。\nDefaultListableBeanFactory实现了所有的结构。\n定义多接口的原因：每个接口有不同的使用场景，主要是为了区分Spring内部对象的传递和转化过程中，对对象的数据访问所作的限制。例如，ListableBeanFactory接口标识这些Bean是可列表的，HierarchicalBeanFactory表示这些Bean是有继承关系的，AutowireCapableBeanFactory接口定义Bean的自动装配规则。4个接口共同定义了Bean的集合、Bean之间的关系和Bean的行为。\nBean的定义主要有Beandefinition描述：\nBean是配置文件信息中\u0026lt;bean/\u0026gt;节点信息的转化。Spring解析完成后，内部就是一个BeanDefinition对象。\nBean的解析过程过于繁琐，不赘述。\n13.2.2 Context组件 ApplicationContext继承了BeanFactory。\nApplicationContext的子类主要包含两个方面：\nConfigurableApplicationContext表示该Context是可修改的，也就是构建Context中，用户可以动态添加或修改已有的配置信息。 WebApplication，用于Web，可以直接访问Servletcontext。 ApplicationContext必须完成的事情：\n标识一个应用环境 利用BeanFactory创建Bean对象 保存对象关系表 能够捕获各种事件 13.2.3 Core组件 其中有很多关键类，一个重要的组成部分就是定义了资源的访问方式。\nResource类相关：封装了各种可能的资源类型，也就是说对使用者来说屏蔽了文件类型的不同。通过继承InputStreamSource接口，在这个接口中有个getInputStream方法，返回InputStream类，所有资源都可以通过InputStream来获取，及屏蔽了资源的提供者。\nContext把资源的加载、解析和描述工作委托给了ResourcePatternResolver类来完成。\n13.2.4 Ioc容器如何工作 如何创建BeanFactory工厂 refresh方法。源码已阅就不贴了。步骤如下：\n（1）构建BeanFactory\n（2）注册可能感兴趣的事件\n（3）创建Bean实例对象\n（4）出发被监听的事件\n如何创建Bean实例并构建Bean的关系网 详见源码。\nIoc容器的扩展点 BeanFactoryPostProcessor和BeanPostProcessor，分别在构建BeanFactory和构建Be\u0026rsquo;an对象时调用。还有就是InitPostProcessor和DisposableBean，它们分别在Bean实例创建和销毁时被调用。用户可以实现在这些接口中定义的方法，Spring会在适当的时候调用他们。还有一个是FactoryBean。（会扩展是精通Spring的第一步）\nIoc容器如何为我所用 扩展点。通过扩展点来改变Spring的通用行为。（AOP是一个例子，可以作为参考）\n13.3 Spring中AOP的特性详解 13.3.1 动态代理的实现原理 java.lang.reflect.Proxy。\n重点看公有方法。\n阅读源码部分略。\n13.2.2 Spring AOP如何实现 13.4 设计模式解析之代理模式 给某一个对象创建一个代理对象，有代理对象控制对原对象的引用，而创建代理对象之后可以再调用时增加一些额外的操作。\n13.5 设计模式解析之策略模式 CGLIB与JDK动态代理的选择，就是策略模式的一种实现。","title":"第6章 深入分析ClassLoader工作机制"},{"content":"6.4字节码指令简介 Java虚拟机的指令由一个字节长度的、代表着某种特定操作含义的数字（称为操作码，Opcode）以及跟随其后的零至多个代表此操作所需参数（称为操作数，Operands）而构成。由于Java虚拟机采用面向操作数栈而不是寄存器的架构，所以大多数的指令都不包含操作数，而只有一个操作码。\n6.4.1 字节码与数据类型 Java虚拟机的指令集中，大多数的指令都包含了其操作数所对应的数据类型信息。\n如果每一种与数据类型相关的指令都支持Java虚拟机所有运行时数据类型的话，那指令的数量恐怕就会超出一个字节所能表示的数量范围了\n前面的被系统吞了 之后再补上\n第8章 虚拟机字节码执行系统 8.1概述 8.2 运行时栈帧结构 8.2.1 局部变量表 是一组变量存储空间，用于存放方法参数和方法内部定义的局部变量。\n用于存放方法参数和方法内部定义的局部变量。\n容量以变量槽（Variable Slot）为最小单位，虚拟机规范中并没有明确指明一个Slot应占用的内存空间大小，知识很有导向性地说道每个Slot都应该能存放一个boolean、byte、char、short、int、float、reference或returnAddress类型的数据。\n只要保证计时在64位虚拟机中使用了64位的物理内存空间去实现一个Slot，虚拟机仍要使用对齐和不败的手段让Slot在外观上看起来与32位虚拟机中的一致。\nJava中占用32位以内的数据类型有boolean、byte、char、short、int、float、reference和returnAddress8种类型。（Java语言与Java虚拟机种的剧本数据类型是存在本质差别的）。reference类型表示对一个对象实例的引用。虚拟机规范没有指明长度和结构。但需要做到如下两点：\n从引用中直接或间接地查找到对象在Java堆中的数据存放的起始地址索引； 引用中直接或简介地查找到对象所属数据类型在方法区中地存储的类型信息，否则无法实现Java语言规范中定义的语法约束。 returnAddress类型目前已经很少见了，为字节码指令jsr、jsr_w和ret服务的，指向一条字节码指令的地址，很古老的Java虚拟机曾经使用这几条指令来实现异常处理，现在已经由异常表替代。\nJava中明确的64位的数据类型只有long和double两种。分割存储的做法与“long和double的非原子性协定”类似。\n但在局部变量表中不会引起数据安全问题（线程私有）。\n索引定位。访问32位数据类型的变量，索引n就代表了使用第n个Slot。64位则会同时使用n和n+1两个Slot。对于两个相邻的共同存放一个64位数据的两个Slot，不允许采用任何方式单独访问其中的某一个。\n在方法执行时，如果执行的实例（非static），局部变量表中第0位索引的Slot默认时用于传递方法所属对象实例的引用，在方法中可以通过关键字“this”来访问到这个隐含的参数。\n为了尽可能节省栈帧空间，局部变量表中的Slot是可以重用的，方法中定义的变量，其作用域并不一定会覆盖整个方法体。副作用：某些情况下会直接影响到GC。\n实例，placeholder能否被回收的根本原因是：局部变量表中的Slot是否还存有关于placeholder数组对象的引用。\n局部变量表是GC Roots的一部分。把不用的占用了大量内存的变量手动设置为null值。\n但冲编码角度讲，以恰当的变量作用域来控制变量回收时间才是最优雅的解决方法。\n———————————待补充———————————\n局部变量不负初值会编译不通过。\n","permalink":"http://121.199.2.5:6080/499b1754f0844c299d9bf24652161845/","summary":"6.4字节码指令简介 Java虚拟机的指令由一个字节长度的、代表着某种特定操作含义的数字（称为操作码，Opcode）以及跟随其后的零至多个代表此操作所需参数（称为操作数，Operands）而构成。由于Java虚拟机采用面向操作数栈而不是寄存器的架构，所以大多数的指令都不包含操作数，而只有一个操作码。\n6.4.1 字节码与数据类型 Java虚拟机的指令集中，大多数的指令都包含了其操作数所对应的数据类型信息。\n如果每一种与数据类型相关的指令都支持Java虚拟机所有运行时数据类型的话，那指令的数量恐怕就会超出一个字节所能表示的数量范围了\n前面的被系统吞了 之后再补上\n第8章 虚拟机字节码执行系统 8.1概述 8.2 运行时栈帧结构 8.2.1 局部变量表 是一组变量存储空间，用于存放方法参数和方法内部定义的局部变量。\n用于存放方法参数和方法内部定义的局部变量。\n容量以变量槽（Variable Slot）为最小单位，虚拟机规范中并没有明确指明一个Slot应占用的内存空间大小，知识很有导向性地说道每个Slot都应该能存放一个boolean、byte、char、short、int、float、reference或returnAddress类型的数据。\n只要保证计时在64位虚拟机中使用了64位的物理内存空间去实现一个Slot，虚拟机仍要使用对齐和不败的手段让Slot在外观上看起来与32位虚拟机中的一致。\nJava中占用32位以内的数据类型有boolean、byte、char、short、int、float、reference和returnAddress8种类型。（Java语言与Java虚拟机种的剧本数据类型是存在本质差别的）。reference类型表示对一个对象实例的引用。虚拟机规范没有指明长度和结构。但需要做到如下两点：\n从引用中直接或间接地查找到对象在Java堆中的数据存放的起始地址索引； 引用中直接或简介地查找到对象所属数据类型在方法区中地存储的类型信息，否则无法实现Java语言规范中定义的语法约束。 returnAddress类型目前已经很少见了，为字节码指令jsr、jsr_w和ret服务的，指向一条字节码指令的地址，很古老的Java虚拟机曾经使用这几条指令来实现异常处理，现在已经由异常表替代。\nJava中明确的64位的数据类型只有long和double两种。分割存储的做法与“long和double的非原子性协定”类似。\n但在局部变量表中不会引起数据安全问题（线程私有）。\n索引定位。访问32位数据类型的变量，索引n就代表了使用第n个Slot。64位则会同时使用n和n+1两个Slot。对于两个相邻的共同存放一个64位数据的两个Slot，不允许采用任何方式单独访问其中的某一个。\n在方法执行时，如果执行的实例（非static），局部变量表中第0位索引的Slot默认时用于传递方法所属对象实例的引用，在方法中可以通过关键字“this”来访问到这个隐含的参数。\n为了尽可能节省栈帧空间，局部变量表中的Slot是可以重用的，方法中定义的变量，其作用域并不一定会覆盖整个方法体。副作用：某些情况下会直接影响到GC。\n实例，placeholder能否被回收的根本原因是：局部变量表中的Slot是否还存有关于placeholder数组对象的引用。\n局部变量表是GC Roots的一部分。把不用的占用了大量内存的变量手动设置为null值。\n但冲编码角度讲，以恰当的变量作用域来控制变量回收时间才是最优雅的解决方法。\n———————————待补充———————————\n局部变量不负初值会编译不通过。","title":"第8章 虚拟机字节码执行系统"},{"content":"本书特色 结合JDK源码介绍了Java并发框架、线程池的实现原理。\n不仅仅局限于Java层面，更深入JVM，CPU。\n结合线上应用，给出了一些并发编程实战技巧。\n第一章 并发编程的挑战 如果希望通过多线程执行任务让程序运行得更快，会面临非常多得挑战，如上下文切换的问题、死锁的问题，以及受限于硬件和软件的资源限制问题。\n1.1 上下文切换 单核处理器支持多线程执行代码，CPU通过给每个线程分配COU时间片来实现。\n时间片非常短，所以CPU通过不停的切换线程执行，让我们感觉多个线程是同时执行的，几十毫秒ms。\nCPU通过时间片分配算法来循环执行任务。任务从保存到加载的过程就是一次上下文切换。\n上下文切换回影响多线程的执行速度。\n1.1.1 多线程一定快吗 累加，并发执行的速度比串行慢是因为线程有创建和上下文切换的开销。\n1.1.2 测试上下文切换次数和时长 Lmbech3可以测量上下文切换的时长。\ncmstat可以测量上下文切换的次数。\nCS(Content Switch)表示上下文切换的次数。\n1.1.3 如何减少上下文切换 较少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。\n无所并发编程：多线程竞争锁时，回引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样回造成大量线程都处于等待状态。 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 1.1.4 减少上下文切换实战 通过见扫线上大量WAITING的线程，来减少上下文切换次数。\n1.2 死锁 避免死锁的几个常见方法：\n避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的的情况。 1.3 资源限制的挑战 （1）什么是资源限制\n资源是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。\n硬件资源限制：带宽的上传/下载速读、硬盘读写速度和CPU的处理速度。\n软件资源限制：数据库的连接数和scoket连接数。\n（2）资源限制引发的问题\n在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行，但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的问题。\n（3）如何解决资源限制的问题\n对于硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多级上运行。比如使用ODPS、Hadoop或者自己搭建服务器集群，不同的机器吹不同的数据。可以通过“数据ID%机器数”，计算得到一个机器编号，然后由对应编号的机器处理这笔数据。\n对于软件资源限制，可以考虑使用资源池将资源复用。比如使用连接池将数据库和Socket连接复用，或者调用对方webservice接口获取数据时，只建立一个连接。\n（4）在资源限制情况下进行并发编程\n如何在资源限制的情况下，让程序执行得更快呢？方法就是，根据不同得资源限制调整程序得并发度。\n1.4 本章小结 本章介绍了在进行并发编程时会遇到的几个挑战，并给出了一些建议。\n第二章 Java并发机制的底层实现原理 JVM执行字节码，最终需要转化为汇编指令在CPU上执行，Java中所使用得并发机制依赖于JVM得实现和CPU的指令。\n2.1 volatile的应用 1.volatile的定义与实现原理 volatile是轻量级的synchronized，在多处理器开发中保证了共享变量的“可见性”。使用得当的话，比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。本文分析Intel处理器是如何实现volatile的。\nCPU的术语定义：\n术语 英文单词 术语描述 内存屏障 memory barriers 是一组处理器指令，用于事项对内存操作的顺序限制 缓冲行 cache line 缓存中可以分配的最小存储单位。处理器填写缓存线时会加载整个缓存线，需要使用多个主内存读周期 原子操作 atomic line 不可终端的一个或一系列操作 缓存行填充 cache line fill 当处理器试别到内存中读取操作数时可缓存的，处理器读取整个缓存行到适当的缓存（L1,L2,L3的或所有） 缓存命中 cache hit 如果进行高速缓存行填充操作的内存位置仍然是下次处理器访问的地址时，处理器从缓存中读取操作数，而不是从内存读取 写命中 write hit 当处理器将操作数写回到一个内存缓存的区域时，它首先回检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则处理器将这个操作数写回到缓存，而不是写回到内存，这个操作被称为写命中 写缺失 write misses the cache 一个有效的缓存行被写入到不存在的内存区域 为了提高处理速度，处理器不直接个内存进行通信，二十先将系统内存的数据督导内部缓存（L1,L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会像处理器发送一条Lock前缀的指令，将这个变量所在缓存的数据写回到系统内存。但是就算写回到内存，如过其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，再多处理器下，未了保证各个处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的换粗你一直，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，回重新从系统内存中把数据读到处理器缓存里。\nLock前缀的指令在多核处理器下会引发两件事情：\n将当前处理器缓存行的数据写回到系统内存 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效 volatile的两条实现原则：\nLock前缀指令会引起处理器缓存回写到内存 一个处理器的缓存回写到内存会导致其他处理器的缓存无效 2.volatile的使用优化 追加字节到64字节，因为高速缓存行普遍是64字节宽，不支持部分填充缓存行。如果队列的头节点和尾节点都不足64字节的话，处理器会将它们都读到同一个高速缓存行中，在多处理器下每个处理器都会缓存同样的头、尾节点，当一个处理器试图修改头节点时，会将整个缓存行锁定，那么在缓存一致性机制的作用下，会导致其他处理器不能访问自己告诉缓存中的尾节点，而队列的入队和出队操作则需要不停修改头节点和尾节点，所以在多处理器的情况下将会严重影响到队列的入队和出队效率。\n追加到64字节的方式来填满告诉缓冲区的缓存行，避免头节点和尾节点加载到同一个缓存行，使头，尾节点在修改时不会互相锁定。\n一下两种场景不适用：\n缓存行非64字节宽的处理器。 共享变量不会被频繁地写。 Java7之后不能生效，会淘汰或重新排列无用字段，需要采用其他追加字节的方式。\n2.2 synchronized的实现原理与应用 syschronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现以下3种形式：\n对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是Synchronized括号里配置的对象。 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。\nJVM基于进入和退出Monitor对象来实现方法同步和代码块同步。\nmonnitorenter指令===\u0026gt;同步代码块开头，monitorexit===\u0026gt;方法结束处和异常处。两者配对。任何对象都有一个monitor与之关联，当一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。\n2.2.1 Java对象头 synchronized用的锁时存在Java对象头里的。\n长度 内容 说明 32/64bit Mark Word 存储对象的hashCode或锁信息 32/64bit Class Metadata Address 存储到对象类型数据的指针 32/32bit Array leng 数组的长度（如果当前对象是数组） Mark Word的默认存储结构：\n锁状态 25bit 4bit 1bit偏向锁 2bit锁标志位 无锁状态 对象的hashCode 对象分代年龄 0 01 运行期，Mark Word里存储的数据会随着锁标志位的变化而变化。\n2.2.2 锁的升级与对比 锁从低到高四个状态：无状态所、偏向锁状态、轻量级锁状态和重量级锁状态。为了提高获得锁和释放锁的效率，锁可以升级但不能降级。\n偏向锁 目的：为了让线程获得锁的代价更低。\n当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头地Mar Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。\n（1）偏向锁的撤销\n","permalink":"http://121.199.2.5:6080/f6ad1487667e4880973e3b21505810be/","summary":"本书特色 结合JDK源码介绍了Java并发框架、线程池的实现原理。\n不仅仅局限于Java层面，更深入JVM，CPU。\n结合线上应用，给出了一些并发编程实战技巧。\n第一章 并发编程的挑战 如果希望通过多线程执行任务让程序运行得更快，会面临非常多得挑战，如上下文切换的问题、死锁的问题，以及受限于硬件和软件的资源限制问题。\n1.1 上下文切换 单核处理器支持多线程执行代码，CPU通过给每个线程分配COU时间片来实现。\n时间片非常短，所以CPU通过不停的切换线程执行，让我们感觉多个线程是同时执行的，几十毫秒ms。\nCPU通过时间片分配算法来循环执行任务。任务从保存到加载的过程就是一次上下文切换。\n上下文切换回影响多线程的执行速度。\n1.1.1 多线程一定快吗 累加，并发执行的速度比串行慢是因为线程有创建和上下文切换的开销。\n1.1.2 测试上下文切换次数和时长 Lmbech3可以测量上下文切换的时长。\ncmstat可以测量上下文切换的次数。\nCS(Content Switch)表示上下文切换的次数。\n1.1.3 如何减少上下文切换 较少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。\n无所并发编程：多线程竞争锁时，回引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样回造成大量线程都处于等待状态。 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 1.1.4 减少上下文切换实战 通过见扫线上大量WAITING的线程，来减少上下文切换次数。\n1.2 死锁 避免死锁的几个常见方法：\n避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的的情况。 1.3 资源限制的挑战 （1）什么是资源限制\n资源是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。\n硬件资源限制：带宽的上传/下载速读、硬盘读写速度和CPU的处理速度。\n软件资源限制：数据库的连接数和scoket连接数。\n（2）资源限制引发的问题\n在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行，但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的问题。\n（3）如何解决资源限制的问题\n对于硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多级上运行。比如使用ODPS、Hadoop或者自己搭建服务器集群，不同的机器吹不同的数据。可以通过“数据ID%机器数”，计算得到一个机器编号，然后由对应编号的机器处理这笔数据。\n对于软件资源限制，可以考虑使用资源池将资源复用。比如使用连接池将数据库和Socket连接复用，或者调用对方webservice接口获取数据时，只建立一个连接。\n（4）在资源限制情况下进行并发编程\n如何在资源限制的情况下，让程序执行得更快呢？方法就是，根据不同得资源限制调整程序得并发度。\n1.4 本章小结 本章介绍了在进行并发编程时会遇到的几个挑战，并给出了一些建议。\n第二章 Java并发机制的底层实现原理 JVM执行字节码，最终需要转化为汇编指令在CPU上执行，Java中所使用得并发机制依赖于JVM得实现和CPU的指令。\n2.1 volatile的应用 1.volatile的定义与实现原理 volatile是轻量级的synchronized，在多处理器开发中保证了共享变量的“可见性”。使用得当的话，比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。本文分析Intel处理器是如何实现volatile的。\nCPU的术语定义：\n术语 英文单词 术语描述 内存屏障 memory barriers 是一组处理器指令，用于事项对内存操作的顺序限制 缓冲行 cache line 缓存中可以分配的最小存储单位。处理器填写缓存线时会加载整个缓存线，需要使用多个主内存读周期 原子操作 atomic line 不可终端的一个或一系列操作 缓存行填充 cache line fill 当处理器试别到内存中读取操作数时可缓存的，处理器读取整个缓存行到适当的缓存（L1,L2,L3的或所有） 缓存命中 cache hit 如果进行高速缓存行填充操作的内存位置仍然是下次处理器访问的地址时，处理器从缓存中读取操作数，而不是从内存读取 写命中 write hit 当处理器将操作数写回到一个内存缓存的区域时，它首先回检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则处理器将这个操作数写回到缓存，而不是写回到内存，这个操作被称为写命中 写缺失 write misses the cache 一个有效的缓存行被写入到不存在的内存区域 为了提高处理速度，处理器不直接个内存进行通信，二十先将系统内存的数据督导内部缓存（L1,L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会像处理器发送一条Lock前缀的指令，将这个变量所在缓存的数据写回到系统内存。但是就算写回到内存，如过其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，再多处理器下，未了保证各个处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的换粗你一直，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，回重新从系统内存中把数据读到处理器缓存里。\nLock前缀的指令在多核处理器下会引发两件事情：\n将当前处理器缓存行的数据写回到系统内存 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效 volatile的两条实现原则：","title":"第一章 并发编程的挑战"},{"content":"lantern 是一款专业的代理软件，方便更好的使用互联网，输入我的邀请码 46KBX2 来获得三个月的蓝灯专业版！立即下载 https://github.com/getlantern/forum\n","permalink":"http://121.199.2.5:6080/g6ADCM/","summary":"lantern 是一款专业的代理软件，方便更好的使用互联网，输入我的邀请码 46KBX2 来获得三个月的蓝灯专业版！立即下载 https://github.com/getlantern/forum","title":"lantern 邀请码"},{"content":"eth2 相对于 eth1的优势：\n1、提高可扩展性 ETH1.0每秒仅支持15个左右的事务。而ETH2.0升级后最终将被划分为64（将来可能会更多）个分片，理论上可以使网络每秒完成达上千甚至上万笔交易。ETH2.0解决了主网运算能力过于集中的问题，并进一步提高了可扩展性。\n2、环境可持续性 ETH现阶段仍是使用较为主流的PoW工作量证明共识机制来运行及维护网络安全，PoW模式虽然在确保安全性和去中心化程度上具有一定的优势，但对于维护网络安全的节点付出的代价却是昂贵的。最终只有一个节点会找到正确的哈希值，获得记账权和奖励，但全球所有参与其中的节点却都付出了大量的算力和电力，这样的模式不仅效率低且对于环境资源而言是一种浪费。而升级为PoS权益证明共识机制后，ETH将不再依靠大量的算力和电力来维护、运行网络，而是依靠持币权益验证的方式来创建链上的区块和交易。\n","permalink":"http://121.199.2.5:6080/eth2/","summary":"eth2 相对于 eth1的优势：\n1、提高可扩展性 ETH1.0每秒仅支持15个左右的事务。而ETH2.0升级后最终将被划分为64（将来可能会更多）个分片，理论上可以使网络每秒完成达上千甚至上万笔交易。ETH2.0解决了主网运算能力过于集中的问题，并进一步提高了可扩展性。\n2、环境可持续性 ETH现阶段仍是使用较为主流的PoW工作量证明共识机制来运行及维护网络安全，PoW模式虽然在确保安全性和去中心化程度上具有一定的优势，但对于维护网络安全的节点付出的代价却是昂贵的。最终只有一个节点会找到正确的哈希值，获得记账权和奖励，但全球所有参与其中的节点却都付出了大量的算力和电力，这样的模式不仅效率低且对于环境资源而言是一种浪费。而升级为PoS权益证明共识机制后，ETH将不再依靠大量的算力和电力来维护、运行网络，而是依靠持币权益验证的方式来创建链上的区块和交易。","title":"eth2 的优势"},{"content":"linux复制文件和目录用cp命令，--parents 参数可以确保不存在目录的创建，比如文件 cp --parents java/doc/readme.md ../doc ，如果目录 doc不存在目录：java/doc则会创建。\n高级操作：\n#查找当前目录下所有.md文件，并将它们复制到DataxDoc目录，如果目录不存在则双肩 find . -name \u0026#34;*.md\u0026#34; | xargs -I {} cp --parents {} ../DataxDoc ","permalink":"http://121.199.2.5:6080/a0lZoj/","summary":"linux复制文件和目录用cp命令，--parents 参数可以确保不存在目录的创建，比如文件 cp --parents java/doc/readme.md ../doc ，如果目录 doc不存在目录：java/doc则会创建。\n高级操作：\n#查找当前目录下所有.md文件，并将它们复制到DataxDoc目录，如果目录不存在则双肩 find . -name \u0026#34;*.md\u0026#34; | xargs -I {} cp --parents {} ../DataxDoc ","title":"linux 复制文件，并创建不存在的目录"},{"content":"CassandraReader 插件文档 1 快速介绍 CassandraReader插件实现了从Cassandra读取数据。在底层实现上，CassandraReader通过datastax的java driver连接Cassandra实例，并执行相应的cql语句将数据从cassandra中SELECT出来。\n2 实现原理 简而言之，CassandraReader通过java driver连接到Cassandra实例，并根据用户配置的信息生成查询SELECT CQL语句，然后发送到Cassandra，并将该CQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column的信息，CassandraReader将其拼接为CQL语句发送到Cassandra。\n3 功能说明 3.1 配置样例 配置一个从Cassandra同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cassandrareader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 9042, \u0026#34;useSSL\u0026#34;: false, \u0026#34;keyspace\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;datax_src\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;textCol\u0026#34;, \u0026#34;blobCol\u0026#34;, \u0026#34;writetime(blobCol)\u0026#34;, \u0026#34;boolCol\u0026#34;, \u0026#34;smallintCol\u0026#34;, \u0026#34;tinyintCol\u0026#34;, \u0026#34;intCol\u0026#34;, \u0026#34;bigintCol\u0026#34;, \u0026#34;varintCol\u0026#34;, \u0026#34;floatCol\u0026#34;, \u0026#34;doubleCol\u0026#34;, \u0026#34;decimalCol\u0026#34;, \u0026#34;dateCol\u0026#34;, \u0026#34;timeCol\u0026#34;, \u0026#34;timeStampCol\u0026#34;, \u0026#34;uuidCol\u0026#34;, \u0026#34;inetCol\u0026#34;, \u0026#34;durationCol\u0026#34;, \u0026#34;listCol\u0026#34;, \u0026#34;mapCol\u0026#34;, \u0026#34;setCol\u0026#34; \u0026#34;tupleCol\u0026#34; \u0026#34;udtCol\u0026#34;, ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true } } } ] } } 3.2 参数说明 host\n描述：Cassandra连接点的域名或ip，多个node之间用逗号分隔。 必选：是 默认值：无 port\n描述：Cassandra端口。 必选：是 默认值：9042 username\n描述：数据源的用户名 必选：否 默认值：无 password\n描述：数据源指定用户名的密码 必选：否 默认值：无 useSSL\n描述：是否使用SSL连接。\n必选：否 默认值：false keyspace\n描述：需要同步的表所在的keyspace。\n必选：是 默认值：无 table\n描述：所选取的需要同步的表。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列集合。 其中的元素可以指定列的名称或writetime(column_name)，后一种形式会读取column_name列的时间戳而不是数据。\n必选：是 默认值：无 where\n描述：数据筛选条件的cql表达式，例如: \u0026#34;where\u0026#34;:\u0026#34;textcol=\u0026#39;a\u0026#39;\u0026#34; 必选：否 默认值：无 allowFiltering\n描述：是否在服务端过滤数据。参考cassandra文档中ALLOW FILTERING关键字的相关描述。\n必选：否 默认值：无 consistancyLevel\n描述：数据一致性级别。可选ONE|QUORUM|LOCAL_QUORUM|EACH_QUORUM|ALL|ANY|TWO|THREE|LOCAL_ONE\n必选：否 默认值：LOCAL_QUORUM 3.3 类型转换 目前CassandraReader支持除counter和Custom类型之外的所有类型。\n下面列出CassandraReader针对Cassandra类型转换列表:\nDataX 内部类型 Cassandra 数据类型 Long int, tinyint, smallint,varint,bigint,time Double float, double, decimal String ascii,varchar, text,uuid,timeuuid,duration,list,map,set,tuple,udt,inet Date date, timestamp Boolean bool Bytes blob 请注意:\n目前不支持counter类型和custom类型。 4 性能报告 略\n5 约束限制 5.1 主备同步数据恢复问题 略\n6 FAQ ","permalink":"http://121.199.2.5:6080/b0b0444d45c74c8f8b1a9fc4d2af738a/","summary":"CassandraReader 插件文档 1 快速介绍 CassandraReader插件实现了从Cassandra读取数据。在底层实现上，CassandraReader通过datastax的java driver连接Cassandra实例，并执行相应的cql语句将数据从cassandra中SELECT出来。\n2 实现原理 简而言之，CassandraReader通过java driver连接到Cassandra实例，并根据用户配置的信息生成查询SELECT CQL语句，然后发送到Cassandra，并将该CQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column的信息，CassandraReader将其拼接为CQL语句发送到Cassandra。\n3 功能说明 3.1 配置样例 配置一个从Cassandra同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cassandrareader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 9042, \u0026#34;useSSL\u0026#34;: false, \u0026#34;keyspace\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;datax_src\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;textCol\u0026#34;, \u0026#34;blobCol\u0026#34;, \u0026#34;writetime(blobCol)\u0026#34;, \u0026#34;boolCol\u0026#34;, \u0026#34;smallintCol\u0026#34;, \u0026#34;tinyintCol\u0026#34;, \u0026#34;intCol\u0026#34;, \u0026#34;bigintCol\u0026#34;, \u0026#34;varintCol\u0026#34;, \u0026#34;floatCol\u0026#34;, \u0026#34;doubleCol\u0026#34;, \u0026#34;decimalCol\u0026#34;, \u0026#34;dateCol\u0026#34;, \u0026#34;timeCol\u0026#34;, \u0026#34;timeStampCol\u0026#34;, \u0026#34;uuidCol\u0026#34;, \u0026#34;inetCol\u0026#34;, \u0026#34;durationCol\u0026#34;, \u0026#34;listCol\u0026#34;, \u0026#34;mapCol\u0026#34;, \u0026#34;setCol\u0026#34; \u0026#34;tupleCol\u0026#34; \u0026#34;udtCol\u0026#34;, ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true } } } ] } } 3.2 参数说明 host\n描述：Cassandra连接点的域名或ip，多个node之间用逗号分隔。 必选：是 默认值：无 port\n描述：Cassandra端口。 必选：是 默认值：9042 username\n描述：数据源的用户名 必选：否 默认值：无 password","title":"CassandraReader 插件文档"},{"content":"CassandraWriter 插件文档 1 快速介绍 CassandraWriter插件实现了向Cassandra写入数据。在底层实现上，CassandraWriter通过datastax的java driver连接Cassandra实例，并执行相应的cql语句将数据写入cassandra中。\n2 实现原理 简而言之，CassandraWriter通过java driver连接到Cassandra实例，并根据用户配置的信息生成INSERT CQL语句，然后发送到Cassandra。\n对于用户配置Table、Column的信息，CassandraReader将其拼接为CQL语句发送到Cassandra。\n3 功能说明 3.1 配置样例 配置一个从内存产生到Cassandra导入的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ {\u0026#34;value\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;bool\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;1988-08-08 08:08:08\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;date\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;addr\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;bytes\u0026#34;}, {\u0026#34;value\u0026#34;:1.234,\u0026#34;type\u0026#34;:\u0026#34;double\u0026#34;}, {\u0026#34;value\u0026#34;:12345678,\u0026#34;type\u0026#34;:\u0026#34;long\u0026#34;}, {\u0026#34;value\u0026#34;:2.345,\u0026#34;type\u0026#34;:\u0026#34;double\u0026#34;}, {\u0026#34;value\u0026#34;:3456789,\u0026#34;type\u0026#34;:\u0026#34;long\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;4a0ef8c0-4d97-11d0-db82-ebecdb03ffa5\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;value\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;bytes\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;-838383838,37377373,-383883838,27272772,393993939,-38383883,83883838,-1350403181,817650816,1630642337,251398784,-622020148\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, ], \u0026#34;sliceRecordCount\u0026#34;: 10000000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cassandrawriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 9042, \u0026#34;useSSL\u0026#34;: false, \u0026#34;keyspace\u0026#34;: \u0026#34;stresscql\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;dst\u0026#34;, \u0026#34;batchSize\u0026#34;:10, \u0026#34;column\u0026#34;: [ \u0026#34;name\u0026#34;, \u0026#34;choice\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;address\u0026#34;, \u0026#34;dbl\u0026#34;, \u0026#34;lval\u0026#34;, \u0026#34;fval\u0026#34;, \u0026#34;ival\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;value\u0026#34;, \u0026#34;listval\u0026#34; ] } } } ] } } 3.2 参数说明 host\n描述：Cassandra连接点的域名或ip，多个node之间用逗号分隔。 必选：是 默认值：无 port\n描述：Cassandra端口。 必选：是 默认值：9042 username\n描述：数据源的用户名 必选：否 默认值：无 password\n描述：数据源指定用户名的密码 必选：否 默认值：无 useSSL\n描述：是否使用SSL连接。\n必选：否 默认值：false connectionsPerHost\n描述：客户端连接池配置：与服务器每个节点建多少个连接。\n必选：否 默认值：8 maxPendingPerConnection\n描述：客户端连接池配置：每个连接最大请求数。\n必选：否 默认值：128 keyspace\n描述：需要同步的表所在的keyspace。\n必选：是 默认值：无 table\n描述：所选取的需要同步的表。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列集合。 内容可以是列的名称或\u0026quot;writetime()\u0026quot;。如果将列名配置为writetime()，会将这一列的内容作为时间戳。\n必选：是 默认值：无 consistancyLevel\n描述：数据一致性级别。可选ONE|QUORUM|LOCAL_QUORUM|EACH_QUORUM|ALL|ANY|TWO|THREE|LOCAL_ONE\n必选：否 默认值：LOCAL_QUORUM batchSize\n描述：一次批量提交(UNLOGGED BATCH)的记录数大小（条数）。注意batch的大小有如下限制： （1）不能超过65535。 (2) batch中的内容大小受到服务器端batch_size_fail_threshold_in_kb的限制。 (3) 如果batch中的内容超过了batch_size_warn_threshold_in_kb的限制，会打出warn日志，但并不影响写入，忽略即可。 如果批量提交失败，会把这个批量的所有内容重新逐条写入一遍。\n必选：否 默认值：1 3.3 类型转换 目前CassandraReader支持除counter和Custom类型之外的所有类型。\n下面列出CassandraReader针对Cassandra类型转换列表:\nDataX 内部类型 Cassandra 数据类型 Long int, tinyint, smallint,varint,bigint,time Double float, double, decimal String ascii,varchar, text,uuid,timeuuid,duration,list,map,set,tuple,udt,inet Date date, timestamp Boolean bool Bytes blob 请注意:\n目前不支持counter类型和custom类型。 4 性能报告 略\n5 约束限制 5.1 主备同步数据恢复问题 略\n6 FAQ ","permalink":"http://121.199.2.5:6080/056d47f442c1471da1b62d7062dd00e2/","summary":"CassandraWriter 插件文档 1 快速介绍 CassandraWriter插件实现了向Cassandra写入数据。在底层实现上，CassandraWriter通过datastax的java driver连接Cassandra实例，并执行相应的cql语句将数据写入cassandra中。\n2 实现原理 简而言之，CassandraWriter通过java driver连接到Cassandra实例，并根据用户配置的信息生成INSERT CQL语句，然后发送到Cassandra。\n对于用户配置Table、Column的信息，CassandraReader将其拼接为CQL语句发送到Cassandra。\n3 功能说明 3.1 配置样例 配置一个从内存产生到Cassandra导入的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ {\u0026#34;value\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;bool\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;1988-08-08 08:08:08\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;date\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;addr\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;bytes\u0026#34;}, {\u0026#34;value\u0026#34;:1.234,\u0026#34;type\u0026#34;:\u0026#34;double\u0026#34;}, {\u0026#34;value\u0026#34;:12345678,\u0026#34;type\u0026#34;:\u0026#34;long\u0026#34;}, {\u0026#34;value\u0026#34;:2.345,\u0026#34;type\u0026#34;:\u0026#34;double\u0026#34;}, {\u0026#34;value\u0026#34;:3456789,\u0026#34;type\u0026#34;:\u0026#34;long\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;4a0ef8c0-4d97-11d0-db82-ebecdb03ffa5\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;value\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;bytes\u0026#34;}, {\u0026#34;value\u0026#34;:\u0026#34;-838383838,37377373,-383883838,27272772,393993939,-38383883,83883838,-1350403181,817650816,1630642337,251398784,-622020148\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, ], \u0026#34;sliceRecordCount\u0026#34;: 10000000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cassandrawriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 9042, \u0026#34;useSSL\u0026#34;: false, \u0026#34;keyspace\u0026#34;: \u0026#34;stresscql\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;dst\u0026#34;, \u0026#34;batchSize\u0026#34;:10, \u0026#34;column\u0026#34;: [ \u0026#34;name\u0026#34;, \u0026#34;choice\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;address\u0026#34;, \u0026#34;dbl\u0026#34;, \u0026#34;lval\u0026#34;, \u0026#34;fval\u0026#34;, \u0026#34;ival\u0026#34;, \u0026#34;uid\u0026#34;, \u0026#34;value\u0026#34;, \u0026#34;listval\u0026#34; ] } } } ] } } 3.2 参数说明 host\n描述：Cassandra连接点的域名或ip，多个node之间用逗号分隔。 必选：是 默认值：无 port","title":"CassandraWriter 插件文档"},{"content":" DataX DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。\nDataX 商业版本 阿里云DataWorks数据集成是DataX团队在阿里云上的商业化产品，致力于提供复杂网络环境下、丰富的异构数据源之间高速稳定的数据移动能力，以及繁杂业务背景下的数据同步解决方案。目前已经支持云上近3000家客户，单日同步数据超过3万亿条。DataWorks数据集成目前支持离线50+种数据源，可以进行整库迁移、批量上云、增量同步、分库分表等各类同步解决方案。2020年更新实时同步能力，2020年更新实时同步能力，支持10+种数据源的读写任意组合。提供MySQL，Oracle等多种数据源到阿里云MaxCompute，Hologres等大数据引擎的一键全增量同步解决方案。\nhttps://www.aliyun.com/product/bigdata/ide\nFeatures DataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。\nDataX详细介绍 请参考：DataX-Introduction Quick Start Download DataX下载地址 请点击：Quick Start Support Data Channels DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图，详情请点击：DataX数据源参考指南\n类型 数据源 Reader(读) Writer(写) 文档 RDBMS 关系型数据库 MySQL √ √ 读 、写 Oracle √ √ 读 、写 SQLServer √ √ 读 、写 PostgreSQL √ √ 读 、写 DRDS √ √ 读 、写 通用RDBMS(支持所有关系型数据库) √ √ 读 、写 阿里云数仓数据存储 ODPS √ √ 读 、写 ADS √ 写 OSS √ √ 读 、写 OCS √ √ 读 、写 NoSQL数据存储 OTS √ √ 读 、写 Hbase0.94 √ √ 读 、写 Hbase1.1 √ √ 读 、写 Phoenix4.x √ √ 读 、写 Phoenix5.x √ √ 读 、写 MongoDB √ √ 读 、写 Hive √ √ 读 、写 Cassandra √ √ 读 、写 无结构化数据存储 TxtFile √ √ 读 、写 FTP √ √ 读 、写 HDFS √ √ 读 、写 Elasticsearch √ 写 时间序列数据库 OpenTSDB √ 读 TSDB √ √ 读 、写 阿里云DataWorks数据集成 目前DataX的已有能力已经全部融和进阿里云的数据集成，并且比DataX更加高效、安全，同时数据集成具备DataX不具备的其它高级特性和功能。可以理解为数据集成是DataX的全面升级的商业化用版本，为企业可以提供稳定、可靠、安全的数据传输服务。与DataX相比，数据集成主要有以下几大突出特点：\n支持实时同步：\n功能简介：https://help.aliyun.com/document_detail/181912.html 支持的数据源：https://help.aliyun.com/document_detail/146778.html 支持数据处理：https://help.aliyun.com/document_detail/146777.html 离线同步数据源种类大幅度扩充：\n新增比如：DB2、Kafka、Hologres、MetaQ、SAPHANA、达梦等等，持续扩充中 离线同步支持的数据源：https://help.aliyun.com/document_detail/137670.html 具备同步解决方案： 解决方案系统：https://help.aliyun.com/document_detail/171765.html 一键全增量：https://help.aliyun.com/document_detail/175676.html 整库迁移：https://help.aliyun.com/document_detail/137809.html 批量上云：https://help.aliyun.com/document_detail/146671.html 更新更多能力请访问：https://help.aliyun.com/document_detail/137663.html 我要开发新的插件 请点击：DataX插件开发宝典\n项目成员 核心Contributions: 言柏 、枕水、秋奇、青砾、一斅、云时\n感谢天烬、光戈、祁然、巴真、静行对DataX做出的贡献。\nLicense This software is free to use under the Apache License Apache license.\n请及时提出issue给我们。请前往：DataxIssue\n开源版DataX企业用户 长期招聘 联系邮箱：datax@alibabacloud.com 【JAVA开发职位】 职位名称：JAVA资深开发工程师/专家/高级专家 工作年限 : 2年以上 学历要求 : 本科（如果能力靠谱，这些都不是条件） 期望层级 : P6/P7/P8 岗位描述： 1. 负责阿里云大数据平台（数加）的开发设计。 2. 负责面向政企客户的大数据相关产品开发； 3. 利用大规模机器学习算法挖掘数据之间的联系，探索数据挖掘技术在实际场景中的产品应用 ； 4. 一站式大数据开发平台 5. 大数据任务调度引擎 6. 任务执行引擎 7. 任务监控告警 8. 海量异构数据同步 岗位要求： 1. 拥有3年以上JAVA Web开发经验； 2. 熟悉Java的基础技术体系。包括JVM、类装载、线程、并发、IO资源管理、网络； 3. 熟练使用常用Java技术框架、对新技术框架有敏锐感知能力；深刻理解面向对象、设计原则、封装抽象； 4. 熟悉HTML/HTML5和JavaScript；熟悉SQL语言； 5. 执行力强，具有优秀的团队合作精神、敬业精神； 6. 深刻理解设计模式及应用场景者加分； 7. 具有较强的问题分析和处理能力、比较强的动手能力，对技术有强烈追求者优先考虑； 8. 对高并发、高稳定可用性、高性能、大数据处理有过实际项目及产品经验者优先考虑； 9. 有大数据产品、云产品、中间件技术解决方案者优先考虑。 钉钉用户群：\nDataX开源用户交流群\nDataX开源用户交流群2\nDataX开源用户交流群3\nDataX开源用户交流群4\nDataX开源用户交流群5\n","permalink":"http://121.199.2.5:6080/dcdba0138c4f45f0a08968ab48edb900/","summary":"DataX DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。\nDataX 商业版本 阿里云DataWorks数据集成是DataX团队在阿里云上的商业化产品，致力于提供复杂网络环境下、丰富的异构数据源之间高速稳定的数据移动能力，以及繁杂业务背景下的数据同步解决方案。目前已经支持云上近3000家客户，单日同步数据超过3万亿条。DataWorks数据集成目前支持离线50+种数据源，可以进行整库迁移、批量上云、增量同步、分库分表等各类同步解决方案。2020年更新实时同步能力，2020年更新实时同步能力，支持10+种数据源的读写任意组合。提供MySQL，Oracle等多种数据源到阿里云MaxCompute，Hologres等大数据引擎的一键全增量同步解决方案。\nhttps://www.aliyun.com/product/bigdata/ide\nFeatures DataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。\nDataX详细介绍 请参考：DataX-Introduction Quick Start Download DataX下载地址 请点击：Quick Start Support Data Channels DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图，详情请点击：DataX数据源参考指南\n类型 数据源 Reader(读) Writer(写) 文档 RDBMS 关系型数据库 MySQL √ √ 读 、写 Oracle √ √ 读 、写 SQLServer √ √ 读 、写 PostgreSQL √ √ 读 、写 DRDS √ √ 读 、写 通用RDBMS(支持所有关系型数据库) √ √ 读 、写 阿里云数仓数据存储 ODPS √ √ 读 、写 ADS √ 写 OSS √ √ 读 、写 OCS √ √ 读 、写 NoSQL数据存储 OTS √ √ 读 、写 Hbase0.94 √ √ 读 、写 Hbase1.1 √ √ 读 、写 Phoenix4.x √ √ 读 、写 Phoenix5.","title":"DataX"},{"content":"DataX DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、SQL Server、Oracle、PostgreSQL、HDFS、Hive、HBase、OTS、ODPS 等各种异构数据源之间高效的数据同步功能。\nFeatures DataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。\nSystem Requirements Linux JDK(1.8以上，推荐1.8) Python(推荐Python2.6.X) Apache Maven 3.x (Compile DataX) Quick Start 工具部署\n方法一、直接下载DataX工具包：DataX下载地址\n下载后解压至本地某个目录，进入bin目录，即可运行同步作业：\n$ cd {YOUR_DATAX_HOME}/bin $ python datax.py {YOUR_JOB.json} 自检脚本： python {YOUR_DATAX_HOME}/bin/datax.py {YOUR_DATAX_HOME}/job/job.json\n方法二、下载DataX源码，自己编译：DataX源码\n(1)、下载DataX源码：\n$ git clone git@github.com:alibaba/DataX.git (2)、通过maven打包：\n$ cd {DataX_source_code_home} $ mvn -U clean package assembly:assembly -Dmaven.test.skip=true 打包成功，日志显示如下：\n[INFO] BUILD SUCCESS [INFO] ----------------------------------------------------------------- [INFO] Total time: 08:12 min [INFO] Finished at: 2015-12-13T16:26:48+08:00 [INFO] Final Memory: 133M/960M [INFO] ----------------------------------------------------------------- 打包成功后的DataX包位于 {DataX_source_code_home}/target/datax/datax/ ，结构如下：\n$ cd {DataX_source_code_home} $ ls ./target/datax/datax/ bin\tconf\tjob\tlib\tlog\tlog_perf\tplugin\tscript\ttmp 配置示例：从stream读取数据并打印到控制台\n第一步、创建作业的配置文件（json格式）\n可以通过命令查看配置模板： python datax.py -r {YOUR_READER} -w {YOUR_WRITER}\n$ cd {YOUR_DATAX_HOME}/bin $ python datax.py -r streamreader -w streamwriter DataX (UNKNOWN_DATAX_VERSION), From Alibaba ! Copyright (C) 2010-2015, Alibaba Group. All Rights Reserved. Please refer to the streamreader document: https://github.com/alibaba/DataX/blob/master/streamreader/doc/streamreader.md Please refer to the streamwriter document: https://github.com/alibaba/DataX/blob/master/streamwriter/doc/streamwriter.md Please save the following configuration as a json file and use python {DATAX_HOME}/bin/datax.py {JSON_FILE_NAME}.json to run the job. { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [], \u0026#34;sliceRecordCount\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: \u0026#34;\u0026#34; } } } } 根据模板配置json如下：\n#stream2stream.json { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sliceRecordCount\u0026#34;: 10, \u0026#34;column\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;10\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;hello，你好，世界-DataX\u0026#34; } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } } } } 第二步：启动DataX\n$ cd {YOUR_DATAX_DIR_BIN} $ python datax.py ./stream2stream.json 同步结束，显示日志如下：\n... 2015-12-17 11:20:25.263 [job-0] INFO JobContainer - 任务启动时刻 : 2015-12-17 11:20:15 任务结束时刻 : 2015-12-17 11:20:25 任务总计耗时 : 10s 任务平均流量 : 205B/s 记录写入速度 : 5rec/s 读出记录总数 : 50 读写失败总数 : 0 Contact us Google Groups: DataX-user\n","permalink":"http://121.199.2.5:6080/f4223fe7c5a64b12ae87c43ed48cc971/","summary":"DataX DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、SQL Server、Oracle、PostgreSQL、HDFS、Hive、HBase、OTS、ODPS 等各种异构数据源之间高效的数据同步功能。\nFeatures DataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。\nSystem Requirements Linux JDK(1.8以上，推荐1.8) Python(推荐Python2.6.X) Apache Maven 3.x (Compile DataX) Quick Start 工具部署\n方法一、直接下载DataX工具包：DataX下载地址\n下载后解压至本地某个目录，进入bin目录，即可运行同步作业：\n$ cd {YOUR_DATAX_HOME}/bin $ python datax.py {YOUR_JOB.json} 自检脚本： python {YOUR_DATAX_HOME}/bin/datax.py {YOUR_DATAX_HOME}/job/job.json\n方法二、下载DataX源码，自己编译：DataX源码\n(1)、下载DataX源码：\n$ git clone git@github.com:alibaba/DataX.git (2)、通过maven打包：\n$ cd {DataX_source_code_home} $ mvn -U clean package assembly:assembly -Dmaven.test.skip=true 打包成功，日志显示如下：\n[INFO] BUILD SUCCESS [INFO] ----------------------------------------------------------------- [INFO] Total time: 08:12 min [INFO] Finished at: 2015-12-13T16:26:48+08:00 [INFO] Final Memory: 133M/960M [INFO] ----------------------------------------------------------------- 打包成功后的DataX包位于 {DataX_source_code_home}/target/datax/datax/ ，结构如下：\n$ cd {DataX_source_code_home} $ ls ./target/datax/datax/ bin\tconf\tjob\tlib\tlog\tlog_perf\tplugin\tscript\ttmp 配置示例：从stream读取数据并打印到控制台\n第一步、创建作业的配置文件（json格式）\n可以通过命令查看配置模板： python datax.py -r {YOUR_READER} -w {YOUR_WRITER}","title":"DataX"},{"content":"DataXDataX ADB PG WriterDataX ADS写入CassandraReader 插件文档CassandraWriter 插件文档Readme.mdDataX插件开发宝典DrdsReader 插件文档DataX DRDSWriterREADME.mdDataX ElasticSearchWriterDataX FtpReader 说明DataX FtpWriter 说明DataX GDBReaderDataX GDBWriterHbase094XReader \u0026amp; Hbase11XReader 插件文档Hbase094XWriter \u0026amp; Hbase11XWriter 插件文档Hbase094XReader \u0026amp; Hbase11XReader 插件文档hbase11xsqlreader 插件文档HBase11xsqlwriter插件文档Hbase094XWriter \u0026amp; Hbase11XWriter 插件文档hbase20xsqlreader 插件文档HBase20xsqlwriter插件文档DataX HdfsReader 插件文档DataX HdfsWriter 插件文档阿里云开源离线同步工具DataX3.0介绍KingbaseesReader 插件文档DataX KingbaseesWriterdatax-kudu-plugindatax-kudu-pluginsDatax MongoDBReaderDatax MongoDBWriterMysqlReader 插件文档DataX MysqlWriterDataX OCSWriter 适用memcached客户端写入ocsDataX ODPSReaderDataX ODPS写入OpenTSDBReader 插件文档OracleReader 插件文档DataX OracleWriterDataX OSSReader 说明DataX OSSWriter 说明OTSReader 插件文档TableStore增量数据导出通道：TableStoreStreamReaderOTSWriter 插件文档PostgresqlReader 插件文档DataX PostgresqlWriterRDBMSReader 插件文档RDBMSWriter 插件文档SqlServerReader 插件文档DataX SqlServerWriterDataX TransformerTSDBReader 插件文档TSDBWriter 插件文档DataX TxtFileReader 说明DataX TxtFileWriter 说明DataX\n","permalink":"http://121.199.2.5:6080/bk-4/","summary":"DataXDataX ADB PG WriterDataX ADS写入CassandraReader 插件文档CassandraWriter 插件文档Readme.mdDataX插件开发宝典DrdsReader 插件文档DataX DRDSWriterREADME.mdDataX ElasticSearchWriterDataX FtpReader 说明DataX FtpWriter 说明DataX GDBReaderDataX GDBWriterHbase094XReader \u0026amp; Hbase11XReader 插件文档Hbase094XWriter \u0026amp; Hbase11XWriter 插件文档Hbase094XReader \u0026amp; Hbase11XReader 插件文档hbase11xsqlreader 插件文档HBase11xsqlwriter插件文档Hbase094XWriter \u0026amp; Hbase11XWriter 插件文档hbase20xsqlreader 插件文档HBase20xsqlwriter插件文档DataX HdfsReader 插件文档DataX HdfsWriter 插件文档阿里云开源离线同步工具DataX3.0介绍KingbaseesReader 插件文档DataX KingbaseesWriterdatax-kudu-plugindatax-kudu-pluginsDatax MongoDBReaderDatax MongoDBWriterMysqlReader 插件文档DataX MysqlWriterDataX OCSWriter 适用memcached客户端写入ocsDataX ODPSReaderDataX ODPS写入OpenTSDBReader 插件文档OracleReader 插件文档DataX OracleWriterDataX OSSReader 说明DataX OSSWriter 说明OTSReader 插件文档TableStore增量数据导出通道：TableStoreStreamReaderOTSWriter 插件文档PostgresqlReader 插件文档DataX PostgresqlWriterRDBMSReader 插件文档RDBMSWriter 插件文档SqlServerReader 插件文档DataX SqlServerWriterDataX TransformerTSDBReader 插件文档TSDBWriter 插件文档DataX TxtFileReader 说明DataX TxtFileWriter 说明DataX","title":"datax 3.0 教程"},{"content":"DataX ADB PG Writer 1 快速介绍 AdbpgWriter 插件实现了写入数据到 ABD PG版数据库的功能。在底层实现上，AdbpgWriter 插件会先缓存需要写入的数据，当缓存的 数据量达到 commitSize 时，插件会通过 JDBC 连接远程 ADB PG版 数据库，并执行 COPY 命令将数据写入 ADB PG 数据库。\nAdbpgWriter 可以作为数据迁移工具为用户提供服务。\n2 实现原理 AdbpgWriter 通过 DataX 框架获取 Reader 生成的协议数据，首先会将数据缓存，当缓存的数据量达到commitSize时，插件根据你配置生成相应的COPY语句，执行 COPY命令将数据写入ADB PG数据库中。\n3 功能说明 3.1 配置样例 这里使用一份从内存产生到 AdbpgWriter导入的数据 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 32 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ] }, \u0026#34;sliceRecordCount\u0026#34;: 1000 }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adbpgwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;host\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;1234\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;database\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;schema\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;preSql\u0026#34;: [\u0026#34;delete * from table\u0026#34;], \u0026#34;postSql\u0026#34;: [\u0026#34;select * from table\u0026#34;], \u0026#34;column\u0026#34;: [\u0026#34;*\u0026#34;] } } } ] } } 3.2 参数说明 name\n描述：插件名称 必选：是 默认值：无 username\n描述：目的数据库的用户名 必选：是 默认值：无 password\n描述：目的数据库的密码 必选：是 默认值：无 host\n描述：目的数据库主机名 必选：是 默认值：无 port\n描述：目的数据库的端口 必选：是 默认值：无 database\n描述：需要写入的表所属的数据库名称 必选：是 默认值：无 schema\n描述：需要写入的表所属的schema名称 必选：是 默认值：无 table\n描述：需要写入的表名称 必选：是 默认值：无 column\n描述：目的表需要写入数据的字段,字段之间用英文逗号分隔。例如: \u0026ldquo;column\u0026rdquo;: [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;,\u0026ldquo;age\u0026rdquo;]。如果要依次写入全部列，使用表示, 例如: \u0026ldquo;column\u0026rdquo;: [\u0026quot;\u0026quot;]\n注意：1、我们强烈不推荐你这样配置，因为当你目的表字段个数、类型等有改动时，你的任务可能运行不正确或者失败 2、此处 column 不能配置任何常量值 必选：是 默认值：否 preSql\n描述：写入数据到目的表前，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，可以使用 @table 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。比如你的任务是要写入到目的端的100个同构分表(表名称为:datax_00,datax01, \u0026hellip; datax_98,datax_99)，并且你希望导入数据前，先对表中数据进行删除操作，那么你可以这样配置：\u0026quot;preSql\u0026quot;:[\u0026quot;delete from @table\u0026quot;]，效果是：在执行到每个表写入数据前，会先执行对应的 delete from 对应表名称 必选：否 默认值：否 postSql\n描述：写入数据到目的表后，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，可以使用 @table 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。 必选：否 默认值：否 3.3 类型转换 目前 AdbpgWriter 支持大部分 ADB PG 数据库的类型，但也存在部分没有支持的情况，请注意检查你的类型。\n下面列出 AdbpgWriter 针对 ADB PG 类型转换列表:\nDataX 内部类型 ADB PG 数据类型 Long bigint, bigserial, integer, smallint, serial Double double precision, float, numeric, real String varchar, char, text Date date, time, timestamp Boolean bool 4 性能报告 4.1 环境准备 4.1.1 数据特征 建表语句：\ncreate table schematest.test_datax ( t1 int, t2 bigint, t3 bigserial, t4 float, t5 timestamp, t6 varchar )distributed by(t1); 4.1.2 机器参数 执行DataX的机器参数为:\ncpu: 24核 mem: 96GB ADB PG数据库机器参数为:\n平均core数量:4 primary segment 数量: 4 计算组数量:2 4.2 测试报告 4.2.1 单表测试报告 通道数 commitSize MB DataX速度(Rec/s) DataX流量(M/s) 1 10 54098 15.54 1 20 55000 15.80 4 10 183333 52.66 4 20 173684 49.89 8 10 330000 94.79 8 20 300000 86.17 16 10 412500 118.48 16 20 366666 105.32 32 10 366666 105.32 4.2.2 性能测试小结 channel数对性能影响很大 通常不建议写入数据库时，通道个数 \u0026gt; 32 ","permalink":"http://121.199.2.5:6080/1c4beb639bad47a79445e1bb8ccd68a5/","summary":"DataX ADB PG Writer 1 快速介绍 AdbpgWriter 插件实现了写入数据到 ABD PG版数据库的功能。在底层实现上，AdbpgWriter 插件会先缓存需要写入的数据，当缓存的 数据量达到 commitSize 时，插件会通过 JDBC 连接远程 ADB PG版 数据库，并执行 COPY 命令将数据写入 ADB PG 数据库。\nAdbpgWriter 可以作为数据迁移工具为用户提供服务。\n2 实现原理 AdbpgWriter 通过 DataX 框架获取 Reader 生成的协议数据，首先会将数据缓存，当缓存的数据量达到commitSize时，插件根据你配置生成相应的COPY语句，执行 COPY命令将数据写入ADB PG数据库中。\n3 功能说明 3.1 配置样例 这里使用一份从内存产生到 AdbpgWriter导入的数据 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 32 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ] }, \u0026#34;sliceRecordCount\u0026#34;: 1000 }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adbpgwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;host\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;1234\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;database\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;schema\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;preSql\u0026#34;: [\u0026#34;delete * from table\u0026#34;], \u0026#34;postSql\u0026#34;: [\u0026#34;select * from table\u0026#34;], \u0026#34;column\u0026#34;: [\u0026#34;*\u0026#34;] } } } ] } } 3.","title":"DataX ADB PG Writer"},{"content":"DataX ADS写入 1 快速介绍 欢迎ADS加入DataX生态圈！ADSWriter插件实现了其他数据源向ADS写入功能，现有DataX所有的数据源均可以无缝接入ADS，实现数据快速导入ADS。\nADS写入预计支持两种实现方式：\nADSWriter 支持向ODPS中转落地导入ADS方式，优点在于当数据量较大时(\u0026gt;1KW)，可以以较快速度进行导入，缺点引入了ODPS作为落地中转，因此牵涉三方系统(DataX、ADS、ODPS)鉴权认证。\nADSWriter 同时支持向ADS直接写入的方式，优点在于小批量数据写入能够较快完成(\u0026lt;1KW)，缺点在于大数据导入较慢。\n注意：\n如果从ODPS导入数据到ADS，请用户提前在源ODPS的Project中授权ADS Build账号具有读取你源表ODPS的权限，同时，ODPS源表创建人和ADS写入属于同一个阿里云账号。\n如果从非ODPS导入数据到ADS，请用户提前在目的端ADS空间授权ADS Build账号具备Load data权限。\n以上涉及ADS Build账号请联系ADS管理员提供。\n2 实现原理 ADS写入预计支持两种实现方式：\n2.1 Load模式 DataX 将数据导入ADS为当前导入任务分配的ADS项目表，随后DataX通知ADS完成数据加载。该类数据导入方式实际上是写ADS完成数据同步，由于ADS是分布式存储集群，因此该通道吞吐量较大，可以支持TB级别数据导入。\nDataX底层得到明文的 jdbc://host:port/dbname + username + password + table， 以此连接ADS， 执行show grants; 前置检查该用户是否有ADS中目标表的Load Data或者更高的权限。注意，此时ADSWriter使用用户填写的ADS用户名+密码信息完成登录鉴权工作。\n检查通过后，通过ADS中目标表的元数据反向生成ODPS DDL，在ODPS中间project中，以ADSWriter的账户建立ODPS表（非分区表，生命周期设为1-2Day), 并调用ODPSWriter把数据源的数据写入该ODPS表中。\n注意，这里需要使用中转ODPS的账号AK向中转ODPS写入数据。\n写入完成后，以中转ODPS账号连接ADS，发起Load Data From ‘odps://中转project/中转table/\u0026rsquo; [overwrite] into adsdb.adstable [partition (xx,xx=xx)]; 这个命令返回一个Job ID需要记录。\n注意，此时ADS使用自己的Build账号访问中转ODPS，因此需要中转ODPS对这个Build账号提前开放读取权限。\n连接ADS一分钟一次轮询执行 select state from information_schema.job_instances where job_id like ‘$Job ID’，查询状态，注意这个第一个一分钟可能查不到状态记录。\nSuccess或者Fail后返回给用户，然后删除中转ODPS表，任务结束。\n上述流程是从其他非ODPS数据源导入ADS流程，对于ODPS导入ADS流程使用如下流程：\n2.2 Insert模式 DataX 将数据直连ADS接口，利用ADS暴露的INSERT接口直写到ADS。该类数据导入方式写入吞吐量较小，不适合大批量数据写入。有如下注意点：\nADSWriter使用JDBC连接直连ADS，并只使用了JDBC Statement进行数据插入。ADS不支持PreparedStatement，故ADSWriter只能单行多线程进行写入。\nADSWriter支持筛选部分列，列换序等功能，即用户可以填写列。\n考虑到ADS负载问题，建议ADSWriter Insert模式建议用户使用TPS限流，最高在1W TPS。\nADSWriter在所有Task完成写入任务后，Job Post单例执行flush工作，保证数据在ADS整体更新。\n3 功能说明 3.1 配置样例 这里使用一份从内存产生到ADS，使用Load模式进行导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 100000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;odps\u0026#34;: { \u0026#34;accessId\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;account\u0026#34;: \u0026#34;xxx@aliyun.com\u0026#34;, \u0026#34;odpsServer\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;tunnelServer\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;accountType\u0026#34;: \u0026#34;aliyun\u0026#34;, \u0026#34;project\u0026#34;: \u0026#34;transfer_project\u0026#34; }, \u0026#34;writeMode\u0026#34;: \u0026#34;load\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;127.0.0.1:3306\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;schema\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;partition\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lifeCycle\u0026#34;: 2, \u0026#34;overWrite\u0026#34;: true, } } } ] } } 这里使用一份从内存产生到ADS，使用Insert模式进行导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 100000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;127.0.0.1:3306\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;schema\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;column\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;username\u0026#34;: \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;partition\u0026#34;: \u0026#34;id,ds=2015\u0026#34; } } } ] } } 3.2 参数说明 （用户配置规格） url\n描述：ADS连接信息，格式为\u0026quot;ip:port\u0026quot;。\n必选：是 默认值：无 schema\n描述：ADS的schema名称。\n必选：是 默认值：无 username\n描述：ADS对应的username，目前就是accessId 必选：是 默认值：无 password\n描述：ADS对应的password，目前就是accessKey 必选：是 默认值：无 table\n描述：目的表的表名称。\n必选：是 默认值：无 partition\n描述：目标表的分区名称，当目标表为分区表，需要指定该字段。\n必选：否 默认值：无 writeMode\n描述：支持Load和Insert两种写入模式\n必选：是 默认值：无 column\n描述：目的表字段列表，可以为[\u0026quot;*\u0026quot;]，或者具体的字段列表，例如[\u0026ldquo;a\u0026rdquo;, \u0026ldquo;b\u0026rdquo;, \u0026ldquo;c\u0026rdquo;]\n必选：是 默认值：无 overWrite\n描述：ADS写入是否覆盖当前写入的表，true为覆盖写入，false为不覆盖(追加)写入。当writeMode为Load，该值才会生效。\n必选：是 默认值：无 lifeCycle\n描述：ADS 临时表生命周期。当writeMode为Load时，该值才会生效。\n必选：是 默认值：无 batchSize\n描述：ADS 提交数据写的批量条数，当writeMode为insert时，该值才会生效。\n必选：writeMode为insert时才有用 默认值：32 bufferSize\n描述：DataX数据收集缓冲区大小，缓冲区的目的是攒一个较大的buffer，源头的数据首先进入到此buffer中进行排序，排序完成后再提交ads写。排序是根据ads的分区列模式进行的，排序的目的是数据顺序对ADS服务端更友好，出于性能考虑。bufferSize缓冲区中的数据会经过batchSize批量提交到ADS中，一般如果要设置bufferSize，设置bufferSize为batchSize数量的多倍。当writeMode为insert时，该值才会生效。\n必选：writeMode为insert时才有用 默认值：默认不配置不开启此功能 3.3 类型转换 DataX 内部类型 ADS 数据类型 Long int, tinyint, smallint, int, bigint Double float, double, decimal String varchar Date date Boolean bool Bytes 无 注意:\nmultivalue ADS支持multivalue类型，DataX对于该类型支持待定？ 4 插件约束 如果Reader为ODPS，且ADSWriter写入模式为Load模式时，ODPS的partition只支持如下三种配置方式(以两级分区为例)：\n\u0026#34;partition\u0026#34;:[\u0026#34;pt=*,ds=*\u0026#34;] (读取test表所有分区的数据) \u0026#34;partition\u0026#34;:[\u0026#34;pt=1,ds=*\u0026#34;] (读取test表下面，一级分区pt=1下面的所有二级分区) \u0026#34;partition\u0026#34;:[\u0026#34;pt=1,ds=hangzhou\u0026#34;] (读取test表下面，一级分区pt=1下面，二级分区ds=hz的数据) 5 性能报告（线上环境实测） 5.1 环境准备 5.2 测试报告 6 FAQ ","permalink":"http://121.199.2.5:6080/5d26f13c5f3e4cda84adc613514cdd53/","summary":"DataX ADS写入 1 快速介绍 欢迎ADS加入DataX生态圈！ADSWriter插件实现了其他数据源向ADS写入功能，现有DataX所有的数据源均可以无缝接入ADS，实现数据快速导入ADS。\nADS写入预计支持两种实现方式：\nADSWriter 支持向ODPS中转落地导入ADS方式，优点在于当数据量较大时(\u0026gt;1KW)，可以以较快速度进行导入，缺点引入了ODPS作为落地中转，因此牵涉三方系统(DataX、ADS、ODPS)鉴权认证。\nADSWriter 同时支持向ADS直接写入的方式，优点在于小批量数据写入能够较快完成(\u0026lt;1KW)，缺点在于大数据导入较慢。\n注意：\n如果从ODPS导入数据到ADS，请用户提前在源ODPS的Project中授权ADS Build账号具有读取你源表ODPS的权限，同时，ODPS源表创建人和ADS写入属于同一个阿里云账号。\n如果从非ODPS导入数据到ADS，请用户提前在目的端ADS空间授权ADS Build账号具备Load data权限。\n以上涉及ADS Build账号请联系ADS管理员提供。\n2 实现原理 ADS写入预计支持两种实现方式：\n2.1 Load模式 DataX 将数据导入ADS为当前导入任务分配的ADS项目表，随后DataX通知ADS完成数据加载。该类数据导入方式实际上是写ADS完成数据同步，由于ADS是分布式存储集群，因此该通道吞吐量较大，可以支持TB级别数据导入。\nDataX底层得到明文的 jdbc://host:port/dbname + username + password + table， 以此连接ADS， 执行show grants; 前置检查该用户是否有ADS中目标表的Load Data或者更高的权限。注意，此时ADSWriter使用用户填写的ADS用户名+密码信息完成登录鉴权工作。\n检查通过后，通过ADS中目标表的元数据反向生成ODPS DDL，在ODPS中间project中，以ADSWriter的账户建立ODPS表（非分区表，生命周期设为1-2Day), 并调用ODPSWriter把数据源的数据写入该ODPS表中。\n注意，这里需要使用中转ODPS的账号AK向中转ODPS写入数据。\n写入完成后，以中转ODPS账号连接ADS，发起Load Data From ‘odps://中转project/中转table/\u0026rsquo; [overwrite] into adsdb.adstable [partition (xx,xx=xx)]; 这个命令返回一个Job ID需要记录。\n注意，此时ADS使用自己的Build账号访问中转ODPS，因此需要中转ODPS对这个Build账号提前开放读取权限。\n连接ADS一分钟一次轮询执行 select state from information_schema.job_instances where job_id like ‘$Job ID’，查询状态，注意这个第一个一分钟可能查不到状态记录。\nSuccess或者Fail后返回给用户，然后删除中转ODPS表，任务结束。\n上述流程是从其他非ODPS数据源导入ADS流程，对于ODPS导入ADS流程使用如下流程：\n2.2 Insert模式 DataX 将数据直连ADS接口，利用ADS暴露的INSERT接口直写到ADS。该类数据导入方式写入吞吐量较小，不适合大批量数据写入。有如下注意点：\nADSWriter使用JDBC连接直连ADS，并只使用了JDBC Statement进行数据插入。ADS不支持PreparedStatement，故ADSWriter只能单行多线程进行写入。\nADSWriter支持筛选部分列，列换序等功能，即用户可以填写列。\n考虑到ADS负载问题，建议ADSWriter Insert模式建议用户使用TPS限流，最高在1W TPS。\nADSWriter在所有Task完成写入任务后，Job Post单例执行flush工作，保证数据在ADS整体更新。\n3 功能说明 3.1 配置样例 这里使用一份从内存产生到ADS，使用Load模式进行导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 100000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;odps\u0026#34;: { \u0026#34;accessId\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;account\u0026#34;: \u0026#34;xxx@aliyun.","title":"DataX ADS写入"},{"content":"DataX DRDSWriter 1 快速介绍 DRDSWriter 插件实现了写入数据到 DRDS 的目的表的功能。在底层实现上， DRDSWriter 通过 JDBC 连接远程 DRDS 数据库的 Proxy，并执行相应的 replace into \u0026hellip; 的 sql 语句将数据写入 DRDS，特别注意执行的 Sql 语句是 replace into，为了避免数据重复写入，需要你的表具备主键或者唯一性索引(Unique Key)。\nDRDSWriter 面向ETL开发工程师，他们使用 DRDSWriter 从数仓导入数据到 DRDS。同时 DRDSWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 DRDSWriter 通过 DataX 框架获取 Reader 生成的协议数据，通过 replace into...(没有遇到主键/唯一性索引冲突时，与 insert into 行为一致，冲突时会用新行替换原有行所有字段) 的语句写入数据到 DRDS。DRDSWriter 累积一定数据，提交给 DRDS 的 Proxy，该 Proxy 内部决定数据是写入一张还是多张表以及多张表写入时如何路由数据。 注意：整个任务至少需要具备 replace into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 DRDS 导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;drdswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:mysql://127.0.0.1:3306/datax?useUnicode=true\u0026amp;characterEncoding=gbk\u0026#34;, \u0026#34;table\u0026#34;: [ \u0026#34;test\u0026#34; ] } ] } } } ] } } 3.2 参数说明 jdbcUrl\n描述：目的数据库的 JDBC 连接信息。作业运行时，DataX 会在你提供的 jdbcUrl 后面追加如下属性：yearIsDateType=false\u0026amp;zeroDateTimeBehavior=convertToNull\u0026amp;rewriteBatchedStatements=true\n注意：1、在一个数据库上只能配置一个 jdbcUrl 值 2、一个DRDS 写入任务仅能配置一个 jdbcUrl 3、jdbcUrl按照Mysql/DRDS官方规范，并可以填写连接附加控制信息，比如想指定连接编码为 gbk ，则在 jdbcUrl 后面追加属性 useUnicode=true\u0026amp;characterEncoding=gbk。具体请参看 Mysql/DRDS官方文档或者咨询对应 DBA。 必选：是 默认值：无 username\n描述：目的数据库的用户名 必选：是 默认值：无 password\n描述：目的数据库的密码 必选：是 默认值：无 table\n描述：目的表的表名称。 只能配置一个DRDS 的表名称。\n注意：table 和 jdbcUrl 必须包含在 connection 配置单元中 必选：是 默认值：无 column\n描述：目的表需要写入数据的字段,字段之间用英文逗号分隔。例如: \u0026ldquo;column\u0026rdquo;: [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;,\u0026ldquo;age\u0026rdquo;]。如果要依次写入全部列，使用表示, 例如: \u0026ldquo;column\u0026rdquo;: [\u0026quot;\u0026quot;]\n**column配置项必须指定，不能留空！** 注意：1、我们强烈不推荐你这样配置，因为当你目的表字段个数、类型等有改动时，你的任务可能运行不正确或者失败 2、此处 column 不能配置任何常量值 必选：是 默认值：否 preSql\n描述：写入数据到目的表前，会先执行这里的标准语句。比如你想在导入数据前清空数据表中的数据，那么可以配置为:\u0026quot;preSql\u0026quot;:[\u0026quot;delete from yourTableName\u0026quot;] 必选：否 默认值：无 postSql\n描述：写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ） 必选：否 默认值：无 writeMode\n描述：默认为 replace，目前仅支持 replace，可以不配置。 必选：否 默认值：replace batchSize\n描述：一次性批量提交的记录数大小，该值可以极大减少DataX与DRDS的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。\n必选：否 默认值：\n3.3 类型转换 类似 MysqlWriter ，目前 DRDSWriter 支持大部分 Mysql 类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出 DRDSWriter 针对 Mysql 类型转换列表:\nDataX 内部类型 Mysql 数据类型 Long int, tinyint, smallint, mediumint, int, bigint, year Double float, double, decimal String varchar, char, tinytext, text, mediumtext, longtext Date date, datetime, timestamp, time Boolean bit, bool Bytes tinyblob, mediumblob, blob, longblob, varbinary 4 性能报告 5 约束限制 FAQ Q: DRDSWriter 执行 postSql 语句报错，那么数据导入到目标数据库了吗?\nA: DataX 导入过程存在三块逻辑，pre 操作、导入操作、post 操作，其中任意一环报错，DataX 作业报错。由于 DataX 不能保证在同一个事务完成上述几个操作，因此有可能数据已经落入到目标端。\nQ: 按照上述说法，那么有部分脏数据导入数据库，如果影响到线上数据库怎么办?\nA: 目前有两种解法，第一种配置 pre 语句，该 sql 可以清理当天导入数据， DataX 每次导入时候可以把上次清理干净并导入完整数据。第二种，向临时表导入数据，完成后再 rename 到线上表。\nQ: 上面第二种方法可以避免对线上数据造成影响，那我具体怎样操作?\nA: 可以配置临时表导入\n","permalink":"http://121.199.2.5:6080/d31c9e889b154a0f96e4eb4be5c50467/","summary":"DataX DRDSWriter 1 快速介绍 DRDSWriter 插件实现了写入数据到 DRDS 的目的表的功能。在底层实现上， DRDSWriter 通过 JDBC 连接远程 DRDS 数据库的 Proxy，并执行相应的 replace into \u0026hellip; 的 sql 语句将数据写入 DRDS，特别注意执行的 Sql 语句是 replace into，为了避免数据重复写入，需要你的表具备主键或者唯一性索引(Unique Key)。\nDRDSWriter 面向ETL开发工程师，他们使用 DRDSWriter 从数仓导入数据到 DRDS。同时 DRDSWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 DRDSWriter 通过 DataX 框架获取 Reader 生成的协议数据，通过 replace into...(没有遇到主键/唯一性索引冲突时，与 insert into 行为一致，冲突时会用新行替换原有行所有字段) 的语句写入数据到 DRDS。DRDSWriter 累积一定数据，提交给 DRDS 的 Proxy，该 Proxy 内部决定数据是写入一张还是多张表以及多张表写入时如何路由数据。 注意：整个任务至少需要具备 replace into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 DRDS 导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;drdswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:mysql://127.","title":"DataX DRDSWriter"},{"content":"DataX ElasticSearchWriter 1 快速介绍 数据导入elasticsearch的插件\n2 实现原理 使用elasticsearch的rest api接口， 批量把从reader读入的数据写入elasticsearch\n3 功能说明 3.1 配置样例 job.json { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { ... }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;elasticsearchwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://xxx:9999\u0026#34;, \u0026#34;accessId\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;test-1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;cleanup\u0026#34;: true, \u0026#34;settings\u0026#34;: {\u0026#34;index\u0026#34; :{\u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 0}}, \u0026#34;discovery\u0026#34;: false, \u0026#34;batchSize\u0026#34;: 1000, \u0026#34;splitter\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;column\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;id\u0026#34;}, { \u0026#34;name\u0026#34;: \u0026#34;col_ip\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;ip\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_double\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_long\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_integer\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_keyword\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_text\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;}, { \u0026#34;name\u0026#34;: \u0026#34;col_geo_point\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_date\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;}, { \u0026#34;name\u0026#34;: \u0026#34;col_nested1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_nested2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_object1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_object2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_integer_array\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;integer\u0026#34;, \u0026#34;array\u0026#34;:true}, { \u0026#34;name\u0026#34;: \u0026#34;col_geo_shape\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;geo_shape\u0026#34;, \u0026#34;tree\u0026#34;: \u0026#34;quadtree\u0026#34;, \u0026#34;precision\u0026#34;: \u0026#34;10m\u0026#34;} ] } } } ] } } 3.2 参数说明 endpoint\n描述：ElasticSearch的连接地址\n必选：是\n默认值：无\naccessId\n描述：http auth中的user\n必选：否\n默认值：空\naccessKey\n描述：http auth中的password\n必选：否\n默认值：空\nindex\n描述：elasticsearch中的index名\n必选：是\n默认值：无\ntype\n描述：elasticsearch中index的type名\n必选：否\n默认值：index名\ncleanup\n描述：是否删除原表\n必选：否\n默认值：false\nbatchSize\n描述：每次批量数据的条数\n必选：否\n默认值：1000\ntrySize\n描述：失败后重试的次数\n必选：否\n默认值：30\ntimeout\n描述：客户端超时时间\n必选：否\n默认值：600000\ndiscovery\n描述：启用节点发现将(轮询)并定期更新客户机中的服务器列表。\n必选：否\n默认值：false\ncompression\n描述：http请求，开启压缩\n必选：否\n默认值：true\nmultiThread\n描述：http请求，是否有多线程\n必选：否\n默认值：true\nignoreWriteError\n描述：忽略写入错误，不重试，继续写入\n必选：否\n默认值：false\nignoreParseError\n描述：忽略解析数据格式错误，继续写入\n必选：否\n默认值：true\nalias\n描述：数据导入完成后写入别名\n必选：否\n默认值：无\naliasMode\n描述：数据导入完成后增加别名的模式，append(增加模式), exclusive(只留这一个)\n必选：否\n默认值：append\nsettings\n描述：创建index时候的settings, 与elasticsearch官方相同\n必选：否\n默认值：无\nsplitter\n描述：如果插入数据是array，就使用指定分隔符\n必选：否\n默认值：-,-\ncolumn\n描述：elasticsearch所支持的字段类型，样例中包含了全部\n必选：是\ndynamic\n描述: 不使用datax的mappings，使用es自己的自动mappings\n必选: 否\n默认值: false\n4 性能报告 4.1 环境准备 总数据量 1kw条数据, 每条0.1kb 1个shard, 0个replica 不加id，这样默认是append_only模式，不检查版本，插入速度会有20%左右的提升 4.1.1 输入数据类型(streamreader) {\u0026#34;value\u0026#34;: \u0026#34;1.1.1.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;value\u0026#34;: 19890604.0, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34;}, {\u0026#34;value\u0026#34;: 19890604, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;}, {\u0026#34;value\u0026#34;: 19890604, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;}, {\u0026#34;value\u0026#34;: \u0026#34;hello world\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;value\u0026#34;: \u0026#34;hello world\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;value\u0026#34;: \u0026#34;41.12,-71.34\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;value\u0026#34;: \u0026#34;2017-05-25\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, 4.1.2 输出数据类型(eswriter) { \u0026#34;name\u0026#34;: \u0026#34;col_ip\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;ip\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_double\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_long\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_integer\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_keyword\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_text\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;}, { \u0026#34;name\u0026#34;: \u0026#34;col_geo_point\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_date\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;} 4.1.2 机器参数 cpu: 32 Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz mem: 128G net: 千兆双网卡 4.1.3 DataX jvm 参数 -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError\n4.2 测试报告 通道数 批量提交行数 DataX速度(Rec/s) DataX流量(MB/s) 4 256 11013 0.828 4 1024 19417 1.43 4 4096 23923 1.76 4 8172 24449 1.80 8 256 21459 1.58 8 1024 37037 2.72 8 4096 45454 3.34 8 8172 45871 3.37 16 1024 67567 4.96 16 4096 78125 5.74 16 8172 77519 5.69 32 1024 94339 6.93 32 4096 96153 7.06 64 1024 91743 6.74 4.3 测试总结 最好的结果是32通道，每次传4096，如果单条数据很大， 请适当减少批量数，防止oom 当然这个很容易水平扩展，而且es也是分布式的，多设置几个shard也可以水平扩展 5 约束限制 如果导入id，这样数据导入失败也会重试，重新导入也仅仅是覆盖，保证数据一致性 如果不导入id，就是append_only模式，elasticsearch自动生成id，速度会提升20%左右，但数据无法修复，适合日志型数据(对数据精度要求不高的) ","permalink":"http://121.199.2.5:6080/5e857fe9cf5743dc837f14fbc9f0a876/","summary":"DataX ElasticSearchWriter 1 快速介绍 数据导入elasticsearch的插件\n2 实现原理 使用elasticsearch的rest api接口， 批量把从reader读入的数据写入elasticsearch\n3 功能说明 3.1 配置样例 job.json { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { ... }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;elasticsearchwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://xxx:9999\u0026#34;, \u0026#34;accessId\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;test-1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;cleanup\u0026#34;: true, \u0026#34;settings\u0026#34;: {\u0026#34;index\u0026#34; :{\u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 0}}, \u0026#34;discovery\u0026#34;: false, \u0026#34;batchSize\u0026#34;: 1000, \u0026#34;splitter\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;column\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;pk\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;id\u0026#34;}, { \u0026#34;name\u0026#34;: \u0026#34;col_ip\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;ip\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_double\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_long\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_integer\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_keyword\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_text\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;}, { \u0026#34;name\u0026#34;: \u0026#34;col_geo_point\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_date\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;}, { \u0026#34;name\u0026#34;: \u0026#34;col_nested1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_nested2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_object1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_object2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col_integer_array\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;integer\u0026#34;, \u0026#34;array\u0026#34;:true}, { \u0026#34;name\u0026#34;: \u0026#34;col_geo_shape\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;geo_shape\u0026#34;, \u0026#34;tree\u0026#34;: \u0026#34;quadtree\u0026#34;, \u0026#34;precision\u0026#34;: \u0026#34;10m\u0026#34;} ] } } } ] } } 3.","title":"DataX ElasticSearchWriter"},{"content":"DataX FtpReader 说明 1 快速介绍 FtpReader提供了读取远程FTP文件系统数据存储的能力。在底层实现上，FtpReader获取远程FTP文件数据，并转换为DataX传输协议传递给Writer。\n本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\n2 功能与限制 FtpReader实现了从远程FTP文件读取数据并转为DataX协议的功能，远程FTP文件本身是无结构化数据存储，对于DataX而言，FtpReader实现上类比TxtFileReader，有诸多相似之处。目前FtpReader支持功能如下：\n支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量\n支持递归读取、支持文件名过滤。\n支持文本压缩，现有压缩格式为zip、gzip、bzip2。\n多个File可以支持并发读取。\n我们暂时不能做到：\n单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。\n单个File在压缩情况下，从技术上无法支持多线程并发读取。\n3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ftpreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;sftp\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22, \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;path\u0026#34;: [ \u0026#34;/home/hanfa.shf/ftpReaderTest/data\u0026#34; ], \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy.MM.dd\u0026#34; } ], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ftpWriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/home/hanfa.shf/ftpReaderTest/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;shihf\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } } } ] } } 3.2 参数说明 protocol\n描述：ftp服务器协议，目前支持传输协议有ftp和sftp。 必选：是 默认值：无 host\n描述：ftp服务器地址。 必选：是 默认值：无 port\n描述：ftp服务器端口。 必选：否 默认值：若传输协议是sftp协议，默认值是22；若传输协议是标准ftp协议，默认值是21 timeout\n描述：连接ftp服务器连接超时时间，单位毫秒。 必选：否 默认值：60000（1分钟）\nconnectPattern\n描述：连接模式（主动模式或者被动模式）。该参数只在传输协议是标准ftp协议时使用，值只能为：PORT (主动)，PASV（被动）。两种模式主要的不同是数据连接建立的不同。对于Port模式，是客户端在本地打开一个端口等服务器去连接建立数据连接，而Pasv模式就是服务器打开一个端口等待客户端去建立一个数据连接。\n必选：否 默认值：PASV\nusername\n描述：ftp服务器访问用户名。 必选：是 默认值：无 password\n描述：ftp服务器访问密码。 必选：是 默认值：无 path\n描述：远程FTP文件系统的路径信息，注意这里可以支持填写多个路径。 当指定单个远程FTP文件，FtpReader暂时只能使用单线程进行数据抽取。二期考虑在非压缩文件情况下针对单个File可以进行多线程并发读取。\n当指定多个远程FTP文件，FtpReader支持使用多线程进行数据抽取。线程并发数通过通道数指定。\n当指定通配符，FtpReader尝试遍历出多个文件信息。例如: 指定/*代表读取/目录下所有的文件，指定/bazhen/*代表读取bazhen目录下游所有的文件。FtpReader目前只支持*作为文件通配符。\n特别需要注意的是，DataX会将一个作业下同步的所有Text File视作同一张数据表。用户必须自己保证所有的File能够适配同一套schema信息。读取文件用户必须保证为类CSV格式，并且提供给DataX权限可读。\n特别需要注意的是，如果Path指定的路径下没有符合匹配的文件抽取，DataX将报错。\n必选：是 默认值：无 column\n描述：读取字段列表，type指定源数据的类型，index指定当前列来自于文本第几列(以0开始)，value指定当前类型为常量，不从源头文件读取数据，而是根据value值自动生成对应的列。 默认情况下，用户可以全部按照String类型读取数据，配置如下：\n\u0026#34;column\u0026#34;: [\u0026#34;*\u0026#34;] 用户可以指定Column字段信息，配置如下：\n{ \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;index\u0026#34;: 0 //从远程FTP文件文本第一列获取int字段 }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;alibaba\u0026#34; //从FtpReader内部生成alibaba的字符串字段作为当前字段 } 对于用户指定Column信息，type必须填写，index/value必须选择其一。\n必选：是 默认值：全部按照string类型读取 fieldDelimiter\n描述：读取的字段分隔符 必选：是 默认值：, compress\n描述：文本压缩类型，默认不填写意味着没有压缩。支持压缩类型为zip、gzip、bzip2。 必选：否 默认值：没有压缩 encoding\n描述：读取文件的编码配置。\n必选：否 默认值：utf-8 skipHeader\n描述：类CSV格式文件可能存在表头为标题情况，需要跳过。默认不跳过。\n必选：否 默认值：false nullFormat\n描述：文本文件中无法使用标准字符串定义null(空指针)，DataX提供nullFormat定义哪些字符串可以表示为null。\n例如如果用户配置: nullFormat:\u0026quot;\\N\u0026quot;，那么如果源头数据是\u0026quot;\\N\u0026quot;，DataX视作null字段。\n必选：否 默认值：\\N maxTraversalLevel\n描述：允许遍历文件夹的最大层数。\n必选：否 默认值：100 csvReaderConfig\n描述：读取CSV类型文件参数配置，Map类型。读取CSV类型文件使用的CsvReader进行读取，会有很多配置，不配置则使用默认值。\n必选：否 默认值：无 常见配置：\n\u0026#34;csvReaderConfig\u0026#34;:{ \u0026#34;safetySwitch\u0026#34;: false, \u0026#34;skipEmptyRecords\u0026#34;: false, \u0026#34;useTextQualifier\u0026#34;: false } 所有配置项及默认值,配置时 csvReaderConfig 的map中请严格按照以下字段名字进行配置：\nboolean caseSensitive = true; char textQualifier = 34; boolean trimWhitespace = true; boolean useTextQualifier = true;//是否使用csv转义字符 char delimiter = 44;//分隔符 char recordDelimiter = 0; char comment = 35; boolean useComments = false; int escapeMode = 1; boolean safetySwitch = true;//单列长度是否限制100000字符 boolean skipEmptyRecords = true;//是否跳过空行 boolean captureRawRecord = true; 3.3 类型转换 远程FTP文件本身不提供数据类型，该类型是DataX FtpReader定义：\nDataX 内部类型 远程FTP文件 数据类型 Long Long Double Double String String Boolean Boolean Date Date 其中：\n远程FTP文件 Long是指远程FTP文件文本中使用整形的字符串表示形式，例如\u0026quot;19901219\u0026quot;。 远程FTP文件 Double是指远程FTP文件文本中使用Double的字符串表示形式，例如\u0026quot;3.1415\u0026quot;。 远程FTP文件 Boolean是指远程FTP文件文本中使用Boolean的字符串表示形式，例如\u0026quot;true\u0026quot;、\u0026ldquo;false\u0026rdquo;。不区分大小写。 远程FTP文件 Date是指远程FTP文件文本中使用Date的字符串表示形式，例如\u0026quot;2014-12-31\u0026quot;，Date可以指定format格式。 4 性能报告 5 约束限制 略\n6 FAQ 略\n","permalink":"http://121.199.2.5:6080/0b6970ee252d4875841a98382b04f30b/","summary":"DataX FtpReader 说明 1 快速介绍 FtpReader提供了读取远程FTP文件系统数据存储的能力。在底层实现上，FtpReader获取远程FTP文件数据，并转换为DataX传输协议传递给Writer。\n本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\n2 功能与限制 FtpReader实现了从远程FTP文件读取数据并转为DataX协议的功能，远程FTP文件本身是无结构化数据存储，对于DataX而言，FtpReader实现上类比TxtFileReader，有诸多相似之处。目前FtpReader支持功能如下：\n支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量\n支持递归读取、支持文件名过滤。\n支持文本压缩，现有压缩格式为zip、gzip、bzip2。\n多个File可以支持并发读取。\n我们暂时不能做到：\n单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。\n单个File在压缩情况下，从技术上无法支持多线程并发读取。\n3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ftpreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;sftp\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22, \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;path\u0026#34;: [ \u0026#34;/home/hanfa.shf/ftpReaderTest/data\u0026#34; ], \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy.MM.dd\u0026#34; } ], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ftpWriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/home/hanfa.","title":"DataX FtpReader 说明"},{"content":"DataX FtpWriter 说明 1 快速介绍 FtpWriter提供了向远程FTP文件写入CSV格式的一个或者多个文件，在底层实现上，FtpWriter将DataX传输协议下的数据转换为csv格式，并使用FTP相关的网络协议写出到远程FTP服务器。\n写入FTP文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\n2 功能与限制 FtpWriter实现了从DataX协议转为FTP文件功能，FTP文件本身是无结构化数据存储，FtpWriter如下几个方面约定:\n支持且仅支持写入文本类型(不支持BLOB如视频数据)的文件，且要求文本中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n写出时不支持文本压缩。\n支持多线程写入，每个线程写入不同子文件。\n我们不能做到：\n单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: {}, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ftpwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;sftp\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;port\u0026#34;: 22, \u0026#34;username\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;60000\u0026#34;, \u0026#34;connectPattern\u0026#34;: \u0026#34;PASV\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/tmp/data/\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;yixiao\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate|append|nonConflict\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;nullFormat\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;dateFormat\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34;, \u0026#34;fileFormat\u0026#34;: \u0026#34;csv\u0026#34;, \u0026#34;suffix\u0026#34;: \u0026#34;.csv\u0026#34;, \u0026#34;header\u0026#34;: [] } } } ] } } 3.2 参数说明 protocol\n描述：ftp服务器协议，目前支持传输协议有ftp和sftp。 必选：是 默认值：无 host\n描述：ftp服务器地址。 必选：是 默认值：无 port\n描述：ftp服务器端口。 必选：否 默认值：若传输协议是sftp协议，默认值是22；若传输协议是标准ftp协议，默认值是21 timeout\n描述：连接ftp服务器连接超时时间，单位毫秒。 必选：否 默认值：60000（1分钟）\nusername\n描述：ftp服务器访问用户名。 必选：是 默认值：无 password\n描述：ftp服务器访问密码。 必选：是 默认值：无 path\n描述：FTP文件系统的路径信息，FtpWriter会写入Path目录下属多个文件。 必选：是 默认值：无 fileName\n描述：FtpWriter写入的文件名，该文件名会添加随机的后缀作为每个线程写入实际文件名。 必选：是 默认值：无 writeMode\n描述：FtpWriter写入前数据清理处理模式： truncate，写入前清理目录下一fileName前缀的所有文件。 append，写入前不做任何处理，DataX FtpWriter直接使用filename写入，并保证文件名不冲突。 nonConflict，如果目录下有fileName前缀的文件，直接报错。 必选：是 默认值：无 fieldDelimiter\n描述：读取的字段分隔符 必选：否 默认值：, compress\n描述：文本压缩类型，暂时不支持。 必选：否 默认值：无压缩 encoding\n描述：读取文件的编码配置。\n必选：否 默认值：utf-8 nullFormat\n描述：文本文件中无法使用标准字符串定义null(空指针)，DataX提供nullFormat定义哪些字符串可以表示为null。\n例如如果用户配置: nullFormat=\u0026quot;\\N\u0026quot;，那么如果源头数据是\u0026quot;\\N\u0026quot;，DataX视作null字段。\n必选：否 默认值：\\N dateFormat\n描述：日期类型的数据序列化到文件中时的格式，例如 \u0026ldquo;dateFormat\u0026rdquo;: \u0026ldquo;yyyy-MM-dd\u0026rdquo;。\n必选：否 默认值：无 fileFormat\n描述：文件写出的格式，包括csv (http://zh.wikipedia.org/wiki/%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E5%80%BC) 和text两种，csv是严格的csv格式，如果待写数据包括列分隔符，则会按照csv的转义语法转义，转义符号为双引号\u0026quot;；text格式是用列分隔符简单分割待写数据，对于待写数据包括列分隔符情况下不做转义。\n必选：否 默认值：text suffix\n描述：最后输出文件的后缀，当前支持 \u0026ldquo;.text\u0026quot;以及\u0026rdquo;.csv\u0026quot;\n必选：否 默认值：\u0026quot;\u0026quot; header\n描述：txt写出时的表头，示例[\u0026lsquo;id\u0026rsquo;, \u0026rsquo;name\u0026rsquo;, \u0026lsquo;age\u0026rsquo;]。\n必选：否 默认值：无 3.3 类型转换 FTP文件本身不提供数据类型，该类型是DataX FtpWriter定义：\nDataX 内部类型 FTP文件 数据类型 Long Long -\u0026gt; 字符串序列化表示 Double Double -\u0026gt; 字符串序列化表示 String String -\u0026gt; 字符串序列化表示 Boolean Boolean -\u0026gt; 字符串序列化表示 Date Date -\u0026gt; 字符串序列化表示 其中：\nFTP文件 Long是指FTP文件文本中使用整形的字符串表示形式，例如\u0026quot;19901219\u0026quot;。 FTP文件 Double是指FTP文件文本中使用Double的字符串表示形式，例如\u0026quot;3.1415\u0026quot;。 FTP文件 Boolean是指FTP文件文本中使用Boolean的字符串表示形式，例如\u0026quot;true\u0026quot;、\u0026ldquo;false\u0026rdquo;。不区分大小写。 FTP文件 Date是指FTP文件文本中使用Date的字符串表示形式，例如\u0026quot;2014-12-31\u0026quot;，Date可以指定format格式。 4 性能报告 5 约束限制 略\n6 FAQ 略\n","permalink":"http://121.199.2.5:6080/8f4c3b36705842458a8550717b87b66c/","summary":"DataX FtpWriter 说明 1 快速介绍 FtpWriter提供了向远程FTP文件写入CSV格式的一个或者多个文件，在底层实现上，FtpWriter将DataX传输协议下的数据转换为csv格式，并使用FTP相关的网络协议写出到远程FTP服务器。\n写入FTP文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\n2 功能与限制 FtpWriter实现了从DataX协议转为FTP文件功能，FTP文件本身是无结构化数据存储，FtpWriter如下几个方面约定:\n支持且仅支持写入文本类型(不支持BLOB如视频数据)的文件，且要求文本中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n写出时不支持文本压缩。\n支持多线程写入，每个线程写入不同子文件。\n我们不能做到：\n单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: {}, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ftpwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;sftp\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;port\u0026#34;: 22, \u0026#34;username\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;60000\u0026#34;, \u0026#34;connectPattern\u0026#34;: \u0026#34;PASV\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/tmp/data/\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;yixiao\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate|append|nonConflict\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;nullFormat\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;dateFormat\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34;, \u0026#34;fileFormat\u0026#34;: \u0026#34;csv\u0026#34;, \u0026#34;suffix\u0026#34;: \u0026#34;.csv\u0026#34;, \u0026#34;header\u0026#34;: [] } } } ] } } 3.2 参数说明 protocol\n描述：ftp服务器协议，目前支持传输协议有ftp和sftp。 必选：是 默认值：无 host\n描述：ftp服务器地址。 必选：是 默认值：无 port\n描述：ftp服务器端口。 必选：否 默认值：若传输协议是sftp协议，默认值是22；若传输协议是标准ftp协议，默认值是21 timeout\n描述：连接ftp服务器连接超时时间，单位毫秒。 必选：否 默认值：60000（1分钟）","title":"DataX FtpWriter 说明"},{"content":"DataX GDBReader 1. 快速介绍 GDBReader插件实现读取GDB实例数据的功能，通过Gremlin Client连接远程GDB实例，按配置提供的label生成查询DSL，遍历点或边数据，包括属性数据，并将数据写入到Record中给到Writer使用。\n2. 实现原理 GDBReader使用Gremlin Client连接GDB实例，按label分不同Task取点或边数据。 单个Task中按label遍历点或边的id，再切分范围分多次请求查询点或边和属性数据，最后将点或边数据根据配置转换成指定格式记录发送给下游写插件。\nGDBReader按label切分多个Task并发，同一个label的数据批量异步获取来加快读取速度。如果配置读取的label列表为空，任务启动前会从GDB查询所有label再切分Task。\n3. 功能说明 GDB中点和边不同，读取需要区分点和边点配置。\n3.1 点配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } \u0026#34;errorLimit\u0026#34;: { \u0026#34;record\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;gdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.218.145.24\u0026#34;, \u0026#34;port\u0026#34;: 8182, \u0026#34;username\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;fetchBatchSize\u0026#34;: 100, \u0026#34;rangeSplitSize\u0026#34;: 1000, \u0026#34;labelType\u0026#34;: \u0026#34;VERTEX\u0026#34;, \u0026#34;labels\u0026#34;: [\u0026#34;label1\u0026#34;, \u0026#34;label2\u0026#34;], \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;label\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryLabel\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;vertexProperty\u0026#34; } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true } } } ] } } 3.2 边配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 }, \u0026#34;errorLimit\u0026#34;: { \u0026#34;record\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;gdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.218.145.24\u0026#34;, \u0026#34;port\u0026#34;: 8182, \u0026#34;username\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;fetchBatchSize\u0026#34;: 100, \u0026#34;rangeSplitSize\u0026#34;: 1000, \u0026#34;labelType\u0026#34;: \u0026#34;EDGE\u0026#34;, \u0026#34;labels\u0026#34;: [\u0026#34;label1\u0026#34;, \u0026#34;label2\u0026#34;], \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;label\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryLabel\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;srcId\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;srcPrimaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;srcLabel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;srcPrimaryLabel\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;dstId\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;srcPrimaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;dstLabel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;srcPrimaryLabel\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;edgeProperty\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;weight\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;edgeProperty\u0026#34; } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true } } } ] } } 3.3 参数说明 host\n描述：GDB实例连接地址，对应\u0026rsquo;实例管理\u0026rsquo;-\u0026gt;\u0026lsquo;基本信息\u0026rsquo;页面的网络地址 必选：是 默认值：无 port\n描述：GDB实例连接地址对应的端口 必选：是 默认值：8182 username\n描述：GDB实例账号名 必选：是 默认值：无 password\n描述：GDB实例账号名对应的密码 必选：是 默认值：无 fetchBatchSize\n描述：一次GDB请求读取点或边的数量，响应包含点或边以及属性 必选：是 默认值：100 rangeSplitSize\n描述：id遍历，一次遍历请求扫描的id个数 必选：是 默认值：10 * fetchBatchSize labels\n描述：标签数组，即需要导出的点或边标签，支持读取多个标签，用数组表示。如果留空([])，表示GDB中所有点或边标签 必选：是 默认值：无 labelType\n描述：数据标签类型，支持点、边两种枚举值 VERTEX：表示点 EDGE：表示边 必选：是 默认值：无 column\n描述：点或边字段映射关系配置 必选：是 默认值：无 column -\u0026gt; name\n描述：点或边映射关系的字段名，指定属性时表示读取的属性名，读取其他字段时会被忽略 必选：是 默认值：无 column -\u0026gt; type\n描述：点或边映射关系的字段类型 id, label在GDB中都是string类型，配置非string类型时可能会转换失败 普通属性支持基础类型，包括int, long, float, double, boolean, string GDBReader尽量将读取到的数据转换成配置要求的类型，但转换失败会导致该条记录错误 必选：是 默认值：无 column -\u0026gt; columnType\n描述：GDB点或边数据到列数据的映射关系，支持以下枚举值： primaryKey： 表示该字段是点或边的id primaryLabel： 表示该字段是点或边的label srcPrimaryKey： 表示该字段是边关联的起点id，只在读取边时使用 srcPrimaryLabel： 表示该字段是边关联的起点label，只在读取边时使用 dstPrimaryKey： 表示该字段是边关联的终点id，只在读取边时使用 dstPrimaryLabel： 表示该字段是边关联的终点label，只在读取边时使用 vertexProperty： 表示该字段是点的属性，只在读取点时使用，应用到SET属性时只读取其中的一个属性值 vertexJsonProperty： 表示该字段是点的属性集合，只在读取点时使用。属性集合使用JSON格式输出，包含所有的属性，不能与其他vertexProperty配置一起使用 edgeProperty： 表示该字段是边的属性，只在读取边时使用 edgeJsonProperty： 表示该字段是边的属性集合，只在读取边时使用。属性集合使用JSON格式输出，包含所有的属性，不能与其他edgeProperty配置一起使用 必选：是 默认值：无 vertexJsonProperty格式示例，新增c字段区分SET属性，但是SET属性只包含单个属性值时会标记成普通属性 {\u0026#34;properties\u0026#34;:[ {\u0026#34;k\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;t\u0026#34;,\u0026#34;string\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;Jack\u0026#34;,\u0026#34;c\u0026#34;:\u0026#34;set\u0026#34;}, {\u0026#34;k\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;t\u0026#34;,\u0026#34;string\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;Luck\u0026#34;,\u0026#34;c\u0026#34;:\u0026#34;set\u0026#34;}, {\u0026#34;k\u0026#34;:\u0026#34;age\u0026#34;,\u0026#34;t\u0026#34;,\u0026#34;int\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;20\u0026#34;,\u0026#34;c\u0026#34;:\u0026#34;single\u0026#34;} ]} edgeJsonProperty格式示例，边不支持多值属性 {\u0026#34;properties\u0026#34;:[ {\u0026#34;k\u0026#34;:\u0026#34;created_at\u0026#34;,\u0026#34;t\u0026#34;,\u0026#34;long\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;153498653\u0026#34;}, {\u0026#34;k\u0026#34;:\u0026#34;weight\u0026#34;,\u0026#34;t\u0026#34;,\u0026#34;double\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;3.14\u0026#34;} ]} 4 性能报告 (TODO)\n5 使用约束 无\n6 FAQ 无\n","permalink":"http://121.199.2.5:6080/ef41ae35929b479db90214d2ebe5ff8a/","summary":"DataX GDBReader 1. 快速介绍 GDBReader插件实现读取GDB实例数据的功能，通过Gremlin Client连接远程GDB实例，按配置提供的label生成查询DSL，遍历点或边数据，包括属性数据，并将数据写入到Record中给到Writer使用。\n2. 实现原理 GDBReader使用Gremlin Client连接GDB实例，按label分不同Task取点或边数据。 单个Task中按label遍历点或边的id，再切分范围分多次请求查询点或边和属性数据，最后将点或边数据根据配置转换成指定格式记录发送给下游写插件。\nGDBReader按label切分多个Task并发，同一个label的数据批量异步获取来加快读取速度。如果配置读取的label列表为空，任务启动前会从GDB查询所有label再切分Task。\n3. 功能说明 GDB中点和边不同，读取需要区分点和边点配置。\n3.1 点配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } \u0026#34;errorLimit\u0026#34;: { \u0026#34;record\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;gdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;10.218.145.24\u0026#34;, \u0026#34;port\u0026#34;: 8182, \u0026#34;username\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;fetchBatchSize\u0026#34;: 100, \u0026#34;rangeSplitSize\u0026#34;: 1000, \u0026#34;labelType\u0026#34;: \u0026#34;VERTEX\u0026#34;, \u0026#34;labels\u0026#34;: [\u0026#34;label1\u0026#34;, \u0026#34;label2\u0026#34;], \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;label\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryLabel\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;vertexProperty\u0026#34; } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true } } } ] } } 3.","title":"DataX GDBReader"},{"content":"DataX GDBWriter 1 快速介绍 GDBWriter插件实现了写入数据到GDB实例的功能。GDBWriter通过Gremlin Client连接远程GDB实例，获取Reader的数据，生成写入DSL语句，将数据写入到GDB。\n2 实现原理 GDBWriter通过DataX框架获取Reader生成的协议数据，使用g.addV/E(GDB___label).property(id, GDB___id).property(GDB___PK1, GDB___PV1)...语句写入数据到GDB实例。\n可以配置Gremlin Client工作在session模式，由客户端控制事务，在一次事务中实现多个记录的批量写入。\n3 功能说明 因为GDB中点和边的配置不同，导入时需要区分点和边的配置。\n3.1 点配置样例 这里是一份从内存生成点数据导入GDB实例的配置 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;random\u0026#34;: \u0026#34;1,100\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;1000,1200\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;60,64\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;100,1000\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;32,48\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;gdbwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;gdb-endpoint\u0026#34;, \u0026#34;port\u0026#34;: 8182, \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;INSERT\u0026#34;, \u0026#34;labelType\u0026#34;: \u0026#34;VERTEX\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;#{1}\u0026#34;, \u0026#34;idTransRule\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;session\u0026#34;: true, \u0026#34;maxRecordsInBatch\u0026#34;: 64, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{0}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vertex_propKey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{2}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;vertexSetProperty\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vertex_propKey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{3}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;vertexSetProperty\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vertex_propKey2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{4}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;vertexProperty\u0026#34; } ] } } } ] } } 3.2 边配置样例 这里是一份从内存生成边数据导入GDB实例的配置 注意 下面配置导入边时，需要提前在GDB实例中写入点，要求分别存在id为person-{{i}}和book-{{i}}的点，其中i取值0~100。\n{ \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;random\u0026#34;: \u0026#34;100,200\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;1,100\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;1,100\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;2000,2200\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;60,64\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;gdbwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;gdb-endpoint\u0026#34;, \u0026#34;port\u0026#34;: 8182, \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;INSERT\u0026#34;, \u0026#34;labelType\u0026#34;: \u0026#34;EDGE\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;#{3}\u0026#34;, \u0026#34;idTransRule\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;srcIdTransRule\u0026#34;: \u0026#34;labelPrefix\u0026#34;, \u0026#34;dstIdTransRule\u0026#34;: \u0026#34;labelPrefix\u0026#34;, \u0026#34;srcLabel\u0026#34;:\u0026#34;person-\u0026#34;, \u0026#34;dstLabel\u0026#34;:\u0026#34;book-\u0026#34;, \u0026#34;session\u0026#34;:false, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{0}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{1}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;srcPrimaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{2}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;dstPrimaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;edge_propKey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{4}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;edgeProperty\u0026#34; } ] } } } ] } } 3.3 参数说明 host\n描述：GDB实例连接域名，对应阿里云控制台-\u0026gt;\u0026ldquo;图数据库 GDB\u0026rdquo;-\u0026gt;\u0026ldquo;实例管理\u0026rdquo;-\u0026gt;\u0026ldquo;基本信息\u0026rdquo; 中的\u0026quot;内网地址\u0026quot;； 必选：是 默认值：无 port\n描述：GDB实例连接端口 必选：是 默认值：8182 username\n描述：GDB实例账号名 必选：是 默认值：无 password\n描述：图实例账号名对应密码 必选：是 默认值：无 label\n描述：类型名，即点/边名称； label支持从源列中读取，如#{0}，表示取第一列字段作为label名。源列索引从0开始； 必选：是 默认值：无 labelType\n描述：label类型； 枚举值\u0026quot;VERTEX\u0026quot;表示点 枚举值\u0026quot;EDGE\u0026quot;表示边 必选：是 默认值：无 srcLabel\n描述：当label为边时，表示起点的点名称；srcLabel支持从源列中读取，如#{0}，表示取第一列字段作为label名。源列索引从0开始； 必选：labelType为边，srcIdTransRule为none时可不填写，否则必填； 默认值：无 dstLabel\n描述：当label为边时，表示终点的点名称；dstLabel支持从源列中读取，如#{0}，表示取第一列字段作为label名。源列索引从0开始； 必选：labelType为边，dstIdTransRule为none时可不填写，否则必填； 默认值：无 writeMode\n描述：导入id重复时的处理模式； 枚举值\u0026quot;INSERT\u0026quot;表示会报错，错误记录数加1； 枚举值\u0026quot;MERGE\u0026quot;表示更新属性值，不计入错误； 枚举值\u0026quot;SKIP\u0026quot;表示跳过，不计入错误 必选：是 默认值：INSERT idTransRule\n描述：主键id转换规则； 枚举值\u0026quot;labelPrefix\u0026quot;表示将映射的值转换为{label名}{源字段} 枚举值\u0026quot;none\u0026quot;表示映射的值不做转换 必选：是 默认值：\u0026ldquo;none\u0026rdquo; srcIdTransRule\n描述：当label为边时，表示起点的主键id转换规则； 枚举值\u0026quot;labelPrefix\u0026quot;表示映射的值转换为为{label名}{源字段} 枚举值\u0026quot;none\u0026quot;表示映射的值不做转换，此时srcLabel 可不填写 必选：label为边时必选 默认值：\u0026ldquo;none\u0026rdquo; dstIdTransRule\n描述：当label为边时，表示终点的主键id转换规则； 枚举值\u0026quot;labelPrefix\u0026quot;表示映射的值转换为为{label名}{源字段} 枚举值\u0026quot;none\u0026quot;表示映射的值不做转换，此时dstLabel 可不填写 必选：label为边时必选 默认值：\u0026ldquo;none\u0026rdquo; session\n描述：是否使用Gremlin Client的session模式写入数据 必选：否 默认值：false maxRecordsInBatch\n描述：使用Gremlin Client的session模式时，一次事务处理的记录数 必选：否 默认值：16 column\n描述：点/边字段映射关系配置 必选：是 默认值：无 column -\u0026gt; name\n描述：点/边映射关系的字段名 必选：是 默认值：无 column -\u0026gt; value\n描述：点/边映射关系的字段值； #{N}表示直接映射源端值，N为源端column索引，从0开始；#{0}表示映射源端column第1个字段； test-#{0} 表示源端值做拼接转换，#{0}值前/后可添加固定字符串; #{0}-#{1}表示做多字段拼接，也可在任意位置添加固定字符串，如test-#{0}-test1-#{1}-test2 必选：是 默认值：无 column -\u0026gt; type\n描述：点/边映射关系的字段值类型； 主键id只支持string类型，GDBWriter插件会强制转换，源id必须保证可转换为string； 普通属性支持类型：int, long, float, double, boolean, string 必选：是 默认值：无 column -\u0026gt; columnType\n描述：点/边映射关系字段对应到GDB点/边数据的类型，支持以下几类枚举值： 公共枚举值： primaryKey：表示该字段是主键id 点枚举值： vertexProperty：labelType为点时，表示该字段是点的普通属性 vertexSetProperty：labelType为点时，表示该字段是点的SET属性，value是SET属性中的一个属性值 vertexJsonProperty：labelType为点时，表示是点json属性，value结构请见备注json properties示例，点配置最多只允许出现一个json属性； 边枚举值： srcPrimaryKey：labelType为边时，表示该字段是起点主键id dstPrimaryKey：labelType为边时，表示该字段是终点主键id edgeProperty：labelType为边时，表示该字段是边的普通属性 edgeJsonProperty：labelType为边时，表示是边json属性，value结构请见备注json properties示例，边配置最多只允许出现一个json属性； 必选：是 默认值：无 备注：json properties示例 {\u0026#34;properties\u0026#34;:[ {\u0026#34;k\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;t\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;tom\u0026#34;}, {\u0026#34;k\u0026#34;:\u0026#34;age\u0026#34;,\u0026#34;t\u0026#34;:\u0026#34;int\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;20\u0026#34;}, {\u0026#34;k\u0026#34;:\u0026#34;sex\u0026#34;,\u0026#34;t\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;male\u0026#34;} ]} # json格式同样支持给点添加SET属性，格式如下 {\u0026#34;properties\u0026#34;:[ {\u0026#34;k\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;t\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;tom\u0026#34;,\u0026#34;c\u0026#34;:\u0026#34;set\u0026#34;}, {\u0026#34;k\u0026#34;:\u0026#34;name\u0026#34;,\u0026#34;t\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;jack\u0026#34;,\u0026#34;c\u0026#34;:\u0026#34;set\u0026#34;}, {\u0026#34;k\u0026#34;:\u0026#34;age\u0026#34;,\u0026#34;t\u0026#34;:\u0026#34;int\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;20\u0026#34;}, {\u0026#34;k\u0026#34;:\u0026#34;sex\u0026#34;,\u0026#34;t\u0026#34;:\u0026#34;string\u0026#34;,\u0026#34;v\u0026#34;:\u0026#34;male\u0026#34;} ]} 4 性能报告 4.1 环境参数 GDB实例规格\n16core 128GB, 1TB SSD DataX压测机器\ncpu: 4 * Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz mem: 16GB net: 千兆双网卡 os: CentOS 7, 3.10.0-957.5.1.el7.x86_64 jvm: -Xms4g -Xmx4g 4.2 数据特征 { id: random double(1~10000) from: random long(1~40000000) to: random long(1~40000000) label: random long(20000000 ~ 20005000) propertyKey: random string(len: 120~128) propertyName: random string(len: 120~128) } 点/边都有一个属性，属性key和value都是长度120~128字节的随机字符串 label是范围20000000 ~ 20005000的随机整数转换的字符串 id是浮点数转换的字符串，防止重复 边包含关联起点和终点，测试边时已经提前导入twitter数据集的点数据(4200W) 4.3 任务配置 分点和边的配置，具体配置与上述的示例配置相似，下面列出关键的差异点\n增加并发任务数量 \u0026ldquo;channel\u0026rdquo;: 32\n使用session模式 \u0026ldquo;session\u0026rdquo;: true\n增加事务批量处理记录个数 \u0026ldquo;maxRecordsInBatch\u0026rdquo;: 128\n4.4 测试结果 点导入性能：\n任务平均流量： 4.07MB/s 任务总计耗时： 412s 记录写入速度： 15609rec/s 读出记录总数： 6400000 边导入性能：\n任务平均流量： 2.76MB/s 任务总计耗时： 1602s 记录写入速度： 10000rec/s 读出记录总数： 16000000 5 约束限制 导入边记录前要求GDB中已经存在边关联的起点/终点 GDBWriter插件与用户查询DSL使用相同的GDB实例端口，导入时可能会影响查询性能 FAQ 使用SET属性需要升级GDB实例到1.0.20版本及以上。 边只支持普通单值属性，不能给边写SET属性数据。 ","permalink":"http://121.199.2.5:6080/3830a303d3e34cec88b98eab9934006d/","summary":"DataX GDBWriter 1 快速介绍 GDBWriter插件实现了写入数据到GDB实例的功能。GDBWriter通过Gremlin Client连接远程GDB实例，获取Reader的数据，生成写入DSL语句，将数据写入到GDB。\n2 实现原理 GDBWriter通过DataX框架获取Reader生成的协议数据，使用g.addV/E(GDB___label).property(id, GDB___id).property(GDB___PK1, GDB___PV1)...语句写入数据到GDB实例。\n可以配置Gremlin Client工作在session模式，由客户端控制事务，在一次事务中实现多个记录的批量写入。\n3 功能说明 因为GDB中点和边的配置不同，导入时需要区分点和边的配置。\n3.1 点配置样例 这里是一份从内存生成点数据导入GDB实例的配置 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;random\u0026#34;: \u0026#34;1,100\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;1000,1200\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;60,64\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;100,1000\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;random\u0026#34;: \u0026#34;32,48\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;gdbwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;gdb-endpoint\u0026#34;, \u0026#34;port\u0026#34;: 8182, \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;INSERT\u0026#34;, \u0026#34;labelType\u0026#34;: \u0026#34;VERTEX\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;#{1}\u0026#34;, \u0026#34;idTransRule\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;session\u0026#34;: true, \u0026#34;maxRecordsInBatch\u0026#34;: 64, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{0}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;primaryKey\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vertex_propKey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{2}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;vertexSetProperty\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vertex_propKey\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{3}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;vertexSetProperty\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;vertex_propKey2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;#{4}\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;columnType\u0026#34;: \u0026#34;vertexProperty\u0026#34; } ] } } } ] } } 3.","title":"DataX GDBWriter"},{"content":"DataX HdfsReader 插件文档 1 快速介绍 HdfsReader提供了读取分布式文件系统数据存储的能力。在底层实现上，HdfsReader获取分布式文件系统上文件的数据，并转换为DataX传输协议传递给Writer。\n目前HdfsReader支持的文件格式有textfile（text）、orcfile（orc）、rcfile（rc）、sequence file（seq）和普通逻辑二维表（csv）类型格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表。\nHdfsReader需要Jdk1.7及以上版本的支持。\n2 功能与限制 HdfsReader实现了从Hadoop分布式文件系统Hdfs中读取文件数据并转为DataX协议的功能。textfile是Hive建表时默认使用的存储格式，数据不做压缩，本质上textfile就是以文本的形式将数据存放在hdfs中，对于DataX而言，HdfsReader实现上类比TxtFileReader，有诸多相似之处。orcfile，它的全名是Optimized Row Columnar file，是对RCFile做了优化。据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。HdfsReader利用Hive提供的OrcSerde类，读取解析orcfile文件的数据。目前HdfsReader支持的功能如下：\n支持textfile、orcfile、rcfile、sequence file和csv格式的文件，且要求文件内容存放的是一张逻辑意义上的二维表。\n支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量\n支持递归读取、支持正则表达式（\u0026quot;*\u0026ldquo;和\u0026rdquo;?\u0026quot;）。\n支持orcfile数据压缩，目前支持SNAPPY，ZLIB两种压缩方式。\n多个File可以支持并发读取。\n支持sequence file数据压缩，目前支持lzo压缩方式。\ncsv类型支持压缩格式有：gzip、bz2、zip、lzo、lzo_deflate、snappy。\n目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试；\n支持kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效）\n我们暂时不能做到：\n单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。 目前还不支持hdfs HA; 3 功能说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hdfsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/user/hive/warehouse/mytable01/*\u0026#34;, \u0026#34;defaultFS\u0026#34;: \u0026#34;hdfs://xxx:port\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;hello\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } ], \u0026#34;fileType\u0026#34;: \u0026#34;orc\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true } } } ] } } 3.2 参数说明（各个配置项值前后不允许有空格） path\n描述：要读取的文件路径，如果要读取多个文件，可以使用正则表达式\u0026quot;*\u0026quot;，注意这里可以支持填写多个路径。。 当指定单个Hdfs文件，HdfsReader暂时只能使用单线程进行数据抽取。二期考虑在非压缩文件情况下针对单个File可以进行多线程并发读取。\n当指定多个Hdfs文件，HdfsReader支持使用多线程进行数据抽取。线程并发数通过通道数指定。\n当指定通配符，HdfsReader尝试遍历出多个文件信息。例如: 指定/代表读取/目录下所有的文件，指定/bazhen/*代表读取bazhen目录下游所有的文件。HdfsReader目前只支持\u0026quot;\u0026ldquo;和\u0026rdquo;?\u0026ldquo;作为文件通配符。\n特别需要注意的是，DataX会将一个作业下同步的所有的文件视作同一张数据表。用户必须自己保证所有的File能够适配同一套schema信息。并且提供给DataX权限可读。\n必选：是 默认值：无 defaultFS\n描述：Hadoop hdfs文件系统namenode节点地址。 目前HdfsReader已经支持Kerberos认证，如果需要权限认证，则需要用户配置kerberos参数，见下面\n必选：是 默认值：无 fileType\n描述：文件的类型，目前只支持用户配置为\u0026quot;text\u0026rdquo;、\u0026ldquo;orc\u0026rdquo;、\u0026ldquo;rc\u0026rdquo;、\u0026ldquo;seq\u0026rdquo;、\u0026ldquo;csv\u0026rdquo;。 text表示textfile文件格式\norc表示orcfile文件格式\nrc表示rcfile文件格式\nseq表示sequence file文件格式\ncsv表示普通hdfs文件格式（逻辑二维表）\n特别需要注意的是，HdfsReader能够自动识别文件是orcfile、textfile或者还是其它类型的文件，但该项是必填项，HdfsReader则会只读取用户配置的类型的文件，忽略路径下其他格式的文件\n另外需要注意的是，由于textfile和orcfile是两种完全不同的文件格式，所以HdfsReader对这两种文件的解析方式也存在差异，这种差异导致hive支持的复杂复合类型(比如map,array,struct,union)在转换为DataX支持的String类型时，转换的结果格式略有差异，比如以map类型为例：\norcfile map类型经hdfsreader解析转换成datax支持的string类型后，结果为\u0026quot;{job=80, team=60, person=70}\u0026quot;\ntextfile map类型经hdfsreader解析转换成datax支持的string类型后，结果为\u0026quot;job:80,team:60,person:70\u0026quot;\n从上面的转换结果可以看出，数据本身没有变化，但是表示的格式略有差异，所以如果用户配置的文件路径中要同步的字段在Hive中是复合类型的话，建议配置统一的文件格式。\n如果需要统一复合类型解析出来的格式，我们建议用户在hive客户端将textfile格式的表导成orcfile格式的表\n必选：是 默认值：无 column\n描述：读取字段列表，type指定源数据的类型，index指定当前列来自于文本第几列(以0开始)，value指定当前类型为常量，不从源头文件读取数据，而是根据value值自动生成对应的列。 默认情况下，用户可以全部按照String类型读取数据，配置如下：\n\u0026#34;column\u0026#34;: [\u0026#34;*\u0026#34;] 用户可以指定Column字段信息，配置如下：\n{ \u0026ldquo;type\u0026rdquo;: \u0026ldquo;long\u0026rdquo;, \u0026ldquo;index\u0026rdquo;: 0 //从本地文件文本第一列获取int字段 }, { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;alibaba\u0026rdquo; //HdfsReader内部生成alibaba的字符串字段作为当前字段 } ```\n对于用户指定Column信息，type必须填写，index/value必须选择其一。 * 必选：是 \u0026lt;br /\u0026gt; * 默认值：全部按照string类型读取 \u0026lt;br /\u0026gt; fieldDelimiter\n描述：读取的字段分隔符 另外需要注意的是，HdfsReader在读取textfile数据时，需要指定字段分割符，如果不指定默认为\u0026rsquo;,\u0026rsquo;，HdfsReader在读取orcfile时，用户无需指定字段分割符\n必选：否 默认值：, encoding\n描述：读取文件的编码配置。\n必选：否 默认值：utf-8 nullFormat\n描述：文本文件中无法使用标准字符串定义null(空指针)，DataX提供nullFormat定义哪些字符串可以表示为null。\n例如如果用户配置: nullFormat:\u0026quot;\\N\u0026quot;，那么如果源头数据是\u0026quot;\\N\u0026quot;，DataX视作null字段。\n必选：否 默认值：无 haveKerberos\n描述：是否有Kerberos认证，默认false\n例如如果用户配置true，则配置项kerberosKeytabFilePath，kerberosPrincipal为必填。\n必选：haveKerberos 为true必选 默认值：false kerberosKeytabFilePath\n描述：Kerberos认证 keytab文件路径，绝对路径\n必选：否 默认值：无 kerberosPrincipal\n描述：Kerberos认证Principal名，如xxxx/hadoopclient@xxx.xxx 必选：haveKerberos 为true必选 默认值：无 compress\n描述：当fileType（文件类型）为csv下的文件压缩方式，目前仅支持 gzip、bz2、zip、lzo、lzo_deflate、hadoop-snappy、framing-snappy压缩；值得注意的是，lzo存在两种压缩格式：lzo和lzo_deflate，用户在配置的时候需要留心，不要配错了；另外，由于snappy目前没有统一的stream format，datax目前只支持最主流的两种：hadoop-snappy（hadoop上的snappy stream format）和framing-snappy（google建议的snappy stream format）;orc文件类型下无需填写。\n必选：否 默认值：无 hadoopConfig\n描述：hadoopConfig里可以配置与Hadoop相关的一些高级参数，比如HA的配置。\n\u0026#34;hadoopConfig\u0026#34;:{ \u0026#34;dfs.nameservices\u0026#34;: \u0026#34;testDfs\u0026#34;, \u0026#34;dfs.ha.namenodes.testDfs\u0026#34;: \u0026#34;namenode1,namenode2\u0026#34;, \u0026#34;dfs.namenode.rpc-address.aliDfs.namenode1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dfs.namenode.rpc-address.aliDfs.namenode2\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dfs.client.failover.proxy.provider.testDfs\u0026#34;: \u0026#34;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\u0026#34; } 必选：否 默认值：无 csvReaderConfig\n描述：读取CSV类型文件参数配置，Map类型。读取CSV类型文件使用的CsvReader进行读取，会有很多配置，不配置则使用默认值。\n必选：否 默认值：无 常见配置：\n\u0026#34;csvReaderConfig\u0026#34;:{ \u0026#34;safetySwitch\u0026#34;: false, \u0026#34;skipEmptyRecords\u0026#34;: false, \u0026#34;useTextQualifier\u0026#34;: false } 所有配置项及默认值,配置时 csvReaderConfig 的map中请严格按照以下字段名字进行配置：\nboolean caseSensitive = true; char textQualifier = 34; boolean trimWhitespace = true; boolean useTextQualifier = true;//是否使用csv转义字符 char delimiter = 44;//分隔符 char recordDelimiter = 0; char comment = 35; boolean useComments = false; int escapeMode = 1; boolean safetySwitch = true;//单列长度是否限制100000字符 boolean skipEmptyRecords = true;//是否跳过空行 boolean captureRawRecord = true; 3.3 类型转换 由于textfile和orcfile文件表的元数据信息由Hive维护并存放在Hive自己维护的数据库（如mysql）中，目前HdfsReader不支持对Hive元数\n据数据库进行访问查询，因此用户在进行类型转换的时候，必须指定数据类型，如果用户配置的column为\u0026quot;*\u0026quot;，则所有column默认转换为\nstring类型。HdfsReader提供了类型转换的建议表如下：\nDataX 内部类型 Hive表 数据类型 Long TINYINT,SMALLINT,INT,BIGINT Double FLOAT,DOUBLE String String,CHAR,VARCHAR,STRUCT,MAP,ARRAY,UNION,BINARY Boolean BOOLEAN Date Date,TIMESTAMP 其中：\nLong是指Hdfs文件文本中使用整形的字符串表示形式，例如\u0026quot;123456789\u0026quot;。 Double是指Hdfs文件文本中使用Double的字符串表示形式，例如\u0026quot;3.1415\u0026quot;。 Boolean是指Hdfs文件文本中使用Boolean的字符串表示形式，例如\u0026quot;true\u0026quot;、\u0026ldquo;false\u0026rdquo;。不区分大小写。 Date是指Hdfs文件文本中使用Date的字符串表示形式，例如\u0026quot;2014-12-31\u0026quot;。 特别提醒：\nHive支持的数据类型TIMESTAMP可以精确到纳秒级别，所以textfile、orcfile中TIMESTAMP存放的数据类似于\u0026quot;2015-08-21 22:40:47.397898389\u0026quot;，如果转换的类型配置为DataX的Date，转换之后会导致纳秒部分丢失，所以如果需要保留纳秒部分的数据，请配置转换类型为DataX的String类型。 3.4 按分区读取 Hive在建表的时候，可以指定分区partition，例如创建分区partition(day=\u0026ldquo;20150820\u0026rdquo;,hour=\u0026ldquo;09\u0026rdquo;)，对应的hdfs文件系统中，相应的表的目录下则会多出/20150820和/09两个目录，且/20150820是/09的父目录。了解了分区都会列成相应的目录结构，在按照某个分区读取某个表所有数据时，则只需配置好json中path的值即可。\n比如需要读取表名叫mytable01下分区day为20150820这一天的所有数据，则配置如下：\n\u0026#34;path\u0026#34;: \u0026#34;/user/hive/warehouse/mytable01/20150820/*\u0026#34; 4 性能报告 5 约束限制 略\n6 FAQ 如果报java.io.IOException: Maximum column length of 100,000 exceeded in column\u0026hellip;异常信息，说明数据源column字段长度超过了100000字符。 需要在json的reader里增加如下配置\n\u0026#34;csvReaderConfig\u0026#34;:{ \u0026#34;safetySwitch\u0026#34;: false, \u0026#34;skipEmptyRecords\u0026#34;: false, \u0026#34;useTextQualifier\u0026#34;: false } safetySwitch = false;//单列长度不限制100000字符\n","permalink":"http://121.199.2.5:6080/e123c1afd572427f9fa4d27ba10b1e99/","summary":"DataX HdfsReader 插件文档 1 快速介绍 HdfsReader提供了读取分布式文件系统数据存储的能力。在底层实现上，HdfsReader获取分布式文件系统上文件的数据，并转换为DataX传输协议传递给Writer。\n目前HdfsReader支持的文件格式有textfile（text）、orcfile（orc）、rcfile（rc）、sequence file（seq）和普通逻辑二维表（csv）类型格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表。\nHdfsReader需要Jdk1.7及以上版本的支持。\n2 功能与限制 HdfsReader实现了从Hadoop分布式文件系统Hdfs中读取文件数据并转为DataX协议的功能。textfile是Hive建表时默认使用的存储格式，数据不做压缩，本质上textfile就是以文本的形式将数据存放在hdfs中，对于DataX而言，HdfsReader实现上类比TxtFileReader，有诸多相似之处。orcfile，它的全名是Optimized Row Columnar file，是对RCFile做了优化。据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。HdfsReader利用Hive提供的OrcSerde类，读取解析orcfile文件的数据。目前HdfsReader支持的功能如下：\n支持textfile、orcfile、rcfile、sequence file和csv格式的文件，且要求文件内容存放的是一张逻辑意义上的二维表。\n支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量\n支持递归读取、支持正则表达式（\u0026quot;*\u0026ldquo;和\u0026rdquo;?\u0026quot;）。\n支持orcfile数据压缩，目前支持SNAPPY，ZLIB两种压缩方式。\n多个File可以支持并发读取。\n支持sequence file数据压缩，目前支持lzo压缩方式。\ncsv类型支持压缩格式有：gzip、bz2、zip、lzo、lzo_deflate、snappy。\n目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试；\n支持kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效）\n我们暂时不能做到：\n单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。 目前还不支持hdfs HA; 3 功能说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hdfsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/user/hive/warehouse/mytable01/*\u0026#34;, \u0026#34;defaultFS\u0026#34;: \u0026#34;hdfs://xxx:port\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;hello\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } ], \u0026#34;fileType\u0026#34;: \u0026#34;orc\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true } } } ] } } 3.","title":"DataX HdfsReader 插件文档"},{"content":"DataX HdfsWriter 插件文档 1 快速介绍 HdfsWriter提供向HDFS文件系统指定路径中写入TEXTFile文件和ORCFile文件,文件内容可与hive中表关联。\n2 功能与限制 (1)、目前HdfsWriter仅支持textfile和orcfile两种格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表; (2)、由于HDFS是文件系统，不存在schema的概念，因此不支持对部分列写入; (3)、目前仅支持与以下Hive数据类型： 数值型：TINYINT,SMALLINT,INT,BIGINT,FLOAT,DOUBLE 字符串类型：STRING,VARCHAR,CHAR 布尔类型：BOOLEAN 时间类型：DATE,TIMESTAMP 目前不支持：decimal、binary、arrays、maps、structs、union类型; (4)、对于Hive分区表目前仅支持一次写入单个分区; (5)、对于textfile需用户保证写入hdfs文件的分隔符与在Hive上创建表时的分隔符一致,从而实现写入hdfs数据与Hive表字段关联; (6)、HdfsWriter实现过程是：首先根据用户指定的path，创建一个hdfs文件系统上不存在的临时目录，创建规则：path_随机；然后将读取的文件写入这个临时目录；全部写入后再将这个临时目录下的文件移动到用户指定目录（在创建文件时保证文件名不重复）; 最后删除临时目录。如果在中间过程发生网络中断等情况造成无法与hdfs建立连接，需要用户手动删除已经写入的文件和临时目录。 (7)、目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试； (8)、目前HdfsWriter支持Kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效） 3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: [\u0026#34;/Users/shf/workplace/txtWorkplace/job/dataorcfull.txt\u0026#34;], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;DOUBLE\u0026#34; }, { \u0026#34;index\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;DOUBLE\u0026#34; }, { \u0026#34;index\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; }, { \u0026#34;index\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; }, { \u0026#34;index\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; }, { \u0026#34;index\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;BOOLEAN\u0026#34; }, { \u0026#34;index\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;index\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;\\t\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hdfswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;defaultFS\u0026#34;: \u0026#34;hdfs://xxx:port\u0026#34;, \u0026#34;fileType\u0026#34;: \u0026#34;orc\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/user/hive/warehouse/writerorc.db/orcfull\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;col1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;TINYINT\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;SMALLINT\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col3\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;INT\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col4\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;BIGINT\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col5\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FLOAT\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col6\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DOUBLE\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col7\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col8\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;VARCHAR\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col9\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;CHAR\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col10\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;BOOLEAN\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col11\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;col12\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;TIMESTAMP\u0026#34; } ], \u0026#34;writeMode\u0026#34;: \u0026#34;append\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;\\t\u0026#34;, \u0026#34;compress\u0026#34;:\u0026#34;NONE\u0026#34; } } } ] } } 3.2 参数说明 defaultFS\n描述：Hadoop hdfs文件系统namenode节点地址。格式：hdfs://ip:端口；例如：hdfs://127.0.0.1:9000\n必选：是 默认值：无 fileType\n描述：文件的类型，目前只支持用户配置为\u0026quot;text\u0026quot;或\u0026quot;orc\u0026quot;。 text表示textfile文件格式\norc表示orcfile文件格式\n必选：是 默认值：无 path\n描述：存储到Hadoop hdfs文件系统的路径信息，HdfsWriter会根据并发配置在Path目录下写入多个文件。为与hive表关联，请填写hive表在hdfs上的存储路径。例：Hive上设置的数据仓库的存储路径为：/user/hive/warehouse/ ，已建立数据库：test，表：hello；则对应的存储路径为：/user/hive/warehouse/test.db/hello 必选：是 默认值：无 fileName\n描述：HdfsWriter写入时的文件名，实际执行时会在该文件名后添加随机的后缀作为每个线程写入实际文件名。 必选：是 默认值：无 column\n描述：写入数据的字段，不支持对部分列写入。为与hive中表关联，需要指定表中所有字段名和字段类型，其中：name指定字段名，type指定字段类型。 用户可以指定Column字段信息，配置如下：\n\u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;userName\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; } ] 必选：是 默认值：无 writeMode\n描述：hdfswriter写入前数据清理处理模式： append，写入前不做任何处理，DataX hdfswriter直接使用filename写入，并保证文件名不冲突。 nonConflict，如果目录下有fileName前缀的文件，直接报错。 必选：是 默认值：无 fieldDelimiter\n描述：hdfswriter写入时的字段分隔符,需要用户保证与创建的Hive表的字段分隔符一致，否则无法在Hive表中查到数据 必选：是 默认值：无 compress\n描述：hdfs文件压缩类型，默认不填写意味着没有压缩。其中：text类型文件支持压缩类型有gzip、bzip2;orc类型文件支持的压缩类型有NONE、SNAPPY（需要用户安装SnappyCodec）。 必选：否 默认值：无压缩 hadoopConfig\n描述：hadoopConfig里可以配置与Hadoop相关的一些高级参数，比如HA的配置。\n\u0026#34;hadoopConfig\u0026#34;:{ \u0026#34;dfs.nameservices\u0026#34;: \u0026#34;testDfs\u0026#34;, \u0026#34;dfs.ha.namenodes.testDfs\u0026#34;: \u0026#34;namenode1,namenode2\u0026#34;, \u0026#34;dfs.namenode.rpc-address.aliDfs.namenode1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dfs.namenode.rpc-address.aliDfs.namenode2\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dfs.client.failover.proxy.provider.testDfs\u0026#34;: \u0026#34;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\u0026#34; } 必选：否 默认值：无 encoding\n描述：写文件的编码配置。\n必选：否 默认值：utf-8，慎重修改 haveKerberos\n描述：是否有Kerberos认证，默认false\n例如如果用户配置true，则配置项kerberosKeytabFilePath，kerberosPrincipal为必填。\n必选：haveKerberos 为true必选 默认值：false kerberosKeytabFilePath\n描述：Kerberos认证 keytab文件路径，绝对路径\n必选：否 默认值：无 kerberosPrincipal\n描述：Kerberos认证Principal名，如xxxx/hadoopclient@xxx.xxx 必选：haveKerberos 为true必选 默认值：无 3.3 类型转换 目前 HdfsWriter 支持大部分 Hive 类型，请注意检查你的类型。\n下面列出 HdfsWriter 针对 Hive 数据类型转换列表:\nDataX 内部类型 HIVE 数据类型 Long TINYINT,SMALLINT,INT,BIGINT Double FLOAT,DOUBLE String STRING,VARCHAR,CHAR Boolean BOOLEAN Date DATE,TIMESTAMP 4 配置步骤 步骤一、在Hive中创建数据库、表 Hive数据库在HDFS上存储配置,在hive安装目录下 conf/hive-site.xml文件中配置，默认值为：/user/hive/warehouse 如下所示： \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/user/hive/warehouse\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;location of default database for the warehouse\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; Hive建库／建表语法 参考 Hive操作手册\n例： （1）建立存储为textfile文件类型的表\ncreate database IF NOT EXISTS hdfswriter; use hdfswriter; create table text_table( col1 TINYINT, col2 SMALLINT, col3 INT, col4 BIGINT, col5 FLOAT, col6 DOUBLE, col7 STRING, col8 VARCHAR(10), col9 CHAR(10), col10 BOOLEAN, col11 date, col12 TIMESTAMP ) row format delimited fields terminated by \u0026#34;\\t\u0026#34; STORED AS TEXTFILE; text_table在hdfs上存储路径为：/user/hive/warehouse/hdfswriter.db/text_table/\n（2）建立存储为orcfile文件类型的表\ncreate database IF NOT EXISTS hdfswriter; use hdfswriter; create table orc_table( col1 TINYINT, col2 SMALLINT, col3 INT, col4 BIGINT, col5 FLOAT, col6 DOUBLE, col7 STRING, col8 VARCHAR(10), col9 CHAR(10), col10 BOOLEAN, col11 date, col12 TIMESTAMP ) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;\\t\u0026#39; STORED AS ORC; orc_table在hdfs上存储路径为：/user/hive/warehouse/hdfswriter.db/orc_table/\n步骤二、根据步骤一的配置信息配置HdfsWriter作业 5 约束限制 略\n6 FAQ 略\n","permalink":"http://121.199.2.5:6080/dd5ebd2f221f4913ae09f61f3877725e/","summary":"DataX HdfsWriter 插件文档 1 快速介绍 HdfsWriter提供向HDFS文件系统指定路径中写入TEXTFile文件和ORCFile文件,文件内容可与hive中表关联。\n2 功能与限制 (1)、目前HdfsWriter仅支持textfile和orcfile两种格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表; (2)、由于HDFS是文件系统，不存在schema的概念，因此不支持对部分列写入; (3)、目前仅支持与以下Hive数据类型： 数值型：TINYINT,SMALLINT,INT,BIGINT,FLOAT,DOUBLE 字符串类型：STRING,VARCHAR,CHAR 布尔类型：BOOLEAN 时间类型：DATE,TIMESTAMP 目前不支持：decimal、binary、arrays、maps、structs、union类型; (4)、对于Hive分区表目前仅支持一次写入单个分区; (5)、对于textfile需用户保证写入hdfs文件的分隔符与在Hive上创建表时的分隔符一致,从而实现写入hdfs数据与Hive表字段关联; (6)、HdfsWriter实现过程是：首先根据用户指定的path，创建一个hdfs文件系统上不存在的临时目录，创建规则：path_随机；然后将读取的文件写入这个临时目录；全部写入后再将这个临时目录下的文件移动到用户指定目录（在创建文件时保证文件名不重复）; 最后删除临时目录。如果在中间过程发生网络中断等情况造成无法与hdfs建立连接，需要用户手动删除已经写入的文件和临时目录。 (7)、目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试； (8)、目前HdfsWriter支持Kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效） 3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: [\u0026#34;/Users/shf/workplace/txtWorkplace/job/dataorcfull.txt\u0026#34;], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;DOUBLE\u0026#34; }, { \u0026#34;index\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;DOUBLE\u0026#34; }, { \u0026#34;index\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; }, { \u0026#34;index\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; }, { \u0026#34;index\u0026#34;: 8, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; }, { \u0026#34;index\u0026#34;: 9, \u0026#34;type\u0026#34;: \u0026#34;BOOLEAN\u0026#34; }, { \u0026#34;index\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;index\u0026#34;: 11, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;\\t\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hdfswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;defaultFS\u0026#34;: \u0026#34;hdfs://xxx:port\u0026#34;, \u0026#34;fileType\u0026#34;: \u0026#34;orc\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/user/hive/warehouse/writerorc.","title":"DataX HdfsWriter 插件文档"},{"content":"DataX KingbaseesWriter 1 快速介绍 KingbaseesWriter插件实现了写入数据到 KingbaseES主库目的表的功能。在底层实现上，KingbaseesWriter通过JDBC连接远程 KingbaseES 数据库，并执行相应的 insert into \u0026hellip; sql 语句将数据写入 KingbaseES，内部会分批次提交入库。\nKingbaseesWriter面向ETL开发工程师，他们使用KingbaseesWriter从数仓导入数据到KingbaseES。同时 KingbaseesWriter亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 KingbaseesWriter通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL插入语句\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2. KingbaseesWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 KingbaseesWriter导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kingbaseeswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:kingbase8://127.0.0.1:3002/datax\u0026#34;, \u0026#34;table\u0026#34;: [ \u0026#34;test\u0026#34; ] } ] } } } ] } } 3.2 参数说明 jdbcUrl\n描述：目的数据库的 JDBC 连接信息 ,jdbcUrl必须包含在connection配置单元中。\n注意：1、在一个数据库上只能配置一个值。 2、jdbcUrl按照KingbaseES官方规范，并可以填写连接附加参数信息。具体请参看KingbaseES官方文档或者咨询对应 DBA。\n必选：是 默认值：无 username\n描述：目的数据库的用户名 必选：是 默认值：无 password\n描述：目的数据库的密码 必选：是 默认值：无 table\n描述：目的表的表名称。支持写入一个或者多个表。当配置为多张表时，必须确保所有表结构保持一致。\n注意：table 和 jdbcUrl 必须包含在 connection 配置单元中 必选：是 默认值：无 column\n描述：目的表需要写入数据的字段,字段之间用英文逗号分隔。例如: \u0026ldquo;column\u0026rdquo;: [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;,\u0026ldquo;age\u0026rdquo;]。如果要依次写入全部列，使用*表示, 例如: \u0026ldquo;column\u0026rdquo;: [\u0026quot;*\u0026quot;]\n注意：1、我们强烈不推荐你这样配置，因为当你目的表字段个数、类型等有改动时，你的任务可能运行不正确或者失败 2、此处 column 不能配置任何常量值 必选：是 默认值：否 preSql\n描述：写入数据到目的表前，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，请使用 @table 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。比如你的任务是要写入到目的端的100个同构分表(表名称为:datax_00,datax01, \u0026hellip; datax_98,datax_99)，并且你希望导入数据前，先对表中数据进行删除操作，那么你可以这样配置：\u0026quot;preSql\u0026quot;:[\u0026quot;delete from @table\u0026quot;]，效果是：在执行到每个表写入数据前，会先执行对应的 delete from 对应表名称 必选：否 默认值：无 postSql\n描述：写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ） 必选：否 默认值：无 batchSize\n描述：一次性批量提交的记录数大小，该值可以极大减少DataX与KingbaseES的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。\n必选：否 默认值：1024 3.3 类型转换 目前 KingbaseesWriter支持大部分 KingbaseES类型，但也存在部分没有支持的情况，请注意检查你的类型。\n下面列出 KingbaseesWriter针对 KingbaseES类型转换列表:\nDataX 内部类型 KingbaseES 数据类型 Long bigint, bigserial, integer, smallint, serial Double double precision, money, numeric, real String varchar, char, text, bit Date date, time, timestamp Boolean bool Bytes bytea FAQ Q: KingbaseesWriter 执行 postSql 语句报错，那么数据导入到目标数据库了吗?\nA: DataX 导入过程存在三块逻辑，pre 操作、导入操作、post 操作，其中任意一环报错，DataX 作业报错。由于 DataX 不能保证在同一个事务完成上述几个操作，因此有可能数据已经落入到目标端。\nQ: 按照上述说法，那么有部分脏数据导入数据库，如果影响到线上数据库怎么办?\nA: 目前有两种解法，第一种配置 pre 语句，该 sql 可以清理当天导入数据， DataX 每次导入时候可以把上次清理干净并导入完整数据。 第二种，向临时表导入数据，完成后再 rename 到线上表。\n","permalink":"http://121.199.2.5:6080/a50c2cb341e14f7d995d470380e24d34/","summary":"DataX KingbaseesWriter 1 快速介绍 KingbaseesWriter插件实现了写入数据到 KingbaseES主库目的表的功能。在底层实现上，KingbaseesWriter通过JDBC连接远程 KingbaseES 数据库，并执行相应的 insert into \u0026hellip; sql 语句将数据写入 KingbaseES，内部会分批次提交入库。\nKingbaseesWriter面向ETL开发工程师，他们使用KingbaseesWriter从数仓导入数据到KingbaseES。同时 KingbaseesWriter亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 KingbaseesWriter通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL插入语句\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2. KingbaseesWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 KingbaseesWriter导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kingbaseeswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:kingbase8://127.","title":"DataX KingbaseesWriter"},{"content":"Datax MongoDBReader 1 快速介绍 MongoDBReader 插件利用 MongoDB 的java客户端MongoClient进行MongoDB的读操作。最新版本的Mongo已经将DB锁的粒度从DB级别降低到document级别，配合上MongoDB强大的索引功能，基本可以达到高性能的读取MongoDB的需求。\n2 实现原理 MongoDBReader通过Datax框架从MongoDB并行的读取数据，通过主控的JOB程序按照指定的规则对MongoDB中的数据进行分片，并行读取，然后将MongoDB支持的类型通过逐一判断转换成Datax支持的类型。\n3 功能说明 该示例从ODPS读一份数据到MongoDB。\n{ \u0026quot;job\u0026quot;: { \u0026quot;setting\u0026quot;: { \u0026quot;speed\u0026quot;: { \u0026quot;channel\u0026quot;: 2 } }, \u0026quot;content\u0026quot;: [ { \u0026quot;reader\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;mongodbreader\u0026quot;, \u0026quot;parameter\u0026quot;: { \u0026quot;address\u0026quot;: [\u0026quot;127.0.0.1:27017\u0026quot;], \u0026quot;userName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;userPassword\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;dbName\u0026quot;: \u0026quot;tag_per_data\u0026quot;, \u0026quot;collectionName\u0026quot;: \u0026quot;tag_data12\u0026quot;, \u0026quot;column\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;unique_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;sid\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;user_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;auction_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;content_type\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;pool_type\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;frontcat_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;spliter\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;categoryid\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;spliter\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;gmt_create\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;taglist\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;spliter\u0026quot;: \u0026quot; \u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;property\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scorea\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scoreb\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scorec\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; } ] } }, \u0026quot;writer\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;odpswriter\u0026quot;, \u0026quot;parameter\u0026quot;: { \u0026quot;project\u0026quot;: \u0026quot;tb_ai_recommendation\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;jianying_tag_datax_read_test01\u0026quot;, \u0026quot;column\u0026quot;: [ \u0026quot;unique_id\u0026quot;, \u0026quot;sid\u0026quot;, \u0026quot;user_id\u0026quot;, \u0026quot;auction_id\u0026quot;, \u0026quot;content_type\u0026quot;, \u0026quot;pool_type\u0026quot;, \u0026quot;frontcat_id\u0026quot;, \u0026quot;categoryid\u0026quot;, \u0026quot;gmt_create\u0026quot;, \u0026quot;taglist\u0026quot;, \u0026quot;property\u0026quot;, \u0026quot;scorea\u0026quot;, \u0026quot;scoreb\u0026quot; ], \u0026quot;accessId\u0026quot;: \u0026quot;**************\u0026quot;, \u0026quot;accessKey\u0026quot;: \u0026quot;********************\u0026quot;, \u0026quot;truncate\u0026quot;: true, \u0026quot;odpsServer\u0026quot;: \u0026quot;xxx/api\u0026quot;, \u0026quot;tunnelServer\u0026quot;: \u0026quot;xxx\u0026quot;, \u0026quot;accountType\u0026quot;: \u0026quot;aliyun\u0026quot; } } } ] } } 4 参数说明 address： MongoDB的数据地址信息，因为MonogDB可能是个集群，则ip端口信息需要以Json数组的形式给出。【必填】 userName：MongoDB的用户名。【选填】 userPassword： MongoDB的密码。【选填】 collectionName： MonogoDB的集合名。【必填】 column：MongoDB的文档列名。【必填】 name：Column的名字。【必填】 type：Column的类型。【选填】 splitter：因为MongoDB支持数组类型，但是Datax框架本身不支持数组类型，所以mongoDB读出来的数组类型要通过这个分隔符合并成字符串。【选填】 query: MongoDB的额外查询条件。【选填】 5 类型转换 DataX 内部类型 MongoDB 数据类型 Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes 6 性能报告 7 测试报告 ","permalink":"http://121.199.2.5:6080/41dfc822f8ca4bca812501f2801fc78e/","summary":"Datax MongoDBReader 1 快速介绍 MongoDBReader 插件利用 MongoDB 的java客户端MongoClient进行MongoDB的读操作。最新版本的Mongo已经将DB锁的粒度从DB级别降低到document级别，配合上MongoDB强大的索引功能，基本可以达到高性能的读取MongoDB的需求。\n2 实现原理 MongoDBReader通过Datax框架从MongoDB并行的读取数据，通过主控的JOB程序按照指定的规则对MongoDB中的数据进行分片，并行读取，然后将MongoDB支持的类型通过逐一判断转换成Datax支持的类型。\n3 功能说明 该示例从ODPS读一份数据到MongoDB。\n{ \u0026quot;job\u0026quot;: { \u0026quot;setting\u0026quot;: { \u0026quot;speed\u0026quot;: { \u0026quot;channel\u0026quot;: 2 } }, \u0026quot;content\u0026quot;: [ { \u0026quot;reader\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;mongodbreader\u0026quot;, \u0026quot;parameter\u0026quot;: { \u0026quot;address\u0026quot;: [\u0026quot;127.0.0.1:27017\u0026quot;], \u0026quot;userName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;userPassword\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;dbName\u0026quot;: \u0026quot;tag_per_data\u0026quot;, \u0026quot;collectionName\u0026quot;: \u0026quot;tag_data12\u0026quot;, \u0026quot;column\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;unique_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;sid\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;user_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;auction_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;content_type\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;pool_type\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;frontcat_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;spliter\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;categoryid\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;spliter\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;gmt_create\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;taglist\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;spliter\u0026quot;: \u0026quot; \u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;property\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scorea\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scoreb\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scorec\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; } ] } }, \u0026quot;writer\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;odpswriter\u0026quot;, \u0026quot;parameter\u0026quot;: { \u0026quot;project\u0026quot;: \u0026quot;tb_ai_recommendation\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;jianying_tag_datax_read_test01\u0026quot;, \u0026quot;column\u0026quot;: [ \u0026quot;unique_id\u0026quot;, \u0026quot;sid\u0026quot;, \u0026quot;user_id\u0026quot;, \u0026quot;auction_id\u0026quot;, \u0026quot;content_type\u0026quot;, \u0026quot;pool_type\u0026quot;, \u0026quot;frontcat_id\u0026quot;, \u0026quot;categoryid\u0026quot;, \u0026quot;gmt_create\u0026quot;, \u0026quot;taglist\u0026quot;, \u0026quot;property\u0026quot;, \u0026quot;scorea\u0026quot;, \u0026quot;scoreb\u0026quot; ], \u0026quot;accessId\u0026quot;: \u0026quot;**************\u0026quot;, \u0026quot;accessKey\u0026quot;: \u0026quot;********************\u0026quot;, \u0026quot;truncate\u0026quot;: true, \u0026quot;odpsServer\u0026quot;: \u0026quot;xxx/api\u0026quot;, \u0026quot;tunnelServer\u0026quot;: \u0026quot;xxx\u0026quot;, \u0026quot;accountType\u0026quot;: \u0026quot;aliyun\u0026quot; } } } ] } } 4 参数说明 address： MongoDB的数据地址信息，因为MonogDB可能是个集群，则ip端口信息需要以Json数组的形式给出。【必填】 userName：MongoDB的用户名。【选填】 userPassword： MongoDB的密码。【选填】 collectionName： MonogoDB的集合名。【必填】 column：MongoDB的文档列名。【必填】 name：Column的名字。【必填】 type：Column的类型。【选填】 splitter：因为MongoDB支持数组类型，但是Datax框架本身不支持数组类型，所以mongoDB读出来的数组类型要通过这个分隔符合并成字符串。【选填】 query: MongoDB的额外查询条件。【选填】 5 类型转换 DataX 内部类型 MongoDB 数据类型 Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes 6 性能报告 7 测试报告 ","title":"Datax MongoDBReader"},{"content":"Datax MongoDBWriter 1 快速介绍 MongoDBWriter 插件利用 MongoDB 的java客户端MongoClient进行MongoDB的写操作。最新版本的Mongo已经将DB锁的粒度从DB级别降低到document级别，配合上MongoDB强大的索引功能，基本可以满足数据源向MongoDB写入数据的需求，针对数据更新的需求，通过配置业务主键的方式也可以实现。\n2 实现原理 MongoDBWriter通过Datax框架获取Reader生成的数据，然后将Datax支持的类型通过逐一判断转换成MongoDB支持的类型。其中一个值得指出的点就是Datax本身不支持数组类型，但是MongoDB支持数组类型，并且数组类型的索引还是蛮强大的。为了使用MongoDB的数组类型，则可以通过参数的特殊配置，将字符串可以转换成MongoDB中的数组。类型转换之后，就可以依托于Datax框架并行的写入MongoDB。\n3 功能说明 该示例从ODPS读一份数据到MongoDB。\n{ \u0026quot;job\u0026quot;: { \u0026quot;setting\u0026quot;: { \u0026quot;speed\u0026quot;: { \u0026quot;channel\u0026quot;: 2 } }, \u0026quot;content\u0026quot;: [ { \u0026quot;reader\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;odpsreader\u0026quot;, \u0026quot;parameter\u0026quot;: { \u0026quot;accessId\u0026quot;: \u0026quot;********\u0026quot;, \u0026quot;accessKey\u0026quot;: \u0026quot;*********\u0026quot;, \u0026quot;project\u0026quot;: \u0026quot;tb_ai_recommendation\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;jianying_tag_datax_test\u0026quot;, \u0026quot;column\u0026quot;: [ \u0026quot;unique_id\u0026quot;, \u0026quot;sid\u0026quot;, \u0026quot;user_id\u0026quot;, \u0026quot;auction_id\u0026quot;, \u0026quot;content_type\u0026quot;, \u0026quot;pool_type\u0026quot;, \u0026quot;frontcat_id\u0026quot;, \u0026quot;categoryid\u0026quot;, \u0026quot;gmt_create\u0026quot;, \u0026quot;taglist\u0026quot;, \u0026quot;property\u0026quot;, \u0026quot;scorea\u0026quot;, \u0026quot;scoreb\u0026quot; ], \u0026quot;splitMode\u0026quot;: \u0026quot;record\u0026quot;, \u0026quot;odpsServer\u0026quot;: \u0026quot;http://xxx/api\u0026quot; } }, \u0026quot;writer\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;mongodbwriter\u0026quot;, \u0026quot;parameter\u0026quot;: { \u0026quot;address\u0026quot;: [ \u0026quot;127.0.0.1:27017\u0026quot; ], \u0026quot;userName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;userPassword\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;dbName\u0026quot;: \u0026quot;tag_per_data\u0026quot;, \u0026quot;collectionName\u0026quot;: \u0026quot;tag_data\u0026quot;, \u0026quot;column\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;unique_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;sid\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;user_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;auction_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;content_type\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;pool_type\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;frontcat_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;splitter\u0026quot;: \u0026quot; \u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;categoryid\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;splitter\u0026quot;: \u0026quot; \u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;gmt_create\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;taglist\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;splitter\u0026quot;: \u0026quot; \u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;property\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scorea\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scoreb\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scorec\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; } ], \u0026quot;upsertInfo\u0026quot;: { \u0026quot;isUpsert\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;upsertKey\u0026quot;: \u0026quot;unique_id\u0026quot; } } } } ] } } 4 参数说明 address： MongoDB的数据地址信息，因为MonogDB可能是个集群，则ip端口信息需要以Json数组的形式给出。【必填】 userName：MongoDB的用户名。【选填】 userPassword： MongoDB的密码。【选填】 collectionName： MonogoDB的集合名。【必填】 column：MongoDB的文档列名。【必填】 name：Column的名字。【必填】 type：Column的类型。【选填】 splitter：特殊分隔符，当且仅当要处理的字符串要用分隔符分隔为字符数组时，才使用这个参数，通过这个参数指定的分隔符，将字符串分隔存储到MongoDB的数组中。【选填】 upsertInfo：指定了传输数据时更新的信息。【选填】 isUpsert：当设置为true时，表示针对相同的upsertKey做更新操作。【选填】 upsertKey：upsertKey指定了没行记录的业务主键。用来做更新时使用。【选填】 5 类型转换 DataX 内部类型 MongoDB 数据类型 Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes 6 性能报告 7 测试报告 ","permalink":"http://121.199.2.5:6080/1b755458e9fa4e6f89a7d44321cd92ec/","summary":"Datax MongoDBWriter 1 快速介绍 MongoDBWriter 插件利用 MongoDB 的java客户端MongoClient进行MongoDB的写操作。最新版本的Mongo已经将DB锁的粒度从DB级别降低到document级别，配合上MongoDB强大的索引功能，基本可以满足数据源向MongoDB写入数据的需求，针对数据更新的需求，通过配置业务主键的方式也可以实现。\n2 实现原理 MongoDBWriter通过Datax框架获取Reader生成的数据，然后将Datax支持的类型通过逐一判断转换成MongoDB支持的类型。其中一个值得指出的点就是Datax本身不支持数组类型，但是MongoDB支持数组类型，并且数组类型的索引还是蛮强大的。为了使用MongoDB的数组类型，则可以通过参数的特殊配置，将字符串可以转换成MongoDB中的数组。类型转换之后，就可以依托于Datax框架并行的写入MongoDB。\n3 功能说明 该示例从ODPS读一份数据到MongoDB。\n{ \u0026quot;job\u0026quot;: { \u0026quot;setting\u0026quot;: { \u0026quot;speed\u0026quot;: { \u0026quot;channel\u0026quot;: 2 } }, \u0026quot;content\u0026quot;: [ { \u0026quot;reader\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;odpsreader\u0026quot;, \u0026quot;parameter\u0026quot;: { \u0026quot;accessId\u0026quot;: \u0026quot;********\u0026quot;, \u0026quot;accessKey\u0026quot;: \u0026quot;*********\u0026quot;, \u0026quot;project\u0026quot;: \u0026quot;tb_ai_recommendation\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;jianying_tag_datax_test\u0026quot;, \u0026quot;column\u0026quot;: [ \u0026quot;unique_id\u0026quot;, \u0026quot;sid\u0026quot;, \u0026quot;user_id\u0026quot;, \u0026quot;auction_id\u0026quot;, \u0026quot;content_type\u0026quot;, \u0026quot;pool_type\u0026quot;, \u0026quot;frontcat_id\u0026quot;, \u0026quot;categoryid\u0026quot;, \u0026quot;gmt_create\u0026quot;, \u0026quot;taglist\u0026quot;, \u0026quot;property\u0026quot;, \u0026quot;scorea\u0026quot;, \u0026quot;scoreb\u0026quot; ], \u0026quot;splitMode\u0026quot;: \u0026quot;record\u0026quot;, \u0026quot;odpsServer\u0026quot;: \u0026quot;http://xxx/api\u0026quot; } }, \u0026quot;writer\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;mongodbwriter\u0026quot;, \u0026quot;parameter\u0026quot;: { \u0026quot;address\u0026quot;: [ \u0026quot;127.0.0.1:27017\u0026quot; ], \u0026quot;userName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;userPassword\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;dbName\u0026quot;: \u0026quot;tag_per_data\u0026quot;, \u0026quot;collectionName\u0026quot;: \u0026quot;tag_data\u0026quot;, \u0026quot;column\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;unique_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;sid\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;user_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;auction_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;content_type\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;pool_type\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;frontcat_id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;splitter\u0026quot;: \u0026quot; \u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;categoryid\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;splitter\u0026quot;: \u0026quot; \u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;gmt_create\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;taglist\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Array\u0026quot;, \u0026quot;splitter\u0026quot;: \u0026quot; \u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;property\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scorea\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scoreb\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;scorec\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; } ], \u0026quot;upsertInfo\u0026quot;: { \u0026quot;isUpsert\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;upsertKey\u0026quot;: \u0026quot;unique_id\u0026quot; } } } } ] } } 4 参数说明 address： MongoDB的数据地址信息，因为MonogDB可能是个集群，则ip端口信息需要以Json数组的形式给出。【必填】 userName：MongoDB的用户名。【选填】 userPassword： MongoDB的密码。【选填】 collectionName： MonogoDB的集合名。【必填】 column：MongoDB的文档列名。【必填】 name：Column的名字。【必填】 type：Column的类型。【选填】 splitter：特殊分隔符，当且仅当要处理的字符串要用分隔符分隔为字符数组时，才使用这个参数，通过这个参数指定的分隔符，将字符串分隔存储到MongoDB的数组中。【选填】 upsertInfo：指定了传输数据时更新的信息。【选填】 isUpsert：当设置为true时，表示针对相同的upsertKey做更新操作。【选填】 upsertKey：upsertKey指定了没行记录的业务主键。用来做更新时使用。【选填】 5 类型转换 DataX 内部类型 MongoDB 数据类型 Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes 6 性能报告 7 测试报告 ","title":"Datax MongoDBWriter"},{"content":"DataX MysqlWriter 1 快速介绍 MysqlWriter 插件实现了写入数据到 Mysql 主库的目的表的功能。在底层实现上， MysqlWriter 通过 JDBC 连接远程 Mysql 数据库，并执行相应的 insert into \u0026hellip; 或者 ( replace into \u0026hellip;) 的 sql 语句将数据写入 Mysql，内部会分批次提交入库，需要数据库本身采用 innodb 引擎。\nMysqlWriter 面向ETL开发工程师，他们使用 MysqlWriter 从数仓导入数据到 Mysql。同时 MysqlWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 MysqlWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置的 writeMode 生成\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 或者 replace into...(没有遇到主键/唯一性索引冲突时，与 insert into 行为一致，冲突时会用新行替换原有行所有字段) 的语句写入数据到 Mysql。出于性能考虑，采用了 PreparedStatement + Batch，并且设置了：rewriteBatchedStatements=true，将数据缓冲到线程上下文 Buffer 中，当 Buffer 累计到预定阈值时，才发起写入请求。 注意：目的表所在数据库必须是主库才能写入数据；整个任务至少需要具备 insert/replace into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 Mysql 导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mysqlwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;session\u0026#34;: [ \u0026#34;set session sql_mode=\u0026#39;ANSI\u0026#39;\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:mysql://127.0.0.1:3306/datax?useUnicode=true\u0026amp;characterEncoding=gbk\u0026#34;, \u0026#34;table\u0026#34;: [ \u0026#34;test\u0026#34; ] } ] } } } ] } } 3.2 参数说明 jdbcUrl\n描述：目的数据库的 JDBC 连接信息。作业运行时，DataX 会在你提供的 jdbcUrl 后面追加如下属性：yearIsDateType=false\u0026amp;zeroDateTimeBehavior=convertToNull\u0026amp;rewriteBatchedStatements=true\n注意：1、在一个数据库上只能配置一个 jdbcUrl 值。这与 MysqlReader 支持多个备库探测不同，因为此处不支持同一个数据库存在多个主库的情况(双主导入数据情况) 2、jdbcUrl按照Mysql官方规范，并可以填写连接附加控制信息，比如想指定连接编码为 gbk ，则在 jdbcUrl 后面追加属性 useUnicode=true\u0026amp;characterEncoding=gbk。具体请参看 Mysql官方文档或者咨询对应 DBA。 必选：是 默认值：无 username\n描述：目的数据库的用户名 必选：是 默认值：无 password\n描述：目的数据库的密码 必选：是 默认值：无 table\n描述：目的表的表名称。支持写入一个或者多个表。当配置为多张表时，必须确保所有表结构保持一致。\n注意：table 和 jdbcUrl 必须包含在 connection 配置单元中 必选：是 默认值：无 column\n描述：目的表需要写入数据的字段,字段之间用英文逗号分隔。例如: \u0026ldquo;column\u0026rdquo;: [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;,\u0026ldquo;age\u0026rdquo;]。如果要依次写入全部列，使用*表示, 例如: \u0026quot;column\u0026quot;: [\u0026quot;*\u0026quot;]。\n**column配置项必须指定，不能留空！** 注意：1、我们强烈不推荐你这样配置，因为当你目的表字段个数、类型等有改动时，你的任务可能运行不正确或者失败 2、 column 不能配置任何常量值 必选：是 默认值：否 session\n描述: DataX在获取Mysql连接时，执行session指定的SQL语句，修改当前connection session属性\n必须: 否\n默认值: 空\npreSql\n描述：写入数据到目的表前，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，请使用 @table 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。比如你的任务是要写入到目的端的100个同构分表(表名称为:datax_00,datax01, \u0026hellip; datax_98,datax_99)，并且你希望导入数据前，先对表中数据进行删除操作，那么你可以这样配置：\u0026quot;preSql\u0026quot;:[\u0026quot;delete from 表名\u0026quot;]，效果是：在执行到每个表写入数据前，会先执行对应的 delete from 对应表名称 必选：否 默认值：无 postSql\n描述：写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ） 必选：否 默认值：无 writeMode\n描述：控制写入数据到目标表采用 insert into 或者 replace into 或者 ON DUPLICATE KEY UPDATE 语句\n必选：是 所有选项：insert/replace/update 默认值：insert batchSize\n描述：一次性批量提交的记录数大小，该值可以极大减少DataX与Mysql的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。\n必选：否 默认值：1024 3.3 类型转换 类似 MysqlReader ，目前 MysqlWriter 支持大部分 Mysql 类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出 MysqlWriter 针对 Mysql 类型转换列表:\nDataX 内部类型 Mysql 数据类型 Long int, tinyint, smallint, mediumint, int, bigint, year Double float, double, decimal String varchar, char, tinytext, text, mediumtext, longtext Date date, datetime, timestamp, time Boolean bit, bool Bytes tinyblob, mediumblob, blob, longblob, varbinary bit类型目前是未定义类型转换 4 性能报告 4.1 环境准备 4.1.1 数据特征 建表语句：\nCREATE TABLE `datax_mysqlwriter_perf_00` ( `biz_order_id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `key_value` varchar(4000) NOT NULL COMMENT 'Key-value的内容', `gmt_create` datetime NOT NULL COMMENT '创建时间', `gmt_modified` datetime NOT NULL COMMENT '修改时间', `attribute_cc` int(11) DEFAULT NULL COMMENT '防止并发修改的标志', `value_type` int(11) NOT NULL DEFAULT '0' COMMENT '类型', `buyer_id` bigint(20) DEFAULT NULL COMMENT 'buyerid', `seller_id` bigint(20) DEFAULT NULL COMMENT 'seller_id', PRIMARY KEY (`biz_order_id`,`value_type`), KEY `idx_biz_vertical_gmtmodified` (`gmt_modified`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='datax perf test' 单行记录类似于：\nkey_value: ;orderIds:20148888888,2014888888813800; gmt_create: 2011-09-24 11:07:20 gmt_modified: 2011-10-24 17:56:34 attribute_cc: 1 value_type: 3 buyer_id: 8888888 seller_id: 1 4.1.2 机器参数 执行DataX的机器参数为:\ncpu: 24核 Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz mem: 48GB net: 千兆双网卡 disc: DataX 数据不落磁盘，不统计此项 Mysql数据库机器参数为:\ncpu: 32核 Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz mem: 256GB net: 千兆双网卡 disc: BTWL419303E2800RGN INTEL SSDSC2BB800G4 D2010370 4.1.3 DataX jvm 参数 -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError 4.2 测试报告 4.2.1 单表测试报告 通道数 批量提交行数 DataX速度(Rec/s) DataX流量(MB/s) DataX机器网卡流出流量(MB/s) DataX机器运行负载 DB网卡进入流量(MB/s) DB运行负载 DB TPS 1 128 5319 0.260 0.580 0.05 0.620 0.5 50 1 512 14285 0.697 1.6 0.12 1.6 0.6 28 1 1024 17241 0.842 1.9 0.20 1.9 0.6 16 1 2048 31250 1.49 2.8 0.15 3.0 0.8 15 1 4096 31250 1.49 3.5 0.20 3.6 0.8 8 4 128 11764 0.574 1.5 0.21 1.6 0.8 112 4 512 30769 1.47 3.5 0.3 3.6 0.9 88 4 1024 50000 2.38 5.4 0.3 5.5 1.0 66 4 2048 66666 3.18 7.0 0.3 7.1 1.37 46 4 4096 80000 3.81 7.3 0.5 7.3 1.40 26 8 128 17777 0.868 2.9 0.28 2.9 0.8 200 8 512 57142 2.72 8.5 0.5 8.5 0.70 159 8 1024 88888 4.24 12.2 0.9 12.4 1.0 108 8 2048 133333 6.36 14.7 0.9 14.7 1.0 81 8 4096 166666 7.95 19.5 0.9 19.5 3.0 45 16 128 32000 1.53 3.3 0.6 3.4 0.88 401 16 512 106666 5.09 16.1 0.9 16.2 2.16 260 16 1024 173913 8.29 22.1 1.5 22.2 4.5 200 16 2048 228571 10.90 28.6 1.61 28.7 4.60 128 16 4096 246153 11.74 31.1 1.65 31.2 4.66 57 32 1024 246153 11.74 30.5 3.17 30.7 12.10 270 说明：\n这里的单表，主键类型为 bigint(20),自增。 batchSize 和 通道个数，对性能影响较大。 16通道，4096批量提交时，出现 full gc 2次。 4.2.2 分表测试报告(2个分库，每个分库4张分表，共计8张分表) 通道数 批量提交行数 DataX速度(Rec/s) DataX流量(MB/s) DataX机器网卡流出流量(MB/s) DataX机器运行负载 DB网卡进入流量(MB/s) DB运行负载 DB TPS 8 128 26764 1.28 2.9 0.5 3.0 0.8 209 8 512 95180 4.54 10.5 0.7 10.9 0.8 188 8 1024 94117 4.49 12.3 0.6 12.4 1.09 120 8 2048 133333 6.36 19.4 0.9 19.5 1.35 85 8 4096 191692 9.14 22.1 1.0 22.2 1.45 45 4.2.3 分表测试报告(2个分库，每个分库8张分表，共计16张分表) 通道数 批量提交行数 DataX速度(Rec/s) DataX流量(MB/s) DataX机器网卡流出流量(MB/s) DataX机器运行负载 DB网卡进入流量(MB/s) DB运行负载 DB TPS 16 128 50124 2.39 5.6 0.40 6.0 2.42 378 16 512 155084 7.40 18.6 1.30 18.9 2.82 325 16 1024 177777 8.48 24.1 1.43 25.5 3.5 233 16 2048 289382 13.8 33.1 2.5 33.5 4.5 150 16 4096 326451 15.52 33.7 1.5 33.9 4.3 80 4.2.4 性能测试小结 批量提交行数（batchSize）对性能影响很大，当 batchSize\u0026gt;=512 之后，单线程写入速度能达到每秒写入一万行 在 batchSize\u0026gt;=512 的基础上，随着通道数的增加（通道数\u0026lt;32），速度呈线性比增加。 通常不建议写入数据库时，通道个数 \u0026gt;32 5 约束限制 FAQ Q: MysqlWriter 执行 postSql 语句报错，那么数据导入到目标数据库了吗?\nA: DataX 导入过程存在三块逻辑，pre 操作、导入操作、post 操作，其中任意一环报错，DataX 作业报错。由于 DataX 不能保证在同一个事务完成上述几个操作，因此有可能数据已经落入到目标端。\nQ: 按照上述说法，那么有部分脏数据导入数据库，如果影响到线上数据库怎么办?\nA: 目前有两种解法，第一种配置 pre 语句，该 sql 可以清理当天导入数据， DataX 每次导入时候可以把上次清理干净并导入完整数据。第二种，向临时表导入数据，完成后再 rename 到线上表。\nQ: 上面第二种方法可以避免对线上数据造成影响，那我具体怎样操作?\nA: 可以配置临时表导入\n","permalink":"http://121.199.2.5:6080/ee3103b29d5b4fa696cb69e35fefb970/","summary":"DataX MysqlWriter 1 快速介绍 MysqlWriter 插件实现了写入数据到 Mysql 主库的目的表的功能。在底层实现上， MysqlWriter 通过 JDBC 连接远程 Mysql 数据库，并执行相应的 insert into \u0026hellip; 或者 ( replace into \u0026hellip;) 的 sql 语句将数据写入 Mysql，内部会分批次提交入库，需要数据库本身采用 innodb 引擎。\nMysqlWriter 面向ETL开发工程师，他们使用 MysqlWriter 从数仓导入数据到 Mysql。同时 MysqlWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 MysqlWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置的 writeMode 生成\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 或者 replace into...(没有遇到主键/唯一性索引冲突时，与 insert into 行为一致，冲突时会用新行替换原有行所有字段) 的语句写入数据到 Mysql。出于性能考虑，采用了 PreparedStatement + Batch，并且设置了：rewriteBatchedStatements=true，将数据缓冲到线程上下文 Buffer 中，当 Buffer 累计到预定阈值时，才发起写入请求。 注意：目的表所在数据库必须是主库才能写入数据；整个任务至少需要具备 insert/replace into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 Mysql 导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mysqlwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;session\u0026#34;: [ \u0026#34;set session sql_mode=\u0026#39;ANSI\u0026#39;\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:mysql://127.","title":"DataX MysqlWriter"},{"content":"DataX OCSWriter 适用memcached客户端写入ocs 1 快速介绍 1.1 OCS简介 开放缓存服务( Open Cache Service，简称OCS）是基于内存的缓存服务，支持海量小数据的高速访问。OCS可以极大缓解对后端存储的压力，提高网站或应用的响应速度。OCS支持Key-Value的数据结构，兼容Memcached协议的客户端都可与OCS通信。\nOCS 支持即开即用的方式快速部署；对于动态Web、APP应用，可通过缓存服务减轻对数据库的压力，从而提高网站整体的响应速度。\n与本地MemCache相同之处在于OCS兼容Memcached协议，与用户环境兼容，可直接用于OCS服务 不同之处在于硬件和数据部署在云端，有完善的基础设施、网络安全保障、系统维护服务。所有的这些服务，都不需要投资，只需根据使用量进行付费即可。\n1.2 OCSWriter简介 OCSWriter是DataX实现的，基于Memcached协议的数据写入OCS通道。\n2 功能说明 2.1 配置样例 这里使用一份从内存产生的数据导入到OCS。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ocswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;proxy\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;11211\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;set|add|replace|append|prepend\u0026#34;, \u0026#34;writeFormat\u0026#34;: \u0026#34;text|binary\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;\\u0001\u0026#34;, \u0026#34;expireTime\u0026#34;: 1000, \u0026#34;indexes\u0026#34;: \u0026#34;0,2\u0026#34;, \u0026#34;batchSize\u0026#34;: 1000 } } } ] } } 2.2 参数说明 proxy\n描述：OCS机器的ip或host。 必选：是 port\n描述：OCS的连接域名，默认为11211 必选：否 默认值：11211 username\n描述：OCS连接的访问账号。 必选：是 password\n描述：OCS连接的访问密码 必选：是 writeMode\n描述: OCSWriter写入方式，具体为： set: 存储这个数据，如果已经存在则覆盖 add: 存储这个数据，当且仅当这个key不存在的时候 replace: 存储这个数据，当且仅当这个key存在 append: 将数据存放在已存在的key对应的内容的后面，忽略exptime prepend: 将数据存放在已存在的key对应的内容的前面，忽略exptime 必选：是 writeFormat\n描述: OCSWriter写出数据格式，目前支持两类数据写入方式: text: 将源端数据序列化为文本格式，其中第一个字段作为OCS写入的KEY，后续所有字段序列化为STRING类型，使用用户指定的fieldDelimiter作为间隔符，将文本拼接为完整的字符串再写入OCS。 binary: 将源端数据作为二进制直接写入，这类场景为未来做扩展使用，目前不支持。如果填写binary将会报错！ 必选：否 默认值：text expireTime\n描述: OCS值缓存失效时间，目前MemCache支持两类过期时间，\nUnix时间(自1970.1.1开始到现在的秒数)，该时间指定了到未来某个时刻数据失效。 相对当前时间的秒数，该时间指定了从现在开始多长时间后数据失效。 注意：如果过期时间的秒数大于606024*30(即30天)，则服务端认为是Unix时间。 单位：秒 必选：否\n默认值：0【0表示永久有效】\nindexes\n描述: 用数据的第几列当做ocs的key 必选：否 默认值：0 batchSize\n描述：一次性批量提交的记录数大小，该值可以极大减少DataX与OCS的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况[memcached版本暂不支持批量写]。 必选：否 默认值：256 fieldDelimiter\n描述：写入ocs的key和value分隔符。比如：key=tom\\u0001boston, value=28\\u0001lawer\\u0001male\\u0001married 必选：否 默认值：\\u0001 3 性能报告 3.1 datax机器配置 CPU:16核、内存:24GB、网卡:单网卡1000mbps 3.2 任务资源配置 -Xms8g -Xmx8g -XX:+HeapDumpOnOutOfMemoryError 3.3 测试报告 单条数据大小 通道并发数 TPS 通道流量 出口流量 备注 1KB 1 579 tps 583.31KB/s 648.63KB/s 无 1KB 10 6006 tps 5.87MB/s 6.73MB/s 无 1KB 100 49916 tps 48.56MB/s 55.55MB/s 无 10KB 1 438 tps 4.62MB/s 5.07MB/s 无 10KB 10 4313 tps 45.57MB/s 49.51MB/s 无 10KB 100 10713 tps 112.80MB/s 123.01MB/s 无 100KB 1 275 tps 26.09MB/s 144.90KB/s 无。数据冗余大，压缩比高。 100KB 10 2492 tps 236.33MB/s 1.30MB/s 无 100KB 100 3187 tps 302.17MB/s 1.77MB/s 无 3.4 性能测试小结 单条数据小于10KB时建议开启100并发。 不建议10KB以上的数据写入ocs。 ","permalink":"http://121.199.2.5:6080/d4b1e453f8274dda834a305a5e6a38bf/","summary":"DataX OCSWriter 适用memcached客户端写入ocs 1 快速介绍 1.1 OCS简介 开放缓存服务( Open Cache Service，简称OCS）是基于内存的缓存服务，支持海量小数据的高速访问。OCS可以极大缓解对后端存储的压力，提高网站或应用的响应速度。OCS支持Key-Value的数据结构，兼容Memcached协议的客户端都可与OCS通信。\nOCS 支持即开即用的方式快速部署；对于动态Web、APP应用，可通过缓存服务减轻对数据库的压力，从而提高网站整体的响应速度。\n与本地MemCache相同之处在于OCS兼容Memcached协议，与用户环境兼容，可直接用于OCS服务 不同之处在于硬件和数据部署在云端，有完善的基础设施、网络安全保障、系统维护服务。所有的这些服务，都不需要投资，只需根据使用量进行付费即可。\n1.2 OCSWriter简介 OCSWriter是DataX实现的，基于Memcached协议的数据写入OCS通道。\n2 功能说明 2.1 配置样例 这里使用一份从内存产生的数据导入到OCS。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ocswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;proxy\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;11211\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;set|add|replace|append|prepend\u0026#34;, \u0026#34;writeFormat\u0026#34;: \u0026#34;text|binary\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;\\u0001\u0026#34;, \u0026#34;expireTime\u0026#34;: 1000, \u0026#34;indexes\u0026#34;: \u0026#34;0,2\u0026#34;, \u0026#34;batchSize\u0026#34;: 1000 } } } ] } } 2.","title":"DataX OCSWriter 适用memcached客户端写入ocs"},{"content":"DataX ODPSReader 1 快速介绍 ODPSReader 实现了从 ODPS读取数据的功能，有关ODPS请参看(https://help.aliyun.com/document_detail/27800.html?spm=5176.doc27803.6.101.NxCIgY)。 在底层实现上，ODPSReader 根据你配置的 源头项目 / 表 / 分区 / 表字段 等信息，通过 Tunnel 从 ODPS 系统中读取数据。\n注意 1、如果你需要使用ODPSReader/Writer插件，由于 AccessId/AccessKey 解密的需要，请务必使用 JDK 1.6.32 及以上版本。JDK 安装事项，请联系 PE 处理 2、ODPSReader 不是通过 ODPS SQL （select ... from ... where ... ）来抽取数据的 3、注意区分你要读取的表是线上环境还是线下环境 4、目前 DataX3 依赖的 SDK 版本是： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.odps\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;odps-sdk-core-internal\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.13.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2 实现原理 ODPSReader 支持读取分区表、非分区表，不支持读取虚拟视图。当要读取分区表时，需要指定出具体的分区配置，比如读取 t0 表，其分区为 pt=1,ds=hangzhou 那么你需要在配置中配置该值。当要读取非分区表时，你不能提供分区配置。表字段可以依序指定全部列，也可以指定部分列，或者调整列顺序，或者指定常量字段，但是表字段中不能指定分区列（分区列不是表字段）。\n注意：要特别注意 odpsServer、project、table、accessId、accessKey 的配置，因为直接影响到是否能够加载到你需要读取数据的表。很多权限问题都出现在这里。 3 功能说明 3.1 配置样例 这里使用一份读出 ODPS 数据然后打印到屏幕的配置样板。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;odpsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;accessId\u0026#34;: \u0026#34;accessId\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;accessKey\u0026#34;, \u0026#34;project\u0026#34;: \u0026#34;targetProjectName\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;tableName\u0026#34;, \u0026#34;partition\u0026#34;: [ \u0026#34;pt=1,ds=hangzhou\u0026#34; ], \u0026#34;column\u0026#34;: [ \u0026#34;customer_id\u0026#34;, \u0026#34;nickname\u0026#34; ], \u0026#34;packageAuthorizedProject\u0026#34;: \u0026#34;yourCurrentProjectName\u0026#34;, \u0026#34;splitMode\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;odpsServer\u0026#34;: \u0026#34;http://xxx/api\u0026#34;, \u0026#34;tunnelServer\u0026#34;: \u0026#34;http://dt.odps.aliyun.com\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;\\t\u0026#34;, \u0026#34;print\u0026#34;: \u0026#34;true\u0026#34; } } } ] } } 3.2 参数说明 参数 accessId\n描述：ODPS系统登录ID 必选：是 默认值：无 accessKey\n描述：ODPS系统登录Key 必选：是 默认值：无 project\n描述：读取数据表所在的 ODPS 项目名称（大小写不敏感） 必选：是 默认值：无 table\n描述：读取数据表的表名称（大小写不敏感） 必选：是 默认值：无 partition\n描述：读取数据所在的分区信息，支持linux shell通配符，包括 * 表示0个或多个字符，?代表任意一个字符。例如现在有分区表 test，其存在 pt=1,ds=hangzhou pt=1,ds=shanghai pt=2,ds=hangzhou pt=2,ds=beijing 四个分区，如果你想读取 pt=1,ds=shanghai 这个分区的数据，那么你应该配置为: \u0026quot;partition\u0026quot;:[\u0026quot;pt=1,ds=shanghai\u0026quot;]； 如果你想读取 pt=1下的所有分区，那么你应该配置为: \u0026quot;partition\u0026quot;:[\u0026quot;pt=1,ds=* \u0026quot;]；如果你想读取整个 test 表的所有分区的数据，那么你应该配置为: \u0026quot;partition\u0026quot;:[\u0026quot;pt=*,ds=*\u0026quot;] 必选：如果表为分区表，则必填。如果表为非分区表，则不能填写 默认值：无 column\n描述：读取 odps 源头表的列信息。例如现在有表 test，其字段为：id,name,age 如果你想依次读取 id,name,age 那么你应该配置为: \u0026quot;column\u0026quot;:[\u0026quot;id\u0026quot;,\u0026quot;name\u0026quot;,\u0026quot;age\u0026quot;] 或者配置为:\u0026quot;column\u0026quot;=[\u0026quot;*\u0026quot;] 这里 * 表示依次读取表的每个字段，但是我们不推荐你配置抽取字段为 * ，因为当你的表字段顺序调整、类型变更或者个数增减，你的任务就会存在源头表列和目的表列不能对齐的风险，会直接导致你的任务运行结果不正确甚至运行失败。如果你想依次读取 name,id 那么你应该配置为: \u0026quot;coulumn\u0026quot;:[\u0026quot;name\u0026quot;,\u0026quot;id\u0026quot;] 如果你想在源头抽取的字段中添加常量字段(以适配目标表的字段顺序)，比如你想抽取的每一行数据值为 age 列对应的值,name列对应的值,常量日期值1988-08-08 08:08:08,id 列对应的值 那么你应该配置为:\u0026quot;column\u0026quot;:[\u0026quot;age\u0026quot;,\u0026quot;name\u0026quot;,\u0026quot;'1988-08-08 08:08:08'\u0026quot;,\u0026quot;id\u0026quot;] 即常量列首尾用符号' 包住即可，我们内部实现上识别常量是通过检查你配置的每一个字段，如果发现有字段首尾都有'，则认为其是常量字段，其实际值为去除' 之后的值。\n注意：ODPSReader 抽取数据表不是通过 ODPS 的 Select SQL 语句，所以不能在字段上指定函数，也不能指定分区字段名称（分区字段不属于表字段） 必选：是 默认值：无 odpsServer\n描述：源头表 所在 ODPS 系统的server 地址 必选：是 默认值：无 tunnelServer\n描述：源头表 所在 ODPS 系统的tunnel 地址 必选：是 默认值：无 splitMode\n描述：读取源头表时切分所需要的模式。默认值为 record，可不填，表示根据切分份数，按照记录数进行切分。如果你的任务目的端为 Mysql，并且是 Mysql 的多个表，那么根据现在 DataX 结构，你的源头表必须是分区表，并且每个分区依次对应目的端 Mysql 的多个分表，则此时应该配置为\u0026quot;splitMode\u0026quot;:\u0026quot;partition\u0026quot; 必选：否 默认值：record accountProvider [待定]\n描述：读取时使用的 ODPS 账号类型。目前支持 aliyun/taobao 两种类型。默认为 aliyun，可不填 必选：否 默认值：aliyun packageAuthorizedProject\n* 描述：被package授权的project，即用户当前所在project \u0026lt;br /\u0026gt; 必选：否 默认值：无 isCompress\n* 描述：是否压缩读取，bool类型: \u0026quot;true\u0026quot;表示压缩, \u0026quot;false\u0026quot;标示不压缩 \u0026lt;br /\u0026gt; 必选：否 默认值：\u0026ldquo;false\u0026rdquo; : 不压缩 3.3 类型转换 下面列出 ODPSReader 读出类型与 DataX 内部类型的转换关系：\nODPS 数据类型 DataX 内部类型 BIGINT Long DOUBLE Double STRING String DATETIME Date Boolean Bool 4 性能报告（线上环境实测） 4.1 环境准备 4.1.1 数据特征 建表语句：\nuse cdo_datasync; create table datax3_odpswriter_perf_10column_1kb_00( s_0 string, bool_1 boolean, bi_2 bigint, dt_3 datetime, db_4 double, s_5 string, s_6 string, s_7 string, s_8 string, s_9 string )PARTITIONED by (pt string,year string); 单行记录类似于：\ns_0 : 485924f6ab7f272af361cd3f7f2d23e0d764942351#$%^\u0026amp;fdafdasfdas%%^(*\u0026amp;^^\u0026amp;* bool_1 : true bi_2 : 1696248667889 dt_3 : 2013-07-0600: 00: 00 db_4 : 3.141592653578 s_5 : 100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209 s_6 : 100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11fdsafdsfdsa209 s_7 : 100DAFDSAFDSAHOFJDPSAWIFDISHAF;dsadsafdsahfdsajf;dsfdsa;FJDSAL;11209 s_8 : 100dafdsafdsahofjdpsawifdishaf;DSADSAFDSAHFDSAJF;dsfdsa;fjdsal;11209 s_9 : 12~!2345100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209 4.1.2 机器参数 执行DataX的机器参数为:\ncpu : 24 Core Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz cache 15.36MB mem : 50GB net : 千兆双网卡 jvm : -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError disc: DataX 数据不落磁盘，不统计此项 任务配置为:\n{ \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;odpsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;accessId\u0026#34;: \u0026#34;******************************\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;*****************************\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;partition\u0026#34;: [ \u0026#34;pt=20141010000000,year=2014\u0026#34; ], \u0026#34;odpsServer\u0026#34;: \u0026#34;http://xxx/api\u0026#34;, \u0026#34;project\u0026#34;: \u0026#34;cdo_datasync\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;datax3_odpswriter_perf_10column_1kb_00\u0026#34;, \u0026#34;tunnelServer\u0026#34;: \u0026#34;http://xxx\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;485924f6ab7f272af361cd3f7f2d23e0d764942351#$%^\u0026amp;fdafdasfdas%%^(*\u0026amp;^^\u0026amp;*\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1696248667889\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2013-07-06 00:00:00\u0026#34;, \u0026#34;dateFormat\u0026#34;: \u0026#34;yyyy-mm-dd hh:mm:ss\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;3.141592653578\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11fdsafdsfdsa209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100DAFDSAFDSAHOFJDPSAWIFDISHAF;dsadsafdsahfdsajf;dsfdsa;FJDSAL;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;DSADSAFDSAHFDSAJF;dsfdsa;fjdsal;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;12~!2345100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209\u0026#34; } ] } } } ] } } 4.2 测试报告 并发任务数 DataX速度(Rec/s) DataX流量(MB/S) 网卡流量(MB/S) DataX运行负载 1 117507 50.20 53.7 0.62 2 232976 99.54 108.1 0.99 4 387382 165.51 181.3 1.98 5 426054 182.03 202.2 2.35 6 434793 185.76 204.7 2.77 8 495904 211.87 230.2 2.86 16 501596 214.31 234.7 2.84 32 501577 214.30 234.7 2.99 64 501625 214.32 234.7 3.22 说明：\nOdpsReader 影响速度最主要的是channel数目，这里到达8时已经打满网卡，过多调大反而会影响系统性能。 channel数目的选择，可以考虑odps表文件组织，可尝试合并小文件再进行同步调优。 5 约束限制 FAQ（待补充） Q: 你来问\nA: 我来答。\n","permalink":"http://121.199.2.5:6080/5b1b6009df684895ab2ef943e412ee8a/","summary":"DataX ODPSReader 1 快速介绍 ODPSReader 实现了从 ODPS读取数据的功能，有关ODPS请参看(https://help.aliyun.com/document_detail/27800.html?spm=5176.doc27803.6.101.NxCIgY)。 在底层实现上，ODPSReader 根据你配置的 源头项目 / 表 / 分区 / 表字段 等信息，通过 Tunnel 从 ODPS 系统中读取数据。\n注意 1、如果你需要使用ODPSReader/Writer插件，由于 AccessId/AccessKey 解密的需要，请务必使用 JDK 1.6.32 及以上版本。JDK 安装事项，请联系 PE 处理 2、ODPSReader 不是通过 ODPS SQL （select ... from ... where ... ）来抽取数据的 3、注意区分你要读取的表是线上环境还是线下环境 4、目前 DataX3 依赖的 SDK 版本是： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.odps\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;odps-sdk-core-internal\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.13.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2 实现原理 ODPSReader 支持读取分区表、非分区表，不支持读取虚拟视图。当要读取分区表时，需要指定出具体的分区配置，比如读取 t0 表，其分区为 pt=1,ds=hangzhou 那么你需要在配置中配置该值。当要读取非分区表时，你不能提供分区配置。表字段可以依序指定全部列，也可以指定部分列，或者调整列顺序，或者指定常量字段，但是表字段中不能指定分区列（分区列不是表字段）。\n注意：要特别注意 odpsServer、project、table、accessId、accessKey 的配置，因为直接影响到是否能够加载到你需要读取数据的表。很多权限问题都出现在这里。 3 功能说明 3.1 配置样例 这里使用一份读出 ODPS 数据然后打印到屏幕的配置样板。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;odpsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;accessId\u0026#34;: \u0026#34;accessId\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;accessKey\u0026#34;, \u0026#34;project\u0026#34;: \u0026#34;targetProjectName\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;tableName\u0026#34;, \u0026#34;partition\u0026#34;: [ \u0026#34;pt=1,ds=hangzhou\u0026#34; ], \u0026#34;column\u0026#34;: [ \u0026#34;customer_id\u0026#34;, \u0026#34;nickname\u0026#34; ], \u0026#34;packageAuthorizedProject\u0026#34;: \u0026#34;yourCurrentProjectName\u0026#34;, \u0026#34;splitMode\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;odpsServer\u0026#34;: \u0026#34;http://xxx/api\u0026#34;, \u0026#34;tunnelServer\u0026#34;: \u0026#34;http://dt.","title":"DataX ODPSReader"},{"content":"DataX ODPS写入 1 快速介绍 ODPSWriter插件用于实现往ODPS插入或者更新数据，主要提供给etl开发同学将业务数据导入odps，适合于TB,GB数量级的数据传输，如果需要传输PB量级的数据，请选择dt task工具 ;\n2 实现原理 在底层实现上，ODPSWriter是通过DT Tunnel写入ODPS系统的，有关ODPS的更多技术细节请参看 ODPS主站 https://data.aliyun.com/product/odps 和ODPS产品文档 https://help.aliyun.com/product/27797.html\n目前 DataX3 依赖的 SDK 版本是：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.odps\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;odps-sdk-core-internal\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.13.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 注意: 如果你需要使用ODPSReader/Writer插件，请务必使用JDK 1.6-32及以上版本 使用java -version查看Java版本号\n3 功能说明 3.1 配置样例 这里使用一份从内存产生到ODPS导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;byte\u0026#34;: 1048576 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 100000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;odpswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;project\u0026#34;: \u0026#34;chinan_test\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;odps_write_test00_partitioned\u0026#34;, \u0026#34;partition\u0026#34;: \u0026#34;school=SiChuan-School,class=1\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;accessId\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;truncate\u0026#34;: true, \u0026#34;odpsServer\u0026#34;: \u0026#34;http://sxxx/api\u0026#34;, \u0026#34;tunnelServer\u0026#34;: \u0026#34;http://xxx\u0026#34;, \u0026#34;accountType\u0026#34;: \u0026#34;aliyun\u0026#34; } } } ] } } 3.2 参数说明 accessId\n描述：ODPS系统登录ID 必选：是 默认值：无 accessKey\n描述：ODPS系统登录Key 必选：是 默认值：无 project\n描述：ODPS表所属的project，注意:Project只能是字母+数字组合，请填写英文名称。在云端等用户看到的ODPS项目中文名只是显示名，请务必填写底层真实地Project英文标识名。 必选：是 默认值：无 table\n描述：写入数据的表名，不能填写多张表，因为DataX不支持同时导入多张表。 必选：是 默认值：无 partition\n描述：需要写入数据表的分区信息，必须指定到最后一级分区。把数据写入一个三级分区表，必须配置到最后一级分区，例如pt=20150101/type＝1/biz=2。 必选：如果是分区表，该选项必填，如果非分区表，该选项不可填写。 默认值：空 column\n描述：需要导入的字段列表，当导入全部字段时，可以配置为\u0026quot;column\u0026quot;: [\u0026quot;*\u0026quot;], 当需要插入部分odps列填写部分列，例如\u0026quot;column\u0026quot;: [\u0026ldquo;id\u0026rdquo;, \u0026ldquo;name\u0026rdquo;]。ODPSWriter支持列筛选、列换序，例如表有a,b,c三个字段，用户只同步c,b两个字段。可以配置成[\u0026ldquo;c\u0026rdquo;,\u0026ldquo;b\u0026rdquo;], 在导入过程中，字段a自动补空，设置为null。 必选：否 默认值：无 truncate\n描述：ODPSWriter通过配置\u0026quot;truncate\u0026quot;: true，保证写入的幂等性，即当出现写入失败再次运行时，ODPSWriter将清理前述数据，并导入新数据，这样可以保证每次重跑之后的数据都保持一致。 **truncate选项不是原子操作！ODPS SQL无法做到原子性。因此当多个任务同时向一个Table/Partition清理分区时候，可能出现并发时序问题，请务必注意！**针对这类问题，我们建议尽量不要多个作业DDL同时操作同一份分区，或者在多个并发作业启动前，提前创建分区。\n必选：是 默认值：无 odpsServer\n描述：ODPS的server地址，线上地址为 http://service.odps.aliyun.com/api 必选：是 默认值：无 tunnelServer\n描述：ODPS的tunnelserver地址，线上地址为 http://dt.odps.aliyun.com 必选：是， 默认值：无 3.3 类型转换 类似ODPSReader，目前ODPSWriter支持大部分ODPS类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出ODPSWriter针对ODPS类型转换列表:\nDataX 内部类型 ODPS 数据类型 Long bigint Double double String string Date datetime Boolean bool 4 插件特点 4.1 关于列筛选的问题 ODPS本身不支持列筛选、重排序、补空等等，但是DataX ODPSWriter完成了上述需求，支持列筛选、重排序、补空。例如需要导入的字段列表，当导入全部字段时，可以配置为\u0026quot;column\u0026quot;: [\u0026quot;*\u0026quot;]，odps表有a,b,c三个字段，用户只同步c,b两个字段，在列配置中可以写成\u0026quot;column\u0026quot;: [\u0026ldquo;c\u0026rdquo;,\u0026ldquo;b\u0026rdquo;]，表示会把reader的第一列和第二列导入odps的c字段和b字段，而odps表中新插入记录的a字段会被置为null. 4.2 列配置错误的处理 为了保证写入数据的可靠性，避免多余列数据丢失造成数据质量故障。对于写入多余的列，ODPSWriter将报错。例如ODPS表字段为a,b,c，但是ODPSWriter写入的字段为多于3列的话ODPSWriter将报错。 4.3 分区配置注意事项 ODPSWriter只提供 写入到最后一级分区 功能，不支持写入按照某个字段进行分区路由等功能。假设表一共有3级分区，那么在分区配置中就必须指明写入到某个三级分区，例如把数据写入一个表的第三级分区，可以配置为 pt=20150101/type＝1/biz=2，但是不能配置为pt=20150101/type＝1或者pt=20150101。 4.4 任务重跑和failover ODPSWriter通过配置\u0026quot;truncate\u0026quot;: true，保证写入的幂等性，即当出现写入失败再次运行时，ODPSWriter将清理前述数据，并导入新数据，这样可以保证每次重跑之后的数据都保持一致。如果在运行过程中因为其他的异常导致了任务中断，是不能保证数据的原子性的，数据不会回滚也不会自动重跑，需要用户利用幂等性这一特点重跑去确保保证数据的完整性。truncate为true的情况下，会将指定分区\\表的数据全部清理，请谨慎使用！ 5 性能报告（线上环境实测） 5.1 环境准备 5.1.1 数据特征 建表语句：\nuse cdo_datasync; create table datax3_odpswriter_perf_10column_1kb_00( s_0 string, bool_1 boolean, bi_2 bigint, dt_3 datetime, db_4 double, s_5 string, s_6 string, s_7 string, s_8 string, s_9 string )PARTITIONED by (pt string,year string); 单行记录类似于：\ns_0 : 485924f6ab7f272af361cd3f7f2d23e0d764942351#$%^\u0026amp;fdafdasfdas%%^(*\u0026amp;^^\u0026amp;* bool_1 : true bi_2 : 1696248667889 dt_3 : 2013-07-0600: 00: 00 db_4 : 3.141592653578 s_5 : 100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209 s_6 : 100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11fdsafdsfdsa209 s_7 : 100DAFDSAFDSAHOFJDPSAWIFDISHAF;dsadsafdsahfdsajf;dsfdsa;FJDSAL;11209 s_8 : 100dafdsafdsahofjdpsawifdishaf;DSADSAFDSAHFDSAJF;dsfdsa;fjdsal;11209 s_9 : 12~!2345100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209 5.1.2 机器参数 执行DataX的机器参数为:\ncpu : 24 Core Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz cache 15.36MB mem : 50GB net : 千兆双网卡 jvm : -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError disc: DataX 数据不落磁盘，不统计此项 任务配置为:\n{ \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: \u0026#34;1,2,4,5,6,8,16,32,64\u0026#34; } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;odpsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;accessId\u0026#34;: \u0026#34;******************************\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;*****************************\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;partition\u0026#34;: [ \u0026#34;pt=20141010000000,year=2014\u0026#34; ], \u0026#34;odpsServer\u0026#34;: \u0026#34;http://service.odps.aliyun.com/api\u0026#34;, \u0026#34;project\u0026#34;: \u0026#34;cdo_datasync\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;datax3_odpswriter_perf_10column_1kb_00\u0026#34;, \u0026#34;tunnelServer\u0026#34;: \u0026#34;http://dt.odps.aliyun.com\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;485924f6ab7f272af361cd3f7f2d23e0d764942351#$%^\u0026amp;fdafdasfdas%%^(*\u0026amp;^^\u0026amp;*\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1696248667889\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2013-07-06 00:00:00\u0026#34;, \u0026#34;dateFormat\u0026#34;: \u0026#34;yyyy-mm-dd hh:mm:ss\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;3.141592653578\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11fdsafdsfdsa209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100DAFDSAFDSAHOFJDPSAWIFDISHAF;dsadsafdsahfdsajf;dsfdsa;FJDSAL;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;DSADSAFDSAHFDSAJF;dsfdsa;fjdsal;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;12~!2345100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209\u0026#34; } ] } } } ] } } 5.2 测试报告 并发任务数 blockSizeInMB DataX速度(Rec/s) DataX流量(MB/S) 网卡流量(MB/S) DataX运行负载 1 32 30303 13.03 14.5 0.12 1 64 38461 16.54 16.5 0.44 1 128 46454 20.55 26.7 0.47 1 256 52631 22.64 26.7 0.47 1 512 58823 25.30 28.7 0.44 4 32 114816 49.38 55.3 0.75 4 64 147577 63.47 71.3 0.82 4 128 177744 76.45 83.2 0.97 4 256 173913 74.80 80.1 1.01 4 512 200000 86.02 95.1 1.41 8 32 204480 87.95 92.7 1.16 8 64 294224 126.55 135.3 1.65 8 128 365475 157.19 163.7 2.89 8 256 394713 169.83 176.7 2.72 8 512 241691 103.95 125.7 2.29 16 32 420838 181.01 198.0 2.56 16 64 458144 197.05 217.4 2.85 16 128 443219 190.63 210.5 3.29 16 256 315235 135.58 140.0 0.95 16 512 OOM 说明：\nOdpsWriter 影响速度的是channel 和 blockSizeInMB。blockSizeInMB 取32 和 64时，速度比较稳定,过分大的 blockSizeInMB 可能造成速度波动以及内存OOM。 channel 和 blockSizeInMB 对速度的影响都很明显，建议综合考虑配合选择。 channel 数目的选择，可以综合考虑源端数据特征进行选择，对于StreamReader，在16个channel时将网卡打满。 6 FAQ 1 导数据到 odps 的日志中有以下报错，该怎么处理呢？\u0026ldquo;ODPS-0420095: Access Denied - Authorization Failed [4002], You doesn‘t exist in project example_dev“ 解决办法 :找ODPS Prject 的 owner给用户的云账号授权，授权语句： grant Describe,Select,Alter,Update on table [tableName] to user XXX\n2 可以导入数据到odps的视图吗？ 目前不支持通过视图到数据到odps,视图是ODPS非实体化数据存储对象，技术上无法向视图导入数据。\n","permalink":"http://121.199.2.5:6080/ab33f89c494e47eaae02f9c21465b322/","summary":"DataX ODPS写入 1 快速介绍 ODPSWriter插件用于实现往ODPS插入或者更新数据，主要提供给etl开发同学将业务数据导入odps，适合于TB,GB数量级的数据传输，如果需要传输PB量级的数据，请选择dt task工具 ;\n2 实现原理 在底层实现上，ODPSWriter是通过DT Tunnel写入ODPS系统的，有关ODPS的更多技术细节请参看 ODPS主站 https://data.aliyun.com/product/odps 和ODPS产品文档 https://help.aliyun.com/product/27797.html\n目前 DataX3 依赖的 SDK 版本是：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.odps\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;odps-sdk-core-internal\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.13.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 注意: 如果你需要使用ODPSReader/Writer插件，请务必使用JDK 1.6-32及以上版本 使用java -version查看Java版本号\n3 功能说明 3.1 配置样例 这里使用一份从内存产生到ODPS导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;byte\u0026#34;: 1048576 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 100000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;odpswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;project\u0026#34;: \u0026#34;chinan_test\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;odps_write_test00_partitioned\u0026#34;, \u0026#34;partition\u0026#34;: \u0026#34;school=SiChuan-School,class=1\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;accessId\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;truncate\u0026#34;: true, \u0026#34;odpsServer\u0026#34;: \u0026#34;http://sxxx/api\u0026#34;, \u0026#34;tunnelServer\u0026#34;: \u0026#34;http://xxx\u0026#34;, \u0026#34;accountType\u0026#34;: \u0026#34;aliyun\u0026#34; } } } ] } } 3.","title":"DataX ODPS写入"},{"content":"DataX OracleWriter 1 快速介绍 OracleWriter 插件实现了写入数据到 Oracle 主库的目的表的功能。在底层实现上， OracleWriter 通过 JDBC 连接远程 Oracle 数据库，并执行相应的 insert into \u0026hellip; sql 语句将数据写入 Oracle，内部会分批次提交入库。\nOracleWriter 面向ETL开发工程师，他们使用 OracleWriter 从数仓导入数据到 Oracle。同时 OracleWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 OracleWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL语句\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2.OracleWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 Oracle 导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;oraclewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\u0026#34;, \u0026#34;table\u0026#34;: [ \u0026#34;test\u0026#34; ] } ] } } } ] } } 3.2 参数说明 jdbcUrl\n描述：目的数据库的 JDBC 连接信息 ,jdbcUrl必须包含在connection配置单元中。\n注意：1、在一个数据库上只能配置一个值。这与 OracleReader 支持多个备库探测不同，因为此处不支持同一个数据库存在多个主库的情况(双主导入数据情况) 2、jdbcUrl按照Oracle官方规范，并可以填写连接附加参数信息。具体请参看 Oracle官方文档或者咨询对应 DBA。 必选：是 默认值：无 username\n描述：目的数据库的用户名 必选：是 默认值：无 password\n描述：目的数据库的密码 必选：是 默认值：无 table\n描述：目的表的表名称。支持写入一个或者多个表。当配置为多张表时，必须确保所有表结构保持一致。\n注意：table 和 jdbcUrl 必须包含在 connection 配置单元中 必选：是 默认值：无 column\n描述：目的表需要写入数据的字段,字段之间用英文逗号分隔。例如: \u0026ldquo;column\u0026rdquo;: [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;,\u0026ldquo;age\u0026rdquo;]。如果要依次写入全部列，使用表示, 例如: \u0026ldquo;column\u0026rdquo;: [\u0026quot;\u0026quot;]\n**column配置项必须指定，不能留空！** 注意：1、我们强烈不推荐你这样配置，因为当你目的表字段个数、类型等有改动时，你的任务可能运行不正确或者失败 2、此处 column 不能配置任何常量值 必选：是 默认值：否 preSql\n描述：写入数据到目的表前，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，请使用 @table 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。比如你的任务是要写入到目的端的100个同构分表(表名称为:datax_00,datax01, \u0026hellip; datax_98,datax_99)，并且你希望导入数据前，先对表中数据进行删除操作，那么你可以这样配置：\u0026quot;preSql\u0026quot;:[\u0026quot;delete from @table\u0026quot;]，效果是：在执行到每个表写入数据前，会先执行对应的 delete from 对应表名称 必选：否 默认值：无 postSql\n描述：写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ） 必选：否 默认值：无 batchSize\n描述：一次性批量提交的记录数大小，该值可以极大减少DataX与Oracle的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。\n必选：否 默认值：1024 session\n描述：设置oracle连接时的session信息，格式示例如下： \u0026#34;session\u0026#34;:[ \u0026#34;alter session set nls_date_format = \u0026#39;dd.mm.yyyy hh24:mi:ss\u0026#39;;\u0026#34; \u0026#34;alter session set NLS_LANG = \u0026#39;AMERICAN\u0026#39;;\u0026#34; ] 必选：否 默认值：无 3.3 类型转换 类似 OracleReader ，目前 OracleWriter 支持大部分 Oracle 类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出 OracleWriter 针对 Oracle 类型转换列表:\nDataX 内部类型 Oracle 数据类型 Long NUMBER,INTEGER,INT,SMALLINT Double NUMERIC,DECIMAL,FLOAT,DOUBLE PRECISION,REAL String LONG,CHAR,NCHAR,VARCHAR,VARCHAR2,NVARCHAR2,CLOB,NCLOB,CHARACTER,CHARACTER VARYING,CHAR VARYING,NATIONAL CHARACTER,NATIONAL CHAR,NATIONAL CHARACTER VARYING,NATIONAL CHAR VARYING,NCHAR VARYING Date TIMESTAMP,DATE Boolean bit, bool Bytes BLOB,BFILE,RAW,LONG RAW 4 性能报告 4.1 环境准备 4.1.1 数据特征 建表语句：\n--DROP TABLE PERF_ORACLE_WRITER; CREATE TABLE PERF_ORACLE_WRITER ( COL1 VARCHAR2(255 BYTE) NULL , COL2 NUMBER(32) NULL , COL3 NUMBER(32) NULL , COL4 DATE NULL , COL5 FLOAT NULL , COL6 VARCHAR2(255 BYTE) NULL , COL7 VARCHAR2(255 BYTE) NULL , COL8 VARCHAR2(255 BYTE) NULL , COL9 VARCHAR2(255 BYTE) NULL , COL10 VARCHAR2(255 BYTE) NULL ) LOGGING NOCOMPRESS NOCACHE; 单行记录类似于：\ncol1:485924f6ab7f272af361cd3f7f2d23e0d764942351#$%^\u0026amp;fdafdasfdas%%^(*\u0026amp;^^\u0026amp;* co12:1 co13:1696248667889 co14:2013-01-06 00:00:00 co15:3.141592653578 co16:100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209 co17:100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11fdsafdsfdsa209 co18:100DAFDSAFDSAHOFJDPSAWIFDISHAF;dsadsafdsahfdsajf;dsfdsa;FJDSAL;11209 co19:100dafdsafdsahofjdpsawifdishaf;DSADSAFDSAHFDSAJF;dsfdsa;fjdsal;11209 co110:12~!2345100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209 4.1.2 机器参数 执行 DataX 的机器参数为:\ncpu: 24 Core Intel(R) Xeon(R) CPU E5-2430 0 @ 2.20GHz mem: 94GB net: 千兆双网卡 disc: DataX 数据不落磁盘，不统计此项 Oracle 数据库机器参数为:\ncpu: 4 Core Intel(R) Xeon(R) CPU E5420 @ 2.50GHz mem: 7GB 4.1.3 DataX jvm 参数 -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError 4.1.4 性能测试作业配置 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 4 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sliceRecordCount\u0026#34;: 1000000000, \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;485924f6ab7f272af361cd3f7f2d23e0d764942351#$%^\u0026amp;fdafdasfdas%%^(*\u0026amp;^^\u0026amp;*\u0026#34; }, { \u0026#34;value\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1696248667889\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2013-07-06 00:00:00\u0026#34;, \u0026#34;dateFormat\u0026#34;: \u0026#34;yyyy-mm-dd hh:mm:ss\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;3.141592653578\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11fdsafdsfdsa209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100DAFDSAFDSAHOFJDPSAWIFDISHAF;dsadsafdsahfdsajf;dsfdsa;FJDSAL;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;100dafdsafdsahofjdpsawifdishaf;DSADSAFDSAHFDSAJF;dsfdsa;fjdsal;11209\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;12~!2345100dafdsafdsahofjdpsawifdishaf;dsadsafdsahfdsajf;dsfdsa;fjdsal;11209\u0026#34; } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;oraclewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;batchSize\u0026#34;: \u0026#34;512\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;, \u0026#34;col3\u0026#34;, \u0026#34;col4\u0026#34;, \u0026#34;col5\u0026#34;, \u0026#34;col6\u0026#34;, \u0026#34;col7\u0026#34;, \u0026#34;col8\u0026#34;, \u0026#34;col9\u0026#34;, \u0026#34;col10\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;PERF_ORACLE_WRITER\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:oracle:thin:@ip:port:dataplat\u0026#34; } ] } } } ] } } 4.2 测试报告 4.2.1 测试报告 通道数 批量提交行数 DataX速度(Rec/s) DataX流量(MB/s) DataX机器网卡流出流量(MB/s) DataX机器运行负载 DB网卡进入流量(MB/s) DB运行负载 1 128 15564 6.51 7.5 0.02 7.4 1.08 1 512 29491 10.90 12.6 0.05 12.4 1.55 1 1024 31529 11.87 13.5 0.22 13.3 1.58 1 2048 33469 12.57 14.3 0.17 14.3 1.53 1 4096 31363 12.48 13.4 0.10 10.0 1.72 4 10 9440 4.05 5.6 0.01 5.0 3.75 4 128 42832 16.48 18.3 0.07 18.5 2.89 4 512 46643 20.02 22.7 0.35 21.1 3.31 4 1024 39116 16.79 18.7 0.10 18.1 3.05 4 2048 39526 16.96 18.5 0.32 17.1 2.86 4 4096 37683 16.17 17.2 0.23 15.5 2.26 8 128 38336 16.45 17.5 0.13 16.2 3.87 8 512 31078 13.34 14.9 0.11 13.4 2.09 8 1024 37888 16.26 18.5 0.20 18.5 3.14 8 2048 38502 16.52 18.5 0.18 18.5 2.96 8 4096 38092 16.35 18.3 0.10 17.8 3.19 16 128 35366 15.18 16.9 0.13 15.6 3.49 16 512 35584 15.27 16.8 0.23 17.4 3.05 16 1024 38297 16.44 17.5 0.20 17.0 3.42 16 2048 28467 12.22 13.8 0.10 12.4 3.38 16 4096 27852 11.95 12.3 0.11 12.3 3.86 32 1024 34406 14.77 15.4 0.09 15.4 3.55 batchSize 和 通道个数，对性能影响较大 通常不建议写入数据库时，通道个数 \u0026gt;32 5 约束限制 FAQ Q: OracleWriter 执行 postSql 语句报错，那么数据导入到目标数据库了吗?\nA: DataX 导入过程存在三块逻辑，pre 操作、导入操作、post 操作，其中任意一环报错，DataX 作业报错。由于 DataX 不能保证在同一个事务完成上述几个操作，因此有可能数据已经落入到目标端。\nQ: 按照上述说法，那么有部分脏数据导入数据库，如果影响到线上数据库怎么办?\nA: 目前有两种解法，第一种配置 pre 语句，该 sql 可以清理当天导入数据， DataX 每次导入时候可以把上次清理干净并导入完整数据。第二种，向临时表导入数据，完成后再 rename 到线上表。\nQ: 上面第二种方法可以避免对线上数据造成影响，那我具体怎样操作?\nA: 可以配置临时表导入\n","permalink":"http://121.199.2.5:6080/1eb2fdd8968d4f6e8bbc0312c6647246/","summary":"DataX OracleWriter 1 快速介绍 OracleWriter 插件实现了写入数据到 Oracle 主库的目的表的功能。在底层实现上， OracleWriter 通过 JDBC 连接远程 Oracle 数据库，并执行相应的 insert into \u0026hellip; sql 语句将数据写入 Oracle，内部会分批次提交入库。\nOracleWriter 面向ETL开发工程师，他们使用 OracleWriter 从数仓导入数据到 Oracle。同时 OracleWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 OracleWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL语句\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2.OracleWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 Oracle 导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;oraclewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\u0026#34;, \u0026#34;table\u0026#34;: [ \u0026#34;test\u0026#34; ] } ] } } } ] } } 3.","title":"DataX OracleWriter"},{"content":"DataX OSSReader 说明 1 快速介绍 OSSReader提供了读取OSS数据存储的能力。在底层实现上，OSSReader使用OSS官方Java SDK获取OSS数据，并转换为DataX传输协议传递给Writer。\nOSS 产品介绍, 参看[阿里云OSS Portal] OSS Java SDK, 参看[阿里云OSS Java SDK] 2 功能与限制 OSSReader实现了从OSS读取数据并转为DataX协议的功能，OSS本身是无结构化数据存储，对于DataX而言，OSSReader实现上类比TxtFileReader，有诸多相似之处。目前OSSReader支持功能如下：\n支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量\n支持递归读取、支持文件名过滤。\n支持文本压缩，现有压缩格式为zip、gzip、bzip2。注意，一个压缩包不允许多文件打包压缩。\n多个object可以支持并发读取。\n我们暂时不能做到：\n单个Object(File)支持多线程并发读取，这里涉及到单个Object内部切分算法。二期考虑支持。\n单个Object在压缩情况下，从技术上无法支持多线程并发读取。\n3 功能说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: {}, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ossreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://oss.aliyuncs.com\u0026#34;, \u0026#34;accessId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;myBucket\u0026#34;, \u0026#34;object\u0026#34;: [ \u0026#34;bazhen/*\u0026#34; ], \u0026#34;column\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;index\u0026#34;: 0 }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;alibaba\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;index\u0026#34;: 1, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } ], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;\\t\u0026#34;, \u0026#34;compress\u0026#34;: \u0026#34;gzip\u0026#34; } }, \u0026#34;writer\u0026#34;: {} } ] } } 3.2 参数说明 endpoint\n描述：OSS Server的EndPoint地址，例如http://oss.aliyuncs.com。\n必选：是 默认值：无 accessId\n描述：OSS的accessId 必选：是 默认值：无 accessKey\n描述：OSS的accessKey 必选：是 默认值：无 bucket\n描述：OSS的bucket 必选：是 默认值：无 object\n描述：OSS的object信息，注意这里可以支持填写多个Object。 当指定单个OSS Object，OSSReader暂时只能使用单线程进行数据抽取。二期考虑在非压缩文件情况下针对单个Object可以进行多线程并发读取。\n当指定多个OSS Object，OSSReader支持使用多线程进行数据抽取。线程并发数通过通道数指定。\n当指定通配符，OSSReader尝试遍历出多个Object信息。例如: 指定/*代表读取bucket下游所有的Object，指定/bazhen/*代表读取bazhen目录下游所有的Object。\n特别需要注意的是，DataX会将一个作业下同步的所有Object视作同一张数据表。用户必须自己保证所有的Object能够适配同一套schema信息。\n必选：是 默认值：无 column\n描述：读取字段列表，type指定源数据的类型，index指定当前列来自于文本第几列(以0开始)，value指定当前类型为常量，不从源头文件读取数据，而是根据value值自动生成对应的列。 默认情况下，用户可以全部按照String类型读取数据，配置如下：\n\u0026#34;column\u0026#34;: [\u0026#34;*\u0026#34;] 用户可以指定Column字段信息，配置如下：\n{ \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;index\u0026#34;: 0 //从OSS文本第一列获取int字段 }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;alibaba\u0026#34; //从OSSReader内部生成alibaba的字符串字段作为当前字段 } 对于用户指定Column信息，type必须填写，index/value必须选择其一。\n必选：是 默认值：全部按照string类型读取 fieldDelimiter\n描述：读取的字段分隔符 必选：是 默认值：, compress\n描述：文本压缩类型，默认不填写意味着没有压缩。支持压缩类型为zip、gzip、bzip2。 必选：否 默认值：不压缩 encoding\n描述：读取文件的编码配置，目前只支持utf-8/gbk。\n必选：否 默认值：utf-8 nullFormat\n描述：文本文件中无法使用标准字符串定义null(空指针)，DataX提供nullFormat定义哪些字符串可以表示为null。\n例如如果用户配置: nullFormat=\u0026quot;\\N\u0026quot;，那么如果源头数据是\u0026quot;\\N\u0026quot;，DataX视作null字段。\n必选：否 默认值：\\N skipHeader\n描述：类CSV格式文件可能存在表头为标题情况，需要跳过。默认不跳过。\n必选：否 默认值：false csvReaderConfig\n描述：读取CSV类型文件参数配置，Map类型。读取CSV类型文件使用的CsvReader进行读取，会有很多配置，不配置则使用默认值。\n必选：否 默认值：无 常见配置：\n\u0026#34;csvReaderConfig\u0026#34;:{ \u0026#34;safetySwitch\u0026#34;: false, \u0026#34;skipEmptyRecords\u0026#34;: false, \u0026#34;useTextQualifier\u0026#34;: false } 所有配置项及默认值,配置时 csvReaderConfig 的map中请严格按照以下字段名字进行配置：\nboolean caseSensitive = true; char textQualifier = 34; boolean trimWhitespace = true; boolean useTextQualifier = true;//是否使用csv转义字符 char delimiter = 44;//分隔符 char recordDelimiter = 0; char comment = 35; boolean useComments = false; int escapeMode = 1; boolean safetySwitch = true;//单列长度是否限制100000字符 boolean skipEmptyRecords = true;//是否跳过空行 boolean captureRawRecord = true; 3.3 类型转换 OSS本身不提供数据类型，该类型是DataX OSSReader定义：\nDataX 内部类型 OSS 数据类型 Long Long Double Double String String Boolean Boolean Date Date 其中：\nOSS Long是指OSS文本中使用整形的字符串表示形式，例如\u0026quot;19901219\u0026quot;。 OSS Double是指OSS文本中使用Double的字符串表示形式，例如\u0026quot;3.1415\u0026quot;。 OSS Boolean是指OSS文本中使用Boolean的字符串表示形式，例如\u0026quot;true\u0026quot;、\u0026ldquo;false\u0026rdquo;。不区分大小写。 OSS Date是指OSS文本中使用Date的字符串表示形式，例如\u0026quot;2014-12-31\u0026quot;，Date可以指定format格式。 4 性能报告 并发数 DataX 流量 Datax 记录数 1 971.40KB/s 10047rec/s 2 1.81MB/s 19181rec/s 4 3.46MB/s 36695rec/s 8 6.57MB/s 69289 records/s 16 7.92MB/s 83920 records/s 32 7.87MB/s 83350 records/s 5 约束限制 略\n6 FAQ 略\n","permalink":"http://121.199.2.5:6080/bf15e9112ab546c9a6d8baab000d9601/","summary":"DataX OSSReader 说明 1 快速介绍 OSSReader提供了读取OSS数据存储的能力。在底层实现上，OSSReader使用OSS官方Java SDK获取OSS数据，并转换为DataX传输协议传递给Writer。\nOSS 产品介绍, 参看[阿里云OSS Portal] OSS Java SDK, 参看[阿里云OSS Java SDK] 2 功能与限制 OSSReader实现了从OSS读取数据并转为DataX协议的功能，OSS本身是无结构化数据存储，对于DataX而言，OSSReader实现上类比TxtFileReader，有诸多相似之处。目前OSSReader支持功能如下：\n支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量\n支持递归读取、支持文件名过滤。\n支持文本压缩，现有压缩格式为zip、gzip、bzip2。注意，一个压缩包不允许多文件打包压缩。\n多个object可以支持并发读取。\n我们暂时不能做到：\n单个Object(File)支持多线程并发读取，这里涉及到单个Object内部切分算法。二期考虑支持。\n单个Object在压缩情况下，从技术上无法支持多线程并发读取。\n3 功能说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: {}, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ossreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://oss.aliyuncs.com\u0026#34;, \u0026#34;accessId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;myBucket\u0026#34;, \u0026#34;object\u0026#34;: [ \u0026#34;bazhen/*\u0026#34; ], \u0026#34;column\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;index\u0026#34;: 0 }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;alibaba\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;index\u0026#34;: 1, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } ], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;\\t\u0026#34;, \u0026#34;compress\u0026#34;: \u0026#34;gzip\u0026#34; } }, \u0026#34;writer\u0026#34;: {} } ] } } 3.2 参数说明 endpoint\n描述：OSS Server的EndPoint地址，例如http://oss.","title":"DataX OSSReader 说明"},{"content":"DataX OSSWriter 说明 1 快速介绍 OSSWriter提供了向OSS写入类CSV格式的一个或者多个表文件。\n写入OSS内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\nOSS 产品介绍, 参看[阿里云OSS Portal] OSS Java SDK, 参看[阿里云OSS Java SDK] 2 功能与限制 OSSWriter实现了从DataX协议转为OSS中的TXT文件功能，OSS本身是无结构化数据存储，OSSWriter需要在如下几个方面增加:\n支持且仅支持写入 TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n暂时不支持文本压缩。\n支持多线程写入，每个线程写入不同子文件。\n文件支持滚动，当文件大于某个size值或者行数值，文件需要切换。 [暂不支持]\n我们不能做到：\n单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: {}, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;osswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://oss.aliyuncs.com\u0026#34;, \u0026#34;accessId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;myBucket\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;cdo/datax\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate|append|nonConflict\u0026#34; } } } ] } } 3.2 参数说明 endpoint\n描述：OSS Server的EndPoint地址，例如http://oss.aliyuncs.com。\n必选：是 默认值：无 accessId\n描述：OSS的accessId 必选：是 默认值：无 accessKey\n描述：OSS的accessKey 必选：是 默认值：无 bucket\n描述：OSS的bucket 必选：是 默认值：无 object\n描述：OSSWriter写入的文件名，OSS使用文件名模拟目录的实现。 使用\u0026quot;object\u0026quot;: \u0026ldquo;datax\u0026rdquo;，写入object以datax开头，后缀添加随机字符串。 使用\u0026quot;object\u0026quot;: \u0026ldquo;cdo/datax\u0026rdquo;，写入的object以cdo/datax开头，后缀随机添加字符串，/作为OSS模拟目录的分隔符。\n必选：是 默认值：无 writeMode\n描述：OSSWriter写入前数据清理处理： truncate，写入前清理object名称前缀匹配的所有object。例如: \u0026ldquo;object\u0026rdquo;: \u0026ldquo;abc\u0026rdquo;，将清理所有abc开头的object。 append，写入前不做任何处理，DataX OSSWriter直接使用object名称写入，并使用随机UUID的后缀名来保证文件名不冲突。例如用户指定的object名为datax，实际写入为datax_xxxxxx_xxxx_xxxx nonConflict，如果指定路径出现前缀匹配的object，直接报错。例如: \u0026ldquo;object\u0026rdquo;: \u0026ldquo;abc\u0026rdquo;，如果存在abc123的object，将直接报错。 必选：是 默认值：无 fieldDelimiter\n描述：读取的字段分隔符 必选：否 默认值：, encoding\n描述：写出文件的编码配置。\n必选：否 默认值：utf-8 nullFormat\n描述：文本文件中无法使用标准字符串定义null(空指针)，DataX提供nullFormat定义哪些字符串可以表示为null。\n例如如果用户配置: nullFormat=\u0026quot;\\N\u0026quot;，那么如果源头数据是\u0026quot;\\N\u0026quot;，DataX视作null字段。\n必选：否 默认值：\\N dateFormat\n描述：日期类型的数据序列化到object中时的格式，例如 \u0026ldquo;dateFormat\u0026rdquo;: \u0026ldquo;yyyy-MM-dd\u0026rdquo;。\n必选：否 默认值：无 fileFormat\n描述：文件写出的格式，包括csv (http://zh.wikipedia.org/wiki/%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E5%80%BC) 和text两种，csv是严格的csv格式，如果待写数据包括列分隔符，则会按照csv的转义语法转义，转义符号为双引号\u0026quot;；text格式是用列分隔符简单分割待写数据，对于待写数据包括列分隔符情况下不做转义。\n必选：否 默认值：text header\n描述：Oss写出时的表头，示例[\u0026lsquo;id\u0026rsquo;, \u0026rsquo;name\u0026rsquo;, \u0026lsquo;age\u0026rsquo;]。\n必选：否 默认值：无 maxFileSize\n描述：Oss写出时单个Object文件的最大大小，默认为10000*10MB，类似log4j日志打印时根据日志文件大小轮转。OSS分块上传时，每个分块大小为10MB，每个OSS InitiateMultipartUploadRequest支持的分块最大数量为10000。轮转发生时，object名字规则是：在原有object前缀加UUID随机数的基础上，拼接_1,_2,_3等后缀。\n必选：否 默认值：100000MB 3.3 类型转换 4 性能报告 OSS本身不提供数据类型，该类型是DataX OSSWriter定义：\nDataX 内部类型 OSS 数据类型 Long Long Double Double String String Boolean Boolean Date Date 其中：\nOSS Long是指OSS文本中使用整形的字符串表示形式，例如\u0026quot;19901219\u0026quot;。 OSS Double是指OSS文本中使用Double的字符串表示形式，例如\u0026quot;3.1415\u0026quot;。 OSS Boolean是指OSS文本中使用Boolean的字符串表示形式，例如\u0026quot;true\u0026quot;、\u0026ldquo;false\u0026rdquo;。不区分大小写。 OSS Date是指OSS文本中使用Date的字符串表示形式，例如\u0026quot;2014-12-31\u0026quot;，Date可以指定format格式。 5 约束限制 略\n6 FAQ 略\n","permalink":"http://121.199.2.5:6080/516202a7c10f4658ad32aa517c141a05/","summary":"DataX OSSWriter 说明 1 快速介绍 OSSWriter提供了向OSS写入类CSV格式的一个或者多个表文件。\n写入OSS内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\nOSS 产品介绍, 参看[阿里云OSS Portal] OSS Java SDK, 参看[阿里云OSS Java SDK] 2 功能与限制 OSSWriter实现了从DataX协议转为OSS中的TXT文件功能，OSS本身是无结构化数据存储，OSSWriter需要在如下几个方面增加:\n支持且仅支持写入 TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n暂时不支持文本压缩。\n支持多线程写入，每个线程写入不同子文件。\n文件支持滚动，当文件大于某个size值或者行数值，文件需要切换。 [暂不支持]\n我们不能做到：\n单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: {}, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;osswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://oss.aliyuncs.com\u0026#34;, \u0026#34;accessId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;myBucket\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;cdo/datax\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate|append|nonConflict\u0026#34; } } } ] } } 3.2 参数说明 endpoint\n描述：OSS Server的EndPoint地址，例如http://oss.aliyuncs.com。\n必选：是 默认值：无 accessId\n描述：OSS的accessId 必选：是 默认值：无 accessKey\n描述：OSS的accessKey 必选：是 默认值：无 bucket\n描述：OSS的bucket 必选：是 默认值：无 object\n描述：OSSWriter写入的文件名，OSS使用文件名模拟目录的实现。 使用\u0026quot;object\u0026quot;: \u0026ldquo;datax\u0026rdquo;，写入object以datax开头，后缀添加随机字符串。 使用\u0026quot;object\u0026quot;: \u0026ldquo;cdo/datax\u0026rdquo;，写入的object以cdo/datax开头，后缀随机添加字符串，/作为OSS模拟目录的分隔符。\n必选：是 默认值：无 writeMode","title":"DataX OSSWriter 说明"},{"content":"DataX PostgresqlWriter 1 快速介绍 PostgresqlWriter插件实现了写入数据到 PostgreSQL主库目的表的功能。在底层实现上，PostgresqlWriter通过JDBC连接远程 PostgreSQL 数据库，并执行相应的 insert into \u0026hellip; sql 语句将数据写入 PostgreSQL，内部会分批次提交入库。\nPostgresqlWriter面向ETL开发工程师，他们使用PostgresqlWriter从数仓导入数据到PostgreSQL。同时 PostgresqlWriter亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 PostgresqlWriter通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL插入语句\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2. PostgresqlWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 PostgresqlWriter导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;postgresqlwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:postgresql://127.0.0.1:3002/datax\u0026#34;, \u0026#34;table\u0026#34;: [ \u0026#34;test\u0026#34; ] } ] } } } ] } } 3.2 参数说明 jdbcUrl\n描述：目的数据库的 JDBC 连接信息 ,jdbcUrl必须包含在connection配置单元中。\n注意：1、在一个数据库上只能配置一个值。 2、jdbcUrl按照PostgreSQL官方规范，并可以填写连接附加参数信息。具体请参看PostgreSQL官方文档或者咨询对应 DBA。\n必选：是 默认值：无 username\n描述：目的数据库的用户名 必选：是 默认值：无 password\n描述：目的数据库的密码 必选：是 默认值：无 table\n描述：目的表的表名称。支持写入一个或者多个表。当配置为多张表时，必须确保所有表结构保持一致。\n注意：table 和 jdbcUrl 必须包含在 connection 配置单元中 必选：是 默认值：无 column\n描述：目的表需要写入数据的字段,字段之间用英文逗号分隔。例如: \u0026ldquo;column\u0026rdquo;: [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;,\u0026ldquo;age\u0026rdquo;]。如果要依次写入全部列，使用*表示, 例如: \u0026ldquo;column\u0026rdquo;: [\u0026quot;*\u0026quot;]\n注意：1、我们强烈不推荐你这样配置，因为当你目的表字段个数、类型等有改动时，你的任务可能运行不正确或者失败 2、此处 column 不能配置任何常量值 必选：是 默认值：否 preSql\n描述：写入数据到目的表前，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，请使用 @table 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。比如你的任务是要写入到目的端的100个同构分表(表名称为:datax_00,datax01, \u0026hellip; datax_98,datax_99)，并且你希望导入数据前，先对表中数据进行删除操作，那么你可以这样配置：\u0026quot;preSql\u0026quot;:[\u0026quot;delete from @table\u0026quot;]，效果是：在执行到每个表写入数据前，会先执行对应的 delete from 对应表名称 必选：否 默认值：无 postSql\n描述：写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ） 必选：否 默认值：无 batchSize\n描述：一次性批量提交的记录数大小，该值可以极大减少DataX与PostgreSql的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。\n必选：否 默认值：1024 3.3 类型转换 目前 PostgresqlWriter支持大部分 PostgreSQL类型，但也存在部分没有支持的情况，请注意检查你的类型。\n下面列出 PostgresqlWriter针对 PostgreSQL类型转换列表:\nDataX 内部类型 PostgreSQL 数据类型 Long bigint, bigserial, integer, smallint, serial Double double precision, money, numeric, real String varchar, char, text, bit Date date, time, timestamp Boolean bool Bytes bytea 4 性能报告 4.1 环境准备 4.1.1 数据特征 建表语句：\ncreate table pref_test( id serial, a_bigint bigint, a_bit bit(10), a_boolean boolean, a_char character(5), a_date date, a_double double precision, a_integer integer, a_money money, a_num numeric(10,2), a_real real, a_smallint smallint, a_text text, a_time time, a_timestamp timestamp )\n4.1.2 机器参数 执行DataX的机器参数为:\ncpu: 16核 Intel(R) Xeon(R) CPU E5620 @ 2.40GHz mem: MemTotal: 24676836kB MemFree: 6365080kB net: 百兆双网卡 PostgreSQL数据库机器参数为: D12 24逻辑核 192G内存 12*480G SSD 阵列\n4.2 测试报告 4.2.1 单表测试报告 通道数 批量提交batchSize DataX速度(Rec/s) DataX流量(M/s) DataX机器运行负载 1 128 9259 0.55 0.3 1 512 10869 0.653 0.3 1 2048 9803 0.589 0.8 4 128 30303 1.82 1 4 512 36363 2.18 1 4 2048 36363 2.18 1 8 128 57142 3.43 2 8 512 66666 4.01 1.5 8 2048 66666 4.01 1.1 16 128 88888 5.34 1.8 16 2048 94117 5.65 2.5 32 512 76190 4.58 3 4.2.2 性能测试小结 channel数对性能影响很大 通常不建议写入数据库时，通道个数 \u0026gt; 32 FAQ Q: PostgresqlWriter 执行 postSql 语句报错，那么数据导入到目标数据库了吗?\nA: DataX 导入过程存在三块逻辑，pre 操作、导入操作、post 操作，其中任意一环报错，DataX 作业报错。由于 DataX 不能保证在同一个事务完成上述几个操作，因此有可能数据已经落入到目标端。\nQ: 按照上述说法，那么有部分脏数据导入数据库，如果影响到线上数据库怎么办?\nA: 目前有两种解法，第一种配置 pre 语句，该 sql 可以清理当天导入数据， DataX 每次导入时候可以把上次清理干净并导入完整数据。 第二种，向临时表导入数据，完成后再 rename 到线上表。\n","permalink":"http://121.199.2.5:6080/1c01500e456b4ef6b6c14eb5a072696e/","summary":"DataX PostgresqlWriter 1 快速介绍 PostgresqlWriter插件实现了写入数据到 PostgreSQL主库目的表的功能。在底层实现上，PostgresqlWriter通过JDBC连接远程 PostgreSQL 数据库，并执行相应的 insert into \u0026hellip; sql 语句将数据写入 PostgreSQL，内部会分批次提交入库。\nPostgresqlWriter面向ETL开发工程师，他们使用PostgresqlWriter从数仓导入数据到PostgreSQL。同时 PostgresqlWriter亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 PostgresqlWriter通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL插入语句\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2. PostgresqlWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 PostgresqlWriter导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34; : [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19880808, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 1000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;postgresqlwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from test\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:postgresql://127.","title":"DataX PostgresqlWriter"},{"content":"DataX SqlServerWriter 1 快速介绍 SqlServerWriter 插件实现了写入数据到 SqlServer 库的目的表的功能。在底层实现上， SqlServerWriter 通过 JDBC 连接远程 SqlServer 数据库，并执行相应的 insert into \u0026hellip; sql 语句将数据写入 SqlServer，内部会分批次提交入库。\nSqlServerWriter 面向ETL开发工程师，他们使用 SqlServerWriter 从数仓导入数据到 SqlServer。同时 SqlServerWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 SqlServerWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL语句\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2.SqlServerWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 SqlServer 导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: {}, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sqlserverwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;db_id\u0026#34;, \u0026#34;db_type\u0026#34;, \u0026#34;db_ip\u0026#34;, \u0026#34;db_port\u0026#34;, \u0026#34;db_role\u0026#34;, \u0026#34;db_name\u0026#34;, \u0026#34;db_username\u0026#34;, \u0026#34;db_password\u0026#34;, \u0026#34;db_modify_time\u0026#34;, \u0026#34;db_modify_user\u0026#34;, \u0026#34;db_description\u0026#34;, \u0026#34;db_tddl_info\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;db_info_for_writer\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:sqlserver://[HOST_NAME]:PORT;DatabaseName=[DATABASE_NAME]\u0026#34; } ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from @table where db_id = -1;\u0026#34; ], \u0026#34;postSql\u0026#34;: [ \u0026#34;update @table set db_modify_time = now() where db_id = 1;\u0026#34; ] } } } ] } } 3.2 参数说明 jdbcUrl\n描述：目的数据库的 JDBC 连接信息 ,jdbcUrl必须包含在connection配置单元中。\n注意：1、在一个数据库上只能配置一个值。这与 SqlServerReader 支持多个备库探测不同，因为此处不支持同一个数据库存在多个主库的情况(双主导入数据情况) 2、jdbcUrl按照SqlServer官方规范，并可以填写连接附加参数信息。具体请参看 SqlServer官方文档或者咨询对应 DBA。 必选：是 默认值：无 username\n描述：目的数据库的用户名 必选：是 默认值：无 password\n描述：目的数据库的密码 必选：是 默认值：无 table\n描述：目的表的表名称。支持写入一个或者多个表。当配置为多张表时，必须确保所有表结构保持一致。\n注意：table 和 jdbcUrl 必须包含在 connection 配置单元中 必选：是 默认值：无 column\n描述：目的表需要写入数据的字段,字段之间用英文逗号分隔。例如: \u0026ldquo;column\u0026rdquo;: [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;name\u0026rdquo;,\u0026ldquo;age\u0026rdquo;]。如果要依次写入全部列，使用表示, 例如: \u0026ldquo;column\u0026rdquo;: [\u0026quot;\u0026quot;]\n**column配置项必须指定，不能留空！** 注意：1、我们强烈不推荐你这样配置，因为当你目的表字段个数、类型等有改动时，你的任务可能运行不正确或者失败 2、此处 column 不能配置任何常量值 必选：是 默认值：否 preSql\n描述：写入数据到目的表前，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，请使用 @table 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。\n必选：否 默认值：无 postSql\n描述：写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ） 必选：否 默认值：无 batchSize\n描述：一次性批量提交的记录数大小，该值可以极大减少DataX与SqlServer的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。\n必选：否 默认值：1024 3.3 类型转换 类似 SqlServerReader ，目前 SqlServerWriter 支持大部分 SqlServer 类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出 SqlServerWriter 针对 SqlServer 类型转换列表:\nDataX 内部类型 SqlServer 数据类型 Long Double String Date Boolean Bytes 4 性能报告 4.1 环境准备 4.1.1 数据特征 建表语句：\n单行记录类似于：\n4.1.2 机器参数 执行 DataX 的机器参数为:\ncpu: 24 Core Intel(R) Xeon(R) CPU E5-2430 0 @ 2.20GHz mem: 94GB net: 千兆双网卡 disc: DataX 数据不落磁盘，不统计此项 SqlServer 数据库机器参数为:\ncpu: 4 Core Intel(R) Xeon(R) CPU E5420 @ 2.50GHz mem: 7GB 4.1.3 DataX jvm 参数 -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError 4.1.4 性能测试作业配置 4.2 测试报告 4.2.1 测试报告 5 约束限制 FAQ Q: SqlServerWriter 执行 postSql 语句报错，那么数据导入到目标数据库了吗?\nA: DataX 导入过程存在三块逻辑，pre 操作、导入操作、post 操作，其中任意一环报错，DataX 作业报错。由于 DataX 不能保证在同一个事务完成上述几个操作，因此有可能数据已经落入到目标端。\nQ: 按照上述说法，那么有部分脏数据导入数据库，如果影响到线上数据库怎么办?\nA: 目前有两种解法，第一种配置 pre 语句，该 sql 可以清理当天导入数据， DataX 每次导入时候可以把上次清理干净并导入完整数据。第二种，向临时表导入数据，完成后再 rename 到线上表。\nQ: 上面第二种方法可以避免对线上数据造成影响，那我具体怎样操作?\nA: 可以配置临时表导入\n","permalink":"http://121.199.2.5:6080/f23fb4346cfa45459b6eaa8537ff5d17/","summary":"DataX SqlServerWriter 1 快速介绍 SqlServerWriter 插件实现了写入数据到 SqlServer 库的目的表的功能。在底层实现上， SqlServerWriter 通过 JDBC 连接远程 SqlServer 数据库，并执行相应的 insert into \u0026hellip; sql 语句将数据写入 SqlServer，内部会分批次提交入库。\nSqlServerWriter 面向ETL开发工程师，他们使用 SqlServerWriter 从数仓导入数据到 SqlServer。同时 SqlServerWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 SqlServerWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL语句\ninsert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2.SqlServerWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 SqlServer 导入的数据。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: {}, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sqlserverwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;db_id\u0026#34;, \u0026#34;db_type\u0026#34;, \u0026#34;db_ip\u0026#34;, \u0026#34;db_port\u0026#34;, \u0026#34;db_role\u0026#34;, \u0026#34;db_name\u0026#34;, \u0026#34;db_username\u0026#34;, \u0026#34;db_password\u0026#34;, \u0026#34;db_modify_time\u0026#34;, \u0026#34;db_modify_user\u0026#34;, \u0026#34;db_description\u0026#34;, \u0026#34;db_tddl_info\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;db_info_for_writer\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:sqlserver://[HOST_NAME]:PORT;DatabaseName=[DATABASE_NAME]\u0026#34; } ], \u0026#34;preSql\u0026#34;: [ \u0026#34;delete from @table where db_id = -1;\u0026#34; ], \u0026#34;postSql\u0026#34;: [ \u0026#34;update @table set db_modify_time = now() where db_id = 1;\u0026#34; ] } } } ] } } 3.","title":"DataX SqlServerWriter"},{"content":"DataX Transformer Transformer定义 在数据同步、传输过程中，存在用户对于数据传输进行特殊定制化的需求场景，包括裁剪列、转换列等工作，可以借助ETL的T过程实现(Transformer)。DataX包含了完整的E(Extract)、T(Transformer)、L(Load)支持。\n运行模型 UDF手册 dx_substr 参数：3个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：字段值的开始位置。 第三个参数：目标字段长度。 返回： 从字符串的指定位置（包含）截取指定长度的字符串。如果开始位置非法抛出异常。如果字段为空值，直接返回（即不参与本transformer） 举例： dx_substr(1,\u0026#34;2\u0026#34;,\u0026#34;5\u0026#34;) column 1的value为“dataxTest”=\u0026gt;\u0026#34;taxTe\u0026#34; dx_substr(1,\u0026#34;5\u0026#34;,\u0026#34;10\u0026#34;) column 1的value为“dataxTest”=\u0026gt;\u0026#34;Test\u0026#34; dx_pad 参数：4个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：\u0026ldquo;l\u0026rdquo;,\u0026ldquo;r\u0026rdquo;, 指示是在头进行pad，还是尾进行pad。 第三个参数：目标字段长度。 第四个参数：需要pad的字符。 返回： 如果源字符串长度小于目标字段长度，按照位置添加pad字符后返回。如果长于，直接截断（都截右边）。如果字段为空值，转换为空字符串进行pad，即最后的字符串全是需要pad的字符 举例： dx_pad(1,\u0026#34;l\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;A\u0026#34;), 如果column 1 的值为 xyz=\u0026gt; Axyz， 值为 xyzzzzz =\u0026gt; xyzz dx_pad(1,\u0026#34;r\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;A\u0026#34;), 如果column 1 的值为 xyz=\u0026gt; xyzA， 值为 xyzzzzz =\u0026gt; xyzz dx_replace 参数：4个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：字段值的开始位置。 第三个参数：需要替换的字段长度。 第四个参数：需要替换的字符串。 返回： 从字符串的指定位置（包含）替换指定长度的字符串。如果开始位置非法抛出异常。如果字段为空值，直接返回（即不参与本transformer） 举例： dx_replace(1,\u0026#34;2\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;****\u0026#34;) column 1的value为“dataxTest”=\u0026gt;\u0026#34;da****est\u0026#34; dx_replace(1,\u0026#34;5\u0026#34;,\u0026#34;10\u0026#34;,\u0026#34;****\u0026#34;) column 1的value为“dataxTest”=\u0026gt;\u0026#34;data****\u0026#34; dx_filter （关联filter暂不支持，即多个字段的联合判断，函参太过复杂，用户难以使用。） 参数： 第一个参数：字段编号，对应record中第几个字段。 第二个参数：运算符，支持一下运算符：like, not like, \u0026gt;, =, \u0026lt;, \u0026gt;=, !=, \u0026lt;= 第三个参数：正则表达式（java正则表达式）、值。 返回： 如果匹配正则表达式，返回Null，表示过滤该行。不匹配表达式时，表示保留该行。（注意是该行）。对于\u0026gt;=\u0026lt;都是对字段直接compare的结果. like ， not like是将字段转换成String，然后和目标正则表达式进行全匹配。 , =, \u0026lt;, \u0026gt;=, !=, \u0026lt;= 对于DoubleColumn比较double值，对于LongColumn和DateColumn比较long值，其他StringColumn，BooleanColumn以及ByteColumn均比较的是StringColumn值。\n如果目标colunn为空（null），对于 = null的过滤条件，将满足条件，被过滤。！=null的过滤条件，null不满足过滤条件，不被过滤。 like，字段为null不满足条件，不被过滤，和not like，字段为null满足条件，被过滤。 举例： dx_filter(1,\u0026#34;like\u0026#34;,\u0026#34;dataTest\u0026#34;) dx_filter(1,\u0026#34;\u0026gt;=\u0026#34;,\u0026#34;10\u0026#34;) dx_groovy 参数。 第一个参数： groovy code 第二个参数（列表或者为空）：extraPackage 备注： dx_groovy只能调用一次。不能多次调用。 groovy code中支持java.lang, java.util的包，可直接引用的对象有record，以及element下的各种column（BoolColumn.class,BytesColumn.class,DateColumn.class,DoubleColumn.class,LongColumn.class,StringColumn.class）。不支持其他包，如果用户有需要用到其他包，可设置extraPackage，注意extraPackage不支持第三方jar包。 groovy code中，返回更新过的Record（比如record.setColumn(columnIndex, new StringColumn(newValue));），或者null。返回null表示过滤此行。 用户可以直接调用静态的Util方式（GroovyTransformerStaticUtil），目前GroovyTransformerStaticUtil的方法列表 (按需补充)： 举例: groovy 实现的subStr: String code = \u0026#34;Column column = record.getColumn(1);\\n\u0026#34; + \u0026#34; String oriValue = column.asString();\\n\u0026#34; + \u0026#34; String newValue = oriValue.substring(0, 3);\\n\u0026#34; + \u0026#34; record.setColumn(1, new StringColumn(newValue));\\n\u0026#34; + \u0026#34; return record;\u0026#34;; dx_groovy(record); groovy 实现的Replace String code2 = \u0026#34;Column column = record.getColumn(1);\\n\u0026#34; + \u0026#34; String oriValue = column.asString();\\n\u0026#34; + \u0026#34; String newValue = \\\u0026#34;****\\\u0026#34; + oriValue.substring(3, oriValue.length());\\n\u0026#34; + \u0026#34; record.setColumn(1, new StringColumn(newValue));\\n\u0026#34; + \u0026#34; return record;\u0026#34;; groovy 实现的Pad String code3 = \u0026#34;Column column = record.getColumn(1);\\n\u0026#34; + \u0026#34; String oriValue = column.asString();\\n\u0026#34; + \u0026#34; String padString = \\\u0026#34;12345\\\u0026#34;;\\n\u0026#34; + \u0026#34; String finalPad = \\\u0026#34;\\\u0026#34;;\\n\u0026#34; + \u0026#34; int NeedLength = 8 - oriValue.length();\\n\u0026#34; + \u0026#34; while (NeedLength \u0026gt; 0) {\\n\u0026#34; + \u0026#34;\\n\u0026#34; + \u0026#34; if (NeedLength \u0026gt;= padString.length()) {\\n\u0026#34; + \u0026#34; finalPad += padString;\\n\u0026#34; + \u0026#34; NeedLength -= padString.length();\\n\u0026#34; + \u0026#34; } else {\\n\u0026#34; + \u0026#34; finalPad += padString.substring(0, NeedLength);\\n\u0026#34; + \u0026#34; NeedLength = 0;\\n\u0026#34; + \u0026#34; }\\n\u0026#34; + \u0026#34; }\\n\u0026#34; + \u0026#34; String newValue= finalPad + oriValue;\\n\u0026#34; + \u0026#34; record.setColumn(1, new StringColumn(newValue));\\n\u0026#34; + \u0026#34; return record;\u0026#34;; Job定义 本例中，配置3个UDF。 { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 }, \u0026#34;errorLimit\u0026#34;: { \u0026#34;record\u0026#34;: 0 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;column\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;value\u0026#34;: 19890604, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;1989-06-04 00:00:00\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34; } ], \u0026#34;sliceRecordCount\u0026#34;: 100000 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } }, \u0026#34;transformer\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;dx_substr\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;columnIndex\u0026#34;:5, \u0026#34;paras\u0026#34;:[\u0026#34;1\u0026#34;,\u0026#34;3\u0026#34;] } }, { \u0026#34;name\u0026#34;: \u0026#34;dx_replace\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;columnIndex\u0026#34;:4, \u0026#34;paras\u0026#34;:[\u0026#34;3\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;****\u0026#34;] } }, { \u0026#34;name\u0026#34;: \u0026#34;dx_groovy\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;//groovy code//\u0026#34;, \u0026#34;extraPackage\u0026#34;:[ \u0026#34;import somePackage1;\u0026#34;, \u0026#34;import somePackage2;\u0026#34; ] } } ] } ] } } 计量和脏数据 Transform过程涉及到数据的转换，可能造成数据的增加或减少，因此更加需要精确度量，包括：\nTransform的入参Record条数、字节数。 Transform的出参Record条数、字节数。 Transform的脏数据Record条数、字节数。 如果是多个Transform，某一个发生脏数据，将不会再进行后面的transform，直接统计为脏数据。 目前只提供了所有Transform的计量（成功，失败，过滤的count，以及transform的消耗时间）。 涉及到运行过程的计量数据展现定义如下：\nTotal 1000000 records, 22000000 bytes | Transform 100000 records(in), 10000 records(out) | Speed 2.10MB/s, 100000 records/s | Error 0 records, 0 bytes | Percentage 100.00% 注意，这里主要记录转换的输入输出，需要检测数据输入输出的记录数量变化。\n涉及到最终作业的计量数据展现定义如下：\n任务启动时刻 : 2015-03-10 17:34:21 任务结束时刻 : 2015-03-10 17:34:31 任务总计耗时 : 10s 任务平均流量 : 2.10MB/s 记录写入速度 : 100000rec/s 转换输入总数\t: 1000000 转换输出总数\t: 1000000 读出记录总数 : 1000000 同步失败总数 : 0 注意，这里主要记录转换的输入输出，需要检测数据输入输出的记录数量变化。\n","permalink":"http://121.199.2.5:6080/760d0498cfcd435bafba3fe96793cd01/","summary":"DataX Transformer Transformer定义 在数据同步、传输过程中，存在用户对于数据传输进行特殊定制化的需求场景，包括裁剪列、转换列等工作，可以借助ETL的T过程实现(Transformer)。DataX包含了完整的E(Extract)、T(Transformer)、L(Load)支持。\n运行模型 UDF手册 dx_substr 参数：3个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：字段值的开始位置。 第三个参数：目标字段长度。 返回： 从字符串的指定位置（包含）截取指定长度的字符串。如果开始位置非法抛出异常。如果字段为空值，直接返回（即不参与本transformer） 举例： dx_substr(1,\u0026#34;2\u0026#34;,\u0026#34;5\u0026#34;) column 1的value为“dataxTest”=\u0026gt;\u0026#34;taxTe\u0026#34; dx_substr(1,\u0026#34;5\u0026#34;,\u0026#34;10\u0026#34;) column 1的value为“dataxTest”=\u0026gt;\u0026#34;Test\u0026#34; dx_pad 参数：4个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：\u0026ldquo;l\u0026rdquo;,\u0026ldquo;r\u0026rdquo;, 指示是在头进行pad，还是尾进行pad。 第三个参数：目标字段长度。 第四个参数：需要pad的字符。 返回： 如果源字符串长度小于目标字段长度，按照位置添加pad字符后返回。如果长于，直接截断（都截右边）。如果字段为空值，转换为空字符串进行pad，即最后的字符串全是需要pad的字符 举例： dx_pad(1,\u0026#34;l\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;A\u0026#34;), 如果column 1 的值为 xyz=\u0026gt; Axyz， 值为 xyzzzzz =\u0026gt; xyzz dx_pad(1,\u0026#34;r\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;A\u0026#34;), 如果column 1 的值为 xyz=\u0026gt; xyzA， 值为 xyzzzzz =\u0026gt; xyzz dx_replace 参数：4个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：字段值的开始位置。 第三个参数：需要替换的字段长度。 第四个参数：需要替换的字符串。 返回： 从字符串的指定位置（包含）替换指定长度的字符串。如果开始位置非法抛出异常。如果字段为空值，直接返回（即不参与本transformer） 举例： dx_replace(1,\u0026#34;2\u0026#34;,\u0026#34;4\u0026#34;,\u0026#34;****\u0026#34;) column 1的value为“dataxTest”=\u0026gt;\u0026#34;da****est\u0026#34; dx_replace(1,\u0026#34;5\u0026#34;,\u0026#34;10\u0026#34;,\u0026#34;****\u0026#34;) column 1的value为“dataxTest”=\u0026gt;\u0026#34;data****\u0026#34; dx_filter （关联filter暂不支持，即多个字段的联合判断，函参太过复杂，用户难以使用。） 参数： 第一个参数：字段编号，对应record中第几个字段。 第二个参数：运算符，支持一下运算符：like, not like, \u0026gt;, =, \u0026lt;, \u0026gt;=, !=, \u0026lt;= 第三个参数：正则表达式（java正则表达式）、值。 返回： 如果匹配正则表达式，返回Null，表示过滤该行。不匹配表达式时，表示保留该行。（注意是该行）。对于\u0026gt;=\u0026lt;都是对字段直接compare的结果. like ， not like是将字段转换成String，然后和目标正则表达式进行全匹配。 , =, \u0026lt;, \u0026gt;=, !=, \u0026lt;= 对于DoubleColumn比较double值，对于LongColumn和DateColumn比较long值，其他StringColumn，BooleanColumn以及ByteColumn均比较的是StringColumn值。\n如果目标colunn为空（null），对于 = null的过滤条件，将满足条件，被过滤。！=null的过滤条件，null不满足过滤条件，不被过滤。 like，字段为null不满足条件，不被过滤，和not like，字段为null满足条件，被过滤。 举例： dx_filter(1,\u0026#34;like\u0026#34;,\u0026#34;dataTest\u0026#34;) dx_filter(1,\u0026#34;\u0026gt;=\u0026#34;,\u0026#34;10\u0026#34;) dx_groovy 参数。 第一个参数： groovy code 第二个参数（列表或者为空）：extraPackage 备注： dx_groovy只能调用一次。不能多次调用。 groovy code中支持java.","title":"DataX Transformer"},{"content":"DataX TxtFileReader 说明 1 快速介绍 TxtFileReader提供了读取本地文件系统数据存储的能力。在底层实现上，TxtFileReader获取本地文件数据，并转换为DataX传输协议传递给Writer。\n本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\n2 功能与限制 TxtFileReader实现了从本地文件读取数据并转为DataX协议的功能，本地文件本身是无结构化数据存储，对于DataX而言，TxtFileReader实现上类比OSSReader，有诸多相似之处。目前TxtFileReader支持功能如下：\n支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量\n支持递归读取、支持文件名过滤。\n支持文本压缩，现有压缩格式为zip、gzip、bzip2。\n多个File可以支持并发读取。\n我们暂时不能做到：\n单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。\n单个File在压缩情况下，从技术上无法支持多线程并发读取。\n3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: [\u0026#34;/home/haiwei.luo/case00/data\u0026#34;], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy.MM.dd\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/home/haiwei.luo/case00/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;luohw\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } } } ] } } 3.2 参数说明 path\n描述：本地文件系统的路径信息，注意这里可以支持填写多个路径。 当指定单个本地文件，TxtFileReader暂时只能使用单线程进行数据抽取。二期考虑在非压缩文件情况下针对单个File可以进行多线程并发读取。\n当指定多个本地文件，TxtFileReader支持使用多线程进行数据抽取。线程并发数通过通道数指定。\n当指定通配符，TxtFileReader尝试遍历出多个文件信息。例如: 指定/*代表读取/目录下所有的文件，指定/bazhen/*代表读取bazhen目录下游所有的文件。TxtFileReader目前只支持*作为文件通配符。\n特别需要注意的是，DataX会将一个作业下同步的所有Text File视作同一张数据表。用户必须自己保证所有的File能够适配同一套schema信息。读取文件用户必须保证为类CSV格式，并且提供给DataX权限可读。\n特别需要注意的是，如果Path指定的路径下没有符合匹配的文件抽取，DataX将报错。\n必选：是 默认值：无 column\n描述：读取字段列表，type指定源数据的类型，index指定当前列来自于文本第几列(以0开始)，value指定当前类型为常量，不从源头文件读取数据，而是根据value值自动生成对应的列。 默认情况下，用户可以全部按照String类型读取数据，配置如下：\n\u0026#34;column\u0026#34;: [\u0026#34;*\u0026#34;] 用户可以指定Column字段信息，配置如下：\n{ \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;index\u0026#34;: 0 //从本地文件文本第一列获取int字段 }, { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;alibaba\u0026#34; //从TxtFileReader内部生成alibaba的字符串字段作为当前字段 } 对于用户指定Column信息，type必须填写，index/value必须选择其一。\n必选：是 默认值：全部按照string类型读取 fieldDelimiter\n描述：读取的字段分隔符 必选：是 默认值：, compress\n描述：文本压缩类型，默认不填写意味着没有压缩。支持压缩类型为zip、gzip、bzip2。 必选：否 默认值：没有压缩 encoding\n描述：读取文件的编码配置。\n必选：否 默认值：utf-8 skipHeader\n描述：类CSV格式文件可能存在表头为标题情况，需要跳过。默认不跳过。\n必选：否 默认值：false nullFormat\n描述：文本文件中无法使用标准字符串定义null(空指针)，DataX提供nullFormat定义哪些字符串可以表示为null。\n例如如果用户配置: nullFormat:\u0026quot;\\N\u0026quot;，那么如果源头数据是\u0026quot;\\N\u0026quot;，DataX视作null字段。\n必选：否 默认值：\\N csvReaderConfig\n描述：读取CSV类型文件参数配置，Map类型。读取CSV类型文件使用的CsvReader进行读取，会有很多配置，不配置则使用默认值。\n必选：否 默认值：无 常见配置：\n\u0026#34;csvReaderConfig\u0026#34;:{ \u0026#34;safetySwitch\u0026#34;: false, \u0026#34;skipEmptyRecords\u0026#34;: false, \u0026#34;useTextQualifier\u0026#34;: false } 所有配置项及默认值,配置时 csvReaderConfig 的map中请严格按照以下字段名字进行配置：\nboolean caseSensitive = true; char textQualifier = 34; boolean trimWhitespace = true; boolean useTextQualifier = true;//是否使用csv转义字符 char delimiter = 44;//分隔符 char recordDelimiter = 0; char comment = 35; boolean useComments = false; int escapeMode = 1; boolean safetySwitch = true;//单列长度是否限制100000字符 boolean skipEmptyRecords = true;//是否跳过空行 boolean captureRawRecord = true; 3.3 类型转换 本地文件本身不提供数据类型，该类型是DataX TxtFileReader定义：\nDataX 内部类型 本地文件 数据类型 Long Long Double Double String String Boolean Boolean Date Date 其中：\n本地文件 Long是指本地文件文本中使用整形的字符串表示形式，例如\u0026quot;19901219\u0026quot;。 本地文件 Double是指本地文件文本中使用Double的字符串表示形式，例如\u0026quot;3.1415\u0026quot;。 本地文件 Boolean是指本地文件文本中使用Boolean的字符串表示形式，例如\u0026quot;true\u0026quot;、\u0026ldquo;false\u0026rdquo;。不区分大小写。 本地文件 Date是指本地文件文本中使用Date的字符串表示形式，例如\u0026quot;2014-12-31\u0026quot;，Date可以指定format格式。 4 性能报告 5 约束限制 略\n6 FAQ 略\n","permalink":"http://121.199.2.5:6080/cbeab748638041dfa63c78ebe4fa91db/","summary":"DataX TxtFileReader 说明 1 快速介绍 TxtFileReader提供了读取本地文件系统数据存储的能力。在底层实现上，TxtFileReader获取本地文件数据，并转换为DataX传输协议传递给Writer。\n本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\n2 功能与限制 TxtFileReader实现了从本地文件读取数据并转为DataX协议的功能，本地文件本身是无结构化数据存储，对于DataX而言，TxtFileReader实现上类比OSSReader，有诸多相似之处。目前TxtFileReader支持功能如下：\n支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量\n支持递归读取、支持文件名过滤。\n支持文本压缩，现有压缩格式为zip、gzip、bzip2。\n多个File可以支持并发读取。\n我们暂时不能做到：\n单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。\n单个File在压缩情况下，从技术上无法支持多线程并发读取。\n3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: [\u0026#34;/home/haiwei.luo/case00/data\u0026#34;], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy.MM.dd\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/home/haiwei.luo/case00/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;luohw\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } } } ] } } 3.","title":"DataX TxtFileReader 说明"},{"content":"DataX TxtFileWriter 说明 1 快速介绍 TxtFileWriter提供了向本地文件写入类CSV格式的一个或者多个表文件。TxtFileWriter服务的用户主要在于DataX开发、测试同学。\n写入本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\n2 功能与限制 TxtFileWriter实现了从DataX协议转为本地TXT文件功能，本地文件本身是无结构化数据存储，TxtFileWriter如下几个方面约定:\n支持且仅支持写入 TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n支持文本压缩，现有压缩格式为gzip、bzip2。\n支持多线程写入，每个线程写入不同子文件。\n文件支持滚动，当文件大于某个size值或者行数值，文件需要切换。 [暂不支持]\n我们不能做到：\n单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: [\u0026#34;/home/haiwei.luo/case00/data\u0026#34;], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy.MM.dd\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/home/haiwei.luo/case00/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;luohw\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34;, \u0026#34;dateFormat\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } } } ] } } 3.2 参数说明 path\n描述：本地文件系统的路径信息，TxtFileWriter会写入Path目录下属多个文件。 必选：是 默认值：无 fileName\n描述：TxtFileWriter写入的文件名，该文件名会添加随机的后缀作为每个线程写入实际文件名。 必选：是 默认值：无 writeMode\n描述：TxtFileWriter写入前数据清理处理模式： truncate，写入前清理目录下一fileName前缀的所有文件。 append，写入前不做任何处理，DataX TxtFileWriter直接使用filename写入，并保证文件名不冲突。 nonConflict，如果目录下有fileName前缀的文件，直接报错。 必选：是 默认值：无 fieldDelimiter\n描述：读取的字段分隔符 必选：否 默认值：, compress\n描述：文本压缩类型，默认不填写意味着没有压缩。支持压缩类型为zip、lzo、lzop、tgz、bzip2。 必选：否 默认值：无压缩 encoding\n描述：读取文件的编码配置。\n必选：否 默认值：utf-8 nullFormat\n描述：文本文件中无法使用标准字符串定义null(空指针)，DataX提供nullFormat定义哪些字符串可以表示为null。\n例如如果用户配置: nullFormat=\u0026quot;\\N\u0026quot;，那么如果源头数据是\u0026quot;\\N\u0026quot;，DataX视作null字段。\n必选：否 默认值：\\N dateFormat\n描述：日期类型的数据序列化到文件中时的格式，例如 \u0026ldquo;dateFormat\u0026rdquo;: \u0026ldquo;yyyy-MM-dd\u0026rdquo;。\n必选：否 默认值：无 fileFormat\n描述：文件写出的格式，包括csv (http://zh.wikipedia.org/wiki/%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E5%80%BC) 和text两种，csv是严格的csv格式，如果待写数据包括列分隔符，则会按照csv的转义语法转义，转义符号为双引号\u0026quot;；text格式是用列分隔符简单分割待写数据，对于待写数据包括列分隔符情况下不做转义。\n必选：否 默认值：text header\n描述：txt写出时的表头，示例[\u0026lsquo;id\u0026rsquo;, \u0026rsquo;name\u0026rsquo;, \u0026lsquo;age\u0026rsquo;]。\n必选：否 默认值：无 3.3 类型转换 本地文件本身不提供数据类型，该类型是DataX TxtFileWriter定义：\nDataX 内部类型 本地文件 数据类型 Long Long Double Double String String Boolean Boolean Date Date 其中：\n本地文件 Long是指本地文件文本中使用整形的字符串表示形式，例如\u0026quot;19901219\u0026quot;。 本地文件 Double是指本地文件文本中使用Double的字符串表示形式，例如\u0026quot;3.1415\u0026quot;。 本地文件 Boolean是指本地文件文本中使用Boolean的字符串表示形式，例如\u0026quot;true\u0026quot;、\u0026ldquo;false\u0026rdquo;。不区分大小写。 本地文件 Date是指本地文件文本中使用Date的字符串表示形式，例如\u0026quot;2014-12-31\u0026quot;，Date可以指定format格式。 4 性能报告 5 约束限制 略\n6 FAQ 略\n","permalink":"http://121.199.2.5:6080/599f9ae387e54760ab26e177bd423e82/","summary":"DataX TxtFileWriter 说明 1 快速介绍 TxtFileWriter提供了向本地文件写入类CSV格式的一个或者多个表文件。TxtFileWriter服务的用户主要在于DataX开发、测试同学。\n写入本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。\n2 功能与限制 TxtFileWriter实现了从DataX协议转为本地TXT文件功能，本地文件本身是无结构化数据存储，TxtFileWriter如下几个方面约定:\n支持且仅支持写入 TXT的文件，且要求TXT中shema为一张二维表。\n支持类CSV格式文件，自定义分隔符。\n支持文本压缩，现有压缩格式为gzip、bzip2。\n支持多线程写入，每个线程写入不同子文件。\n文件支持滚动，当文件大于某个size值或者行数值，文件需要切换。 [暂不支持]\n我们不能做到：\n单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { \u0026#34;setting\u0026#34;: {}, \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 2 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: [\u0026#34;/home/haiwei.luo/case00/data\u0026#34;], \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy.MM.dd\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/home/haiwei.luo/case00/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;luohw\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34;, \u0026#34;dateFormat\u0026#34;: \u0026#34;yyyy-MM-dd\u0026#34; } } } ] } } 3.","title":"DataX TxtFileWriter 说明"},{"content":"datax-kudu-plugin datax kudu的writer插件\n仅在kudu11进行过测试\n","permalink":"http://121.199.2.5:6080/3909c1ec487c48198ae49ab46ba98dcb/","summary":"datax-kudu-plugin datax kudu的writer插件\n仅在kudu11进行过测试","title":"datax-kudu-plugin"},{"content":"datax-kudu-plugins datax kudu的writer插件\neg:\n{ \u0026#34;name\u0026#34;: \u0026#34;kuduwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;kuduConfig\u0026#34;: { \u0026#34;kudu.master_addresses\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;timeout\u0026#34;: 60000, \u0026#34;sessionTimeout\u0026#34;: 60000 }, \u0026#34;table\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;replicaCount\u0026#34;: 3, \u0026#34;truncate\u0026#34;: false, \u0026#34;writeMode\u0026#34;: \u0026#34;upsert\u0026#34;, \u0026#34;partition\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;column1\u0026#34;: [ { \u0026#34;lower\u0026#34;: \u0026#34;2020-08-25\u0026#34;, \u0026#34;upper\u0026#34;: \u0026#34;2020-08-26\u0026#34; }, { \u0026#34;lower\u0026#34;: \u0026#34;2020-08-26\u0026#34;, \u0026#34;upper\u0026#34;: \u0026#34;2020-08-27\u0026#34; }, { \u0026#34;lower\u0026#34;: \u0026#34;2020-08-27\u0026#34;, \u0026#34;upper\u0026#34;: \u0026#34;2020-08-28\u0026#34; } ] }, \u0026#34;hash\u0026#34;: { \u0026#34;column\u0026#34;: [ \u0026#34;column1\u0026#34; ], \u0026#34;number\u0026#34;: 3 } }, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;name\u0026#34;: \u0026#34;c1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;primaryKey\u0026#34;: true }, { \u0026#34;index\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;c2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;compress\u0026#34;: \u0026#34;DEFAULT_COMPRESSION\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;AUTO_ENCODING\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;注解xxxx\u0026#34; } ], \u0026#34;batchSize\u0026#34;: 1024, \u0026#34;bufferSize\u0026#34;: 2048, \u0026#34;skipFail\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } 必须参数：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kuduwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;kuduConfig\u0026#34;: { \u0026#34;kudu.master_addresses\u0026#34;: \u0026#34;***\u0026#34; }, \u0026#34;table\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;c1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;primaryKey\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;c2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, }, { \u0026#34;name\u0026#34;: \u0026#34;c3\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;c4\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ] } } 主键列请写到最前面\n配置列表 name default description 是否必须 kuduConfig kudu配置 （kudu.master_addresses等） 是 table 导入目标表名 是 partition 分区 否 column 列 是 name 列名 是 type string 列的类型，现支持INT, FLOAT, STRING, BIGINT, DOUBLE, BOOLEAN, LONG。 否 index 升序排列 列索引位置(要么全部列都写，要么都不写)，如reader中取到的某一字段在第二位置（eg： name， id， age）但kudu目标表结构不同（eg：id，name， age），此时就需要将index赋值为（1，0，2），默认顺序（0，1，2） 否 primaryKey false 是否为主键（请将所有的主键列写在前面）,不表明主键将不会检查过滤脏数据 否 compress DEFAULT_COMPRESSION 压缩格式 否 encoding AUTO_ENCODING 编码 否 replicaCount 3 保留副本个数 否 hash hash分区 否 number 3 hash分区个数 否 range range分区 否 lower range分区下限 (eg: sql建表：partition value=\u0026lsquo;haha\u0026rsquo; 对应：“lower”：“haha”，“upper”：“haha\\000”) 否 upper range分区上限(eg: sql建表：partition \u0026ldquo;10\u0026rdquo; \u0026lt;= VALUES \u0026lt; \u0026ldquo;20\u0026rdquo; 对应：“lower”：“10”，“upper”：“20”) 否 truncate false 是否清空表，本质上是删表重建 否 writeMode upsert upsert，insert，update 否 batchSize 512 每xx行数据flush一次结果（最好不要超过1024） 否 bufferSize 3072 缓冲区大小 否 skipFail false 是否跳过插入不成功的数据 否 timeout 60000 client超时时间,如创建表，删除表操作的超时时间。单位：ms 否 sessionTimeout 60000 session超时时间 单位：ms 否 ","permalink":"http://121.199.2.5:6080/23a56c95c6c4402998fb8bf16e29fc05/","summary":"datax-kudu-plugins datax kudu的writer插件\neg:\n{ \u0026#34;name\u0026#34;: \u0026#34;kuduwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;kuduConfig\u0026#34;: { \u0026#34;kudu.master_addresses\u0026#34;: \u0026#34;***\u0026#34;, \u0026#34;timeout\u0026#34;: 60000, \u0026#34;sessionTimeout\u0026#34;: 60000 }, \u0026#34;table\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;replicaCount\u0026#34;: 3, \u0026#34;truncate\u0026#34;: false, \u0026#34;writeMode\u0026#34;: \u0026#34;upsert\u0026#34;, \u0026#34;partition\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;column1\u0026#34;: [ { \u0026#34;lower\u0026#34;: \u0026#34;2020-08-25\u0026#34;, \u0026#34;upper\u0026#34;: \u0026#34;2020-08-26\u0026#34; }, { \u0026#34;lower\u0026#34;: \u0026#34;2020-08-26\u0026#34;, \u0026#34;upper\u0026#34;: \u0026#34;2020-08-27\u0026#34; }, { \u0026#34;lower\u0026#34;: \u0026#34;2020-08-27\u0026#34;, \u0026#34;upper\u0026#34;: \u0026#34;2020-08-28\u0026#34; } ] }, \u0026#34;hash\u0026#34;: { \u0026#34;column\u0026#34;: [ \u0026#34;column1\u0026#34; ], \u0026#34;number\u0026#34;: 3 } }, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;name\u0026#34;: \u0026#34;c1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;primaryKey\u0026#34;: true }, { \u0026#34;index\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;c2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;compress\u0026#34;: \u0026#34;DEFAULT_COMPRESSION\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;AUTO_ENCODING\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;注解xxxx\u0026#34; } ], \u0026#34;batchSize\u0026#34;: 1024, \u0026#34;bufferSize\u0026#34;: 2048, \u0026#34;skipFail\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } 必须参数：","title":"datax-kudu-plugins"},{"content":"DataX插件开发宝典 本文面向DataX插件开发人员，尝试尽可能全面地阐述开发一个DataX插件所经过的历程，力求消除开发者的困惑，让插件开发变得简单。\n一、开发之前 路走对了，就不怕远。✓ 路走远了，就不管对不对。✕\n当你打开这篇文档，想必已经不用在此解释什么是DataX了。那下一个问题便是：\nDataX为什么要使用插件机制？ 从设计之初，DataX就把异构数据源同步作为自身的使命，为了应对不同数据源的差异、同时提供一致的同步原语和扩展能力，DataX自然而然地采用了框架 + 插件 的模式：\n插件只需关心数据的读取或者写入本身。 而同步的共性问题，比如：类型转换、性能、统计，则交由框架来处理。 作为插件开发人员，则需要关注两个问题：\n数据源本身的读写数据正确性。 如何与框架沟通、合理正确地使用框架。 开工前需要想明白的问题 就插件本身而言，希望在您动手coding之前，能够回答我们列举的这些问题，不然路走远了发现没走对，就尴尬了。\n二、插件视角看框架 逻辑执行模型 插件开发者不用关心太多，基本只需要关注特定系统读和写，以及自己的代码在逻辑上是怎样被执行的，哪一个方法是在什么时候被调用的。在此之前，需要明确以下概念：\nJob: Job是DataX用以描述从一个源头到一个目的端的同步作业，是DataX数据同步的最小业务单元。比如：从一张mysql的表同步到odps的一个表的特定分区。 Task: Task是为最大化而把Job拆分得到的最小执行单元。比如：读一张有1024个分表的mysql分库分表的Job，拆分成1024个读Task，用若干个并发执行。 TaskGroup: 描述的是一组Task集合。在同一个TaskGroupContainer执行下的Task集合称之为TaskGroup JobContainer: Job执行器，负责Job全局拆分、调度、前置语句和后置语句等工作的工作单元。类似Yarn中的JobTracker TaskGroupContainer: TaskGroup执行器，负责执行一组Task的工作单元，类似Yarn中的TaskTracker。 简而言之， Job拆分成Task，在分别在框架提供的容器中执行，插件只需要实现Job和Task两部分逻辑。\n物理执行模型 框架为插件提供物理上的执行能力（线程）。DataX框架有三种运行模式：\nStandalone: 单进程运行，没有外部依赖。 Local: 单进程运行，统计信息、错误信息汇报到集中存储。 Distrubuted: 分布式多进程运行，依赖DataX Service服务。 当然，上述三种模式对插件的编写而言没有什么区别，你只需要避开一些小错误，插件就能够在单机/分布式之间无缝切换了。 当JobContainer和TaskGroupContainer运行在同一个进程内时，就是单机模式（Standalone和Local）；当它们分布在不同的进程中执行时，就是分布式（Distributed）模式。\n是不是很简单？\n编程接口 那么，Job和Task的逻辑应是怎么对应到具体的代码中的？\n首先，插件的入口类必须扩展Reader或Writer抽象类，并且实现分别实现Job和Task两个内部抽象类，Job和Task的实现必须是 内部类 的形式，原因见 加载原理 一节。以Reader为例：\npublic class SomeReader extends Reader { public static class Job extends Reader.Job { @Override public void init() { } @Override public void prepare() { } @Override public List\u0026lt;Configuration\u0026gt; split(int adviceNumber) { return null; } @Override public void post() { } @Override public void destroy() { } } public static class Task extends Reader.Task { @Override public void init() { } @Override public void prepare() { } @Override public void startRead(RecordSender recordSender) { } @Override public void post() { } @Override public void destroy() { } } } Job接口功能如下：\ninit: Job对象初始化工作，此时可以通过super.getPluginJobConf()获取与本插件相关的配置。读插件获得配置中reader部分，写插件获得writer部分。 prepare: 全局准备工作，比如odpswriter清空目标表。 split: 拆分Task。参数adviceNumber框架建议的拆分数，一般是运行时所配置的并发度。值返回的是Task的配置列表。 post: 全局的后置工作，比如mysqlwriter同步完影子表后的rename操作。 destroy: Job对象自身的销毁工作。 Task接口功能如下：\ninit：Task对象的初始化。此时可以通过super.getPluginJobConf()获取与本Task相关的配置。这里的配置是Job的split方法返回的配置列表中的其中一个。 prepare：局部的准备工作。 startRead: 从数据源读数据，写入到RecordSender中。RecordSender会把数据写入连接Reader和Writer的缓存队列。 startWrite：从RecordReceiver中读取数据，写入目标数据源。RecordReceiver中的数据来自Reader和Writer之间的缓存队列。 post: 局部的后置工作。 destroy: Task象自身的销毁工作。 需要注意的是：\nJob和Task之间一定不能有共享变量，因为分布式运行时不能保证共享变量会被正确初始化。两者之间只能通过配置文件进行依赖。 prepare和post在Job和Task中都存在，插件需要根据实际情况确定在什么地方执行操作。 框架按照如下的顺序执行Job和Task的接口：\n上图中，黄色表示Job部分的执行阶段，蓝色表示Task部分的执行阶段，绿色表示框架执行阶段。\n相关类关系如下：\n插件定义 代码写好了，有没有想过框架是怎么找到插件的入口类的？框架是如何加载插件的呢？\n在每个插件的项目中，都有一个plugin.json文件，这个文件定义了插件的相关信息，包括入口类。例如：\n{ \u0026#34;name\u0026#34;: \u0026#34;mysqlwriter\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;com.alibaba.datax.plugin.writer.mysqlwriter.MysqlWriter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Use Jdbc connect to database, execute insert sql.\u0026#34;, \u0026#34;developer\u0026#34;: \u0026#34;alibaba\u0026#34; } name: 插件名称，大小写敏感。框架根据用户在配置文件中指定的名称来搜寻插件。 十分重要 。 class: 入口类的全限定名称，框架通过反射插件入口类的实例。十分重要 。 description: 描述信息。 developer: 开发人员。 打包发布 DataX使用assembly打包，assembly的使用方法请咨询谷哥或者度娘。打包命令如下：\nmvn clean package -DskipTests assembly:assembly DataX插件需要遵循统一的目录结构：\nNULL |-- bin | `-- datax.py |-- conf | |-- core.json | `-- logback.xml |-- lib | `-- datax-core-dependencies.jar `-- plugin |-- reader | `-- mysqlreader | |-- libs | | `-- mysql-reader-plugin-dependencies.jar | |-- mysqlreader-0.0.1-SNAPSHOT.jar | `-- plugin.json `-- writer |-- mysqlwriter | |-- libs | | `-- mysql-writer-plugin-dependencies.jar | |-- mysqlwriter-0.0.1-SNAPSHOT.jar | `-- plugin.json |-- oceanbasewriter `-- odpswriter NULL/bin: 可执行程序目录。 NULL/conf: 框架配置目录。 NULL/lib: 框架依赖库目录。 NULL/plugin: 插件目录。 插件目录分为reader和writer子目录，读写插件分别存放。插件目录规范如下：\nNULL/libs: 插件的依赖库。 NULL/plugin-name-version.jar: 插件本身的jar。 NULL/plugin.json: 插件描述文件。 尽管框架加载插件时，会把NULL下所有的jar放到classpath，但还是推荐依赖库的jar和插件本身的jar分开存放。\n注意： 插件的目录名字必须和plugin.json中定义的插件名称一致。\n配置文件 DataX使用json作为配置文件的格式。一个典型的DataX任务配置如下：\n{ \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;odpsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;accessKey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;accessId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;column\u0026#34;: [\u0026#34;\u0026#34;], \u0026#34;isCompress\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;odpsServer\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;partition\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;project\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tunnelServer\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;oraclewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;column\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;connection\u0026#34;: [ { \u0026#34;jdbcUrl\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;table\u0026#34;: [ \u0026#34;\u0026#34; ] } ] } } } ] } } DataX框架有core.json配置文件，指定了框架的默认行为。任务的配置里头可以指定框架中已经存在的配置项，而且具有更高的优先级，会覆盖core.json中的默认值。\n配置中job.content.reader.parameter的value部分会传给Reader.Job；job.content.writer.parameter的value部分会传给Writer.Job ，Reader.Job和Writer.Job可以通过super.getPluginJobConf()来获取。\nDataX框架支持对特定的配置项进行RSA加密，例子中以*开头的项目便是加密后的值。 配置项加密解密过程对插件是透明，插件仍然以不带*的key来查询配置和操作配置项 。\n如何设计配置参数 配置文件的设计是插件开发的第一步！\n任务配置中reader和writer下parameter部分是插件的配置参数，插件的配置参数应当遵循以下原则：\n驼峰命名：所有配置项采用驼峰命名法，首字母小写，单词首字母大写。\n正交原则：配置项必须正交，功能没有重复，没有潜规则。\n富类型：合理使用json的类型，减少无谓的处理逻辑，减少出错的可能。\n使用正确的数据类型。比如，bool类型的值使用true/false，而非\u0026quot;yes\u0026quot;/\u0026quot;true\u0026quot;/0等。 合理使用集合类型，比如，用数组替代有分隔符的字符串。 类似通用：遵守同一类型的插件的习惯，比如关系型数据库的connection参数都是如下结构：\n{ \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table_1\u0026#34;, \u0026#34;table_2\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:mysql://127.0.0.1:3306/database_1\u0026#34;, \u0026#34;jdbc:mysql://127.0.0.2:3306/database_1_slave\u0026#34; ] }, { \u0026#34;table\u0026#34;: [ \u0026#34;table_3\u0026#34;, \u0026#34;table_4\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:mysql://127.0.0.3:3306/database_2\u0026#34;, \u0026#34;jdbc:mysql://127.0.0.4:3306/database_2_slave\u0026#34; ] } ] } \u0026hellip;\n如何使用Configuration类 为了简化对json的操作，DataX提供了简单的DSL配合Configuration类使用。\nConfiguration提供了常见的get, 带类型get，带默认值get，set等读写配置项的操作，以及clone, toJSON等方法。配置项读写操作都需要传入一个path做为参数，这个path就是DataX定义的DSL。语法有两条：\n子map用.key表示，path的第一个点省略。 数组元素用[index]表示。 比如操作如下json：\n{ \u0026#34;a\u0026#34;: { \u0026#34;b\u0026#34;: { \u0026#34;c\u0026#34;: 2 }, \u0026#34;f\u0026#34;: [ 1, 2, { \u0026#34;g\u0026#34;: true, \u0026#34;h\u0026#34;: false }, 4 ] }, \u0026#34;x\u0026#34;: 4 } 比如调用configuration.get(path)方法，当path为如下值的时候得到的结果为：\nx：4 a.b.c：2 a.b.c.d：null a.b.f[0]：1 a.b.f[2].g：true 注意，因为插件看到的配置只是整个配置的一部分。使用Configuration对象时，需要注意当前的根路径是什么。\n更多Configuration的操作请参考ConfigurationTest.java。\n插件数据传输 跟一般的生产者-消费者模式一样，Reader插件和Writer插件之间也是通过channel来实现数据的传输的。channel可以是内存的，也可能是持久化的，插件不必关心。插件通过RecordSender往channel写入数据，通过RecordReceiver从channel读取数据。\nchannel中的一条数据为一个Record的对象，Record中可以放多个Column对象，这可以简单理解为数据库中的记录和列。\nRecord有如下方法：\npublic interface Record { // 加入一个列，放在最后的位置 void addColumn(Column column); // 在指定下标处放置一个列 void setColumn(int i, final Column column); // 获取一个列 Column getColumn(int i); // 转换为json String String toString(); // 获取总列数 int getColumnNumber(); // 计算整条记录在内存中占用的字节数 int getByteSize(); } 因为Record是一个接口，Reader插件首先调用RecordSender.createRecord()创建一个Record实例，然后把Column一个个添加到Record中。\nWriter插件调用RecordReceiver.getFromReader()方法获取Record，然后把Column遍历出来，写入目标存储中。当Reader尚未退出，传输还在进行时，如果暂时没有数据RecordReceiver.getFromReader()方法会阻塞直到有数据。如果传输已经结束，会返回null，Writer插件可以据此判断是否结束startWrite方法。\nColumn的构造和操作，我们在《类型转换》一节介绍。\n类型转换 为了规范源端和目的端类型转换操作，保证数据不失真，DataX支持六种内部数据类型：\nLong：定点数(Int、Short、Long、BigInteger等)。 Double：浮点数(Float、Double、BigDecimal(无限精度)等)。 String：字符串类型，底层不限长，使用通用字符集(Unicode)。 Date：日期类型。 Bool：布尔值。 Bytes：二进制，可以存放诸如MP3等非结构化数据。 对应地，有DateColumn、LongColumn、DoubleColumn、BytesColumn、StringColumn和BoolColumn六种Column的实现。\nColumn除了提供数据相关的方法外，还提供一系列以as开头的数据类型转换转换方法。\nDataX的内部类型在实现上会选用不同的java类型：\n内部类型 实现类型 备注 Date java.util.Date Long java.math.BigInteger 使用无限精度的大整数，保证不失真 Double java.lang.String 用String表示，保证不失真 Bytes byte[] String java.lang.String Bool java.lang.Boolean 类型之间相互转换的关系如下：\nfrom\\to Date Long Double Bytes String Bool Date - 使用毫秒时间戳 不支持 不支持 使用系统配置的date/time/datetime格式转换 不支持 Long 作为毫秒时间戳构造Date - BigInteger转为BigDecimal，然后BigDecimal.doubleValue() 不支持 BigInteger.toString() 0为false，否则true Double 不支持 内部String构造BigDecimal，然后BigDecimal.longValue() - 不支持 直接返回内部String Bytes 不支持 不支持 不支持 - 按照common.column.encoding配置的编码转换为String，默认utf-8 不支持 String 按照配置的date/time/datetime/extra格式解析 用String构造BigDecimal，然后取longValue() 用String构造BigDecimal，然后取doubleValue(),会正确处理NaN/Infinity/-Infinity 按照common.column.encoding配置的编码转换为byte[]，默认utf-8 - \u0026ldquo;true\u0026quot;为true, \u0026ldquo;false\u0026quot;为false，大小写不敏感。其他字符串不支持 Bool 不支持 true为1L，否则0L true为1.0，否则0.0 不支持 - 脏数据处理 什么是脏数据？ 目前主要有三类脏数据：\nReader读到不支持的类型、不合法的值。 不支持的类型转换，比如：Bytes转换为Date。 写入目标端失败，比如：写mysql整型长度超长。 如何处理脏数据 在Reader.Task和Writer.Task中，通过AbstractTaskPlugin.getTaskPluginCollector()可以拿到一个TaskPluginCollector，它提供了一系列collectDirtyRecord的方法。当脏数据出现时，只需要调用合适的collectDirtyRecord方法，把被认为是脏数据的Record传入即可。\n用户可以在任务的配置中指定脏数据限制条数或者百分比限制，当脏数据超出限制时，框架会结束同步任务，退出。插件需要保证脏数据都被收集到，其他工作交给框架就好。\n加载原理 框架扫描plugin/reader和plugin/writer目录，加载每个插件的plugin.json文件。 以plugin.json文件中name为key，索引所有的插件配置。如果发现重名的插件，框架会异常退出。 用户在插件中在reader/writer配置的name字段指定插件名字。框架根据插件的类型（reader/writer）和插件名称去插件的路径下扫描所有的jar，加入classpath。 根据插件配置中定义的入口类，框架通过反射实例化对应的Job和Task对象。 三、Last but not Least 文档是工程师的良知。\n每个插件都必须在DataX官方wiki中有一篇文档，文档需要包括但不限于以下内容：\n快速介绍：介绍插件的使用场景，特点等。 实现原理：介绍插件实现的底层原理，比如mysqlwriter通过insert into和replace into来实现插入，tair插件通过tair客户端实现写入。 配置说明 给出典型场景下的同步任务的json配置文件。 介绍每个参数的含义、是否必选、默认值、取值范围和其他约束。 类型转换 插件是如何在实际的存储类型和DataX的内部类型之间进行转换的。 以及是否存在特殊处理。 性能报告 软硬件环境，系统版本，java版本，CPU、内存等。 数据特征，记录大小等。 测试参数集（多组），系统参数（比如并发数），插件参数（比如batchSize） 不同参数下同步速度（Rec/s, MB/s），机器负载（load, cpu）等，对数据源压力（load, cpu, mem等）。 约束限制：是否存在其他的使用限制条件。 FAQ：用户经常会遇到的问题。 ","permalink":"http://121.199.2.5:6080/7c5af007072741d2a1b8a4578137e185/","summary":"DataX插件开发宝典 本文面向DataX插件开发人员，尝试尽可能全面地阐述开发一个DataX插件所经过的历程，力求消除开发者的困惑，让插件开发变得简单。\n一、开发之前 路走对了，就不怕远。✓ 路走远了，就不管对不对。✕\n当你打开这篇文档，想必已经不用在此解释什么是DataX了。那下一个问题便是：\nDataX为什么要使用插件机制？ 从设计之初，DataX就把异构数据源同步作为自身的使命，为了应对不同数据源的差异、同时提供一致的同步原语和扩展能力，DataX自然而然地采用了框架 + 插件 的模式：\n插件只需关心数据的读取或者写入本身。 而同步的共性问题，比如：类型转换、性能、统计，则交由框架来处理。 作为插件开发人员，则需要关注两个问题：\n数据源本身的读写数据正确性。 如何与框架沟通、合理正确地使用框架。 开工前需要想明白的问题 就插件本身而言，希望在您动手coding之前，能够回答我们列举的这些问题，不然路走远了发现没走对，就尴尬了。\n二、插件视角看框架 逻辑执行模型 插件开发者不用关心太多，基本只需要关注特定系统读和写，以及自己的代码在逻辑上是怎样被执行的，哪一个方法是在什么时候被调用的。在此之前，需要明确以下概念：\nJob: Job是DataX用以描述从一个源头到一个目的端的同步作业，是DataX数据同步的最小业务单元。比如：从一张mysql的表同步到odps的一个表的特定分区。 Task: Task是为最大化而把Job拆分得到的最小执行单元。比如：读一张有1024个分表的mysql分库分表的Job，拆分成1024个读Task，用若干个并发执行。 TaskGroup: 描述的是一组Task集合。在同一个TaskGroupContainer执行下的Task集合称之为TaskGroup JobContainer: Job执行器，负责Job全局拆分、调度、前置语句和后置语句等工作的工作单元。类似Yarn中的JobTracker TaskGroupContainer: TaskGroup执行器，负责执行一组Task的工作单元，类似Yarn中的TaskTracker。 简而言之， Job拆分成Task，在分别在框架提供的容器中执行，插件只需要实现Job和Task两部分逻辑。\n物理执行模型 框架为插件提供物理上的执行能力（线程）。DataX框架有三种运行模式：\nStandalone: 单进程运行，没有外部依赖。 Local: 单进程运行，统计信息、错误信息汇报到集中存储。 Distrubuted: 分布式多进程运行，依赖DataX Service服务。 当然，上述三种模式对插件的编写而言没有什么区别，你只需要避开一些小错误，插件就能够在单机/分布式之间无缝切换了。 当JobContainer和TaskGroupContainer运行在同一个进程内时，就是单机模式（Standalone和Local）；当它们分布在不同的进程中执行时，就是分布式（Distributed）模式。\n是不是很简单？\n编程接口 那么，Job和Task的逻辑应是怎么对应到具体的代码中的？\n首先，插件的入口类必须扩展Reader或Writer抽象类，并且实现分别实现Job和Task两个内部抽象类，Job和Task的实现必须是 内部类 的形式，原因见 加载原理 一节。以Reader为例：\npublic class SomeReader extends Reader { public static class Job extends Reader.Job { @Override public void init() { } @Override public void prepare() { } @Override public List\u0026lt;Configuration\u0026gt; split(int adviceNumber) { return null; } @Override public void post() { } @Override public void destroy() { } } public static class Task extends Reader.","title":"DataX插件开发宝典"},{"content":"DrdsReader 插件文档 1 快速介绍 DrdsReader插件实现了从DRDS(分布式RDS)读取数据。在底层实现上，DrdsReader通过JDBC连接远程DRDS数据库，并执行相应的sql语句将数据从DRDS库中SELECT出来。\nDRDS的插件目前DataX只适配了Mysql引擎的场景，DRDS对于DataX而言，就是一套分布式Mysql数据库，并且大部分通信协议遵守Mysql使用场景。\n2 实现原理 简而言之，DrdsReader通过JDBC连接器连接到远程的DRDS数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程DRDS数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，DrdsReader将其拼接为SQL语句发送到DRDS数据库。不同于普通的Mysql数据库，DRDS作为分布式数据库系统，无法适配所有Mysql的协议，包括复杂的Join等语句，DRDS暂时无法支持。\n3 功能说明 3.1 配置样例 配置一个从DRDS数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;: 1048576 } //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;drdsReader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;，\u0026#34;name\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:mysql://127.0.0.1:3306/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;drdsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:drds://localhost:3306/database\u0026#34;] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 jdbcUrl\n描述：描述的是到对端数据库的JDBC连接信息，使用JSON的数组描述.注意，jdbcUrl必须包含在connection配置单元中。DRDSReader中关于jdbcUrl中JSON数组填写一个JDBC连接即可。\njdbcUrl按照Mysql官方规范，并可以填写连接附件控制信息。具体请参看mysql官方文档。\n必选：是 默认值：无 username\n描述：数据源的用户名 必选：是 默认值：无 password\n描述：数据源指定用户名的密码 必选：是 默认值：无 table\n描述：所选取需要抽取的表。注意，由于DRDS本身就是分布式数据源，因此填写多张表无意义。系统对多表不做校验。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。用户使用*代表默认使用所有列配置，例如[\u0026rsquo;*\u0026rsquo;]。\n支持列裁剪，即列可以挑选部分列进行导出。\n支持列换序，即列可以不按照表schema信息进行导出。\n支持常量配置，用户需要按照Mysql SQL语法格式: [\u0026ldquo;id\u0026rdquo;, \u0026ldquo;`table`\u0026rdquo;, \u0026ldquo;1\u0026rdquo;, \u0026ldquo;\u0026lsquo;bazhen.csy\u0026rsquo;\u0026rdquo;, \u0026ldquo;null\u0026rdquo;, \u0026ldquo;to_char(a + 1)\u0026rdquo;, \u0026ldquo;2.3\u0026rdquo; , \u0026ldquo;true\u0026rdquo;] id为普通列名，`table`为包含保留在的列名，1为整形数字常量，\u0026lsquo;bazhen.csy\u0026rsquo;为字符串常量，null为空指针，to_char(a + 1)为表达式，2.3为浮点数，true为布尔值。\ncolumn必须用户显示指定同步的列集合，不允许为空！\n必选：是 默认值：无 where\n描述：筛选条件，DrdsReader根据指定的column、table、where条件拼接SQL，并根据这个SQL进行数据抽取。在实际业务场景中，往往会选择当天的数据进行同步，可以将where条件指定为gmt_create \u0026gt; $bizdate 。。\nwhere条件可以有效地进行业务增量同步。where条件不配置或者为空，视作全表同步数据。 必选：否 默认值：无 querySql\n描述：暂时不支持配置querySql模式 3.3 类型转换 目前DrdsReader支持大部分DRDS类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出DrdsReader针对DRDS类型转换列表:\nDataX 内部类型 DRDS 数据类型 Long int, tinyint, smallint, mediumint, int, bigint Double float, double, decimal String varchar, char, tinytext, text, mediumtext, longtext Date date, datetime, timestamp, time, year Boolean bit, bool Bytes tinyblob, mediumblob, blob, longblob, varbinary 请注意:\n除上述罗列字段类型外，其他类型均不支持。 类似Mysql，tinyint(1)视作整形。 类似Mysql，bit类型读取目前是未定义状态。 4 性能报告 4.1 环境准备 4.1.1 数据特征 建表语句：\nCREATE TABLE `tc_biz_vertical_test_0000` ( `biz_order_id` bigint(20) NOT NULL COMMENT 'id', `key_value` varchar(4000) NOT NULL COMMENT 'Key-value的内容', `gmt_create` datetime NOT NULL COMMENT '创建时间', `gmt_modified` datetime NOT NULL COMMENT '修改时间', `attribute_cc` int(11) DEFAULT NULL COMMENT '防止并发修改的标志', `value_type` int(11) NOT NULL DEFAULT '0' COMMENT '类型', `buyer_id` bigint(20) DEFAULT NULL COMMENT 'buyerid', `seller_id` bigint(20) DEFAULT NULL COMMENT 'seller_id', PRIMARY KEY (`biz_order_id`,`value_type`), KEY `idx_biz_vertical_gmtmodified` (`gmt_modified`) ) ENGINE=InnoDB DEFAULT CHARSET=gbk COMMENT='tc_biz_vertical' 单行记录类似于：\nbiz_order_id: 888888888 key_value: ;orderIds:20148888888,2014888888813800; gmt_create: 2011-09-24 11:07:20 gmt_modified: 2011-10-24 17:56:34 attribute_cc: 1 value_type: 3 buyer_id: 8888888 seller_id: 1 4.1.2 机器参数 执行DataX的机器参数为:\ncpu: 24核 Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz mem: 48GB net: 千兆双网卡 disc: DataX 数据不落磁盘，不统计此项 DRDS数据库机器参数为:\ncpu: 32核 Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz mem: 256GB net: 千兆双网卡 disc: BTWL419303E2800RGN INTEL SSDSC2BB800G4 D2010370 4.1.3 DataX jvm 参数 -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError 4.2 测试报告 4.2.1 单表测试报告 通道数 是否按照主键切分 DataX速度(Rec/s) DataX机器运行负载 DB网卡流出流量(MB/s) DB运行负载 说明：\n这里的单表，主键类型为 bigint(20),范围为：190247559466810-570722244711460，从主键范围划分看，数据分布均匀。 对单表如果没有安装主键切分，那么配置通道个数不会提升速度，效果与1个通道一样。 4.2.2 分表测试报告(2个分库，每个分库16张分表，共计32张分表) 通道数 DataX速度(Rec/s) DataX机器运行负载 DB网卡流出流量(MB/s) DB运行负载 5 约束限制 5.1 一致性视图问题 DRDS本身属于分布式数据库，对外无法提供一致性的多库多表视图，不同于Mysql等单库单表同步，DRDSReader无法抽取同一个时间切片的分库分表快照信息，也就是说DataX DrdsReader抽取底层不同的分表将获取不同的分表快照，无法保证强一致性。\n5.2 数据库编码问题 DRDS本身的编码设置非常灵活，包括指定编码到库、表、字段级别，甚至可以均不同编码。优先级从高到低为字段、表、库、实例。我们不推荐数据库用户设置如此混乱的编码，最好在库级别就统一到UTF-8。\nDrdsReader底层使用JDBC进行数据抽取，JDBC天然适配各类编码，并在底层进行了编码转换。因此DrdsReader不需用户指定编码，可以自动获取编码并转码。\n对于DRDS底层写入编码和其设定的编码不一致的混乱情况，DrdsReader对此无法识别，对此也无法提供解决方案，对于这类情况，导出有可能为乱码。\n5.3 增量数据同步 DrdsReader使用JDBC SELECT语句完成数据抽取工作，因此可以使用SELECT\u0026hellip;WHERE\u0026hellip;进行增量数据抽取，方式有多种：\n数据库在线应用写入数据库时，填充modify字段为更改时间戳，包括新增、更新、删除(逻辑删)。对于这类应用，DrdsReader只需要WHERE条件跟上一同步阶段时间戳即可。 对于新增流水型数据，DrdsReader可以WHERE条件后跟上一阶段最大自增ID即可。 对于业务上无字段区分新增、修改数据情况，DrdsReader也无法进行增量数据同步，只能同步全量数据。\n5.4 Sql安全性 DrdsReader提供querySql语句交给用户自己实现SELECT抽取语句，DrdsReader本身对querySql不做任何安全性校验。这块交由DataX用户方自己保证。\n6 FAQ Q: DrdsReader同步报错，报错信息为XXX\nA: 网络或者权限问题，请使用DRDS命令行测试：\nmysql -u -p -h -D -e \u0026ldquo;select * from \u0026lt;表名\u0026gt;\u0026rdquo;\n如果上述命令也报错，那可以证实是环境问题，请联系你的DBA。\nQ: 我想同步DRDS增量数据，怎么配置?\nA: DrdsReader必须业务支持增量字段DataX才能同步增量，例如在淘宝大部分业务表中，通过gmt_modified字段表征这条记录的最新修改时间，那么DataX DrdsReader只需要配置where条件为\n\u0026#34;where\u0026#34;: \u0026#34;Date(add_time) = \u0026#39;2014-06-01\u0026#39;\u0026#34; ","permalink":"http://121.199.2.5:6080/20f3c6090e2448838d8be1baca3a5e1c/","summary":"DrdsReader 插件文档 1 快速介绍 DrdsReader插件实现了从DRDS(分布式RDS)读取数据。在底层实现上，DrdsReader通过JDBC连接远程DRDS数据库，并执行相应的sql语句将数据从DRDS库中SELECT出来。\nDRDS的插件目前DataX只适配了Mysql引擎的场景，DRDS对于DataX而言，就是一套分布式Mysql数据库，并且大部分通信协议遵守Mysql使用场景。\n2 实现原理 简而言之，DrdsReader通过JDBC连接器连接到远程的DRDS数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程DRDS数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，DrdsReader将其拼接为SQL语句发送到DRDS数据库。不同于普通的Mysql数据库，DRDS作为分布式数据库系统，无法适配所有Mysql的协议，包括复杂的Join等语句，DRDS暂时无法支持。\n3 功能说明 3.1 配置样例 配置一个从DRDS数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;: 1048576 } //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;drdsReader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;，\u0026#34;name\u0026#34; ], \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:mysql://127.0.0.1:3306/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;drdsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:drds://localhost:3306/database\u0026#34;] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.","title":"DrdsReader 插件文档"},{"content":"Hbase094XReader \u0026amp; Hbase11XReader 插件文档 1 快速介绍 HbaseReader 插件实现了从 Hbase中读取数据。在底层实现上，HbaseReader 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 Scan 方式读取你指定 rowkey 范围内的数据，并将读取的数据使用 DataX 自定义的数据类型拼装为抽象的数据集，并传递给下游 Writer 处理。\n1.1支持的功能 1、目前HbaseReader支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。\n若您的hbase版本为Hbase0.94.x，reader端的插件请选择：hbase094xreader，即：\n\u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase094xreader\u0026#34; } 若您的hbase版本为Hbase1.1.x，reader端的插件请选择：hbase11xreader,即：\n\u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xreader\u0026#34; } 2、目前HbaseReader支持两模式读取：normal 模式、multiVersionFixedColumn模式；\nnormal 模式：把HBase中的表，当成普通二维表（横表）进行读取,读取最新版本数据。如：\nhbase(main):017:0\u0026gt; scan \u0026lsquo;users\u0026rsquo; ROW COLUMN+CELL lisi column=address:city, timestamp=1457101972764, value=beijing lisi column=address:contry, timestamp=1457102773908, value=china lisi column=address:province, timestamp=1457101972736, value=beijing lisi column=info:age, timestamp=1457101972548, value=27 lisi column=info:birthday, timestamp=1457101972604, value=1987-06-17 lisi column=info:company, timestamp=1457101972653, value=baidu xiaoming column=address:city, timestamp=1457082196082, value=hangzhou xiaoming column=address:contry, timestamp=1457082195729, value=china xiaoming column=address:province, timestamp=1457082195773, value=zhejiang xiaoming column=info:age, timestamp=1457082218735, value=29 xiaoming column=info:birthday, timestamp=1457082186830, value=1987-06-17 xiaoming column=info:company, timestamp=1457082189826, value=alibaba 2 row(s) in 0.0580 seconds\n读取后数据 | rowKey | addres:city | address:contry | address:province | info:age| info:birthday | info:company | | --------| ---------------- |----- |----- |--------| ---------------- |----- | | lisi | beijing| china| beijing |27 | 1987-06-17 | baidu| | xiaoming | hangzhou| china | zhejiang|29 | 1987-06-17 | alibaba| * multiVersionFixedColumn模式：把HBase中的表，当成竖表进行读取。读出的每条记录一定是四列形式，依次为：rowKey，family:qualifier，timestamp，value。读取时需要明确指定要读取的列，把每一个 cell 中的值，作为一条记录（record），若有多个版本就有多条记录（record）。如： ``` hbase(main):018:0\u0026gt; scan \u0026#39;users\u0026#39;,{VERSIONS=\u0026gt;5} ROW COLUMN+CELL lisi column=address:city, timestamp=1457101972764, value=beijing lisi column=address:contry, timestamp=1457102773908, value=china lisi column=address:province, timestamp=1457101972736, value=beijing lisi column=info:age, timestamp=1457101972548, value=27 lisi column=info:birthday, timestamp=1457101972604, value=1987-06-17 lisi column=info:company, timestamp=1457101972653, value=baidu xiaoming column=address:city, timestamp=1457082196082, value=hangzhou xiaoming column=address:contry, timestamp=1457082195729, value=china xiaoming column=address:province, timestamp=1457082195773, value=zhejiang xiaoming column=info:age, timestamp=1457082218735, value=29 xiaoming column=info:age, timestamp=1457082178630, value=24 xiaoming column=info:birthday, timestamp=1457082186830, value=1987-06-17 xiaoming column=info:company, timestamp=1457082189826, value=alibaba 2 row(s) in 0.0260 seconds 读取后数据(4列)\n| rowKey | column:qualifier| timestamp | value | | \u0026mdash;\u0026mdash;\u0026ndash;| \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- |\u0026mdash;\u0026ndash; |\u0026mdash;\u0026ndash; | | lisi | address:city| 1457101972764 | beijing | | lisi | address:contry| 1457102773908 | china | | lisi | address:province| 1457101972736 | beijing | | lisi | info:age| 1457101972548 | 27 | | lisi | info:birthday| 1457101972604 | 1987-06-17 | | lisi | info:company| 1457101972653 | beijing | | xiaoming | address:city| 1457082196082 | hangzhou | | xiaoming | address:contry| 1457082195729 | china | | xiaoming | address:province| 1457082195773 | zhejiang | | xiaoming | info:age| 1457082218735 | 29 | | xiaoming | info:age| 1457082178630 | 24 | | xiaoming | info:birthday| 1457082186830 | 1987-06-17 | | xiaoming | info:company| 1457082189826 | alibaba |\n1.2 限制 1、目前不支持动态列的读取。考虑网络传输流量（支持动态列，需要先将hbase所有列的数据读取出来，再按规则进行过滤），现支持的两种读取模式中需要用户明确指定要读取的列。\n2、关于同步作业的切分：目前的切分方式是根据用户hbase表数据的region分布进行切分。即：在用户填写的［startrowkey，endrowkey］范围内，一个region会切分成一个task，单个region不进行切分。\n3、multiVersionFixedColumn模式下不支持增加常量列\n2 实现原理 简而言之，HbaseReader 通过 HBase 的 Java 客户端，通过 HTable, Scan, ResultScanner 等 API，读取你指定 rowkey 范围内的数据，并将读取的数据使用 DataX 自定义的数据类型拼装为抽象的数据集，并传递给下游 Writer 处理。hbase11xreader与hbase094xreader的主要不同在于API的调用不同，Hbase1.1.x废弃了很多Hbase0.94.x的api。\n3 功能说明 3.1 配置样例 配置一个从 HBase 抽取数据到本地的作业:（normal 模式） { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;xxxf\u0026#34; }, \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;utf-8\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;rowkey\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: birthday\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;yyyy-MM-dd\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: company\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: contry\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: province\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: city\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;range\u0026#34;: { \u0026#34;startRowkey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endRowkey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;isBinaryRowkey\u0026#34;: true } } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xreader/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;qiran\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34; } } } ] } } 配置一个从 HBase 抽取数据到本地的作业:（ multiVersionFixedColumn 模式） { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;xxx\u0026#34; }, \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;utf-8\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;multiVersionFixedColumn\u0026#34;, \u0026#34;maxVersion\u0026#34;: \u0026#34;-1\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;rowkey\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: birthday\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;yyyy-MM-dd\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: company\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: contry\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: province\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: city\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;range\u0026#34;: { \u0026#34;startRowkey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endRowkey\u0026#34;: \u0026#34;\u0026#34; } } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xreader/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;qiran\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34; } } } ] } } 3.2 参数说明 hbaseConfig\n描述：连接HBase集群需要的配置信息，JSON格式。必填的项是hbase.zookeeper.quorum，表示HBase的ZK链接地址。同时可以补充更多HBase client的配置，如：设置scan的cache、batch来优化与服务器的交互。\n必选：是 默认值：无 mode\n描述：读取hbase的模式，支持normal 模式、multiVersionFixedColumn模式，即：normal/multiVersionFixedColumn 必选：是 默认值：无 table\n描述：要读取的 hbase 表名（大小写敏感） 必选：是 默认值：无 encoding\n描述：编码方式，UTF-8 或是 GBK，用于对二进制存储的 HBase byte[] 转为 String 时的编码 必选：否 默认值：UTF-8 column\n描述：要读取的hbase字段，normal 模式与multiVersionFixedColumn 模式下必填项。 (1)、normal 模式下：name指定读取的hbase列，除了rowkey外，必须为 列族:列名 的格式，type指定源数据的类型，format指定日期类型的格式，value指定当前类型为常量，不从hbase读取数据，而是根据value值自动生成对应的列。配置格式如下： \u0026#34;column\u0026#34;: [ { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;rowkey\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; }, { \u0026ldquo;value\u0026rdquo;: \u0026ldquo;test\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; } ]\n``` normal 模式下，对于用户指定Column信息，type必须填写，name/value必须选择其一。 (2)、multiVersionFixedColumn 模式下：name指定读取的hbase列，除了rowkey外，必须为 列族:列名 的格式，type指定源数据的类型，format指定日期类型的格式 。multiVersionFixedColumn模式下不支持常量列。配置格式如下： ``` \u0026quot;column\u0026quot;: [ { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;rowkey\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; }, { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;info: age\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; } ] ```\n* 必选：是\u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; maxVersion\n描述：指定在多版本模式下的hbasereader读取的版本数，取值只能为－1或者大于1的数字，－1表示读取所有版本 必选：multiVersionFixedColumn 模式下必填项\n默认值：无\nrange\n描述：指定hbasereader读取的rowkey范围。\nstartRowkey：指定开始rowkey；\nendRowkey指定结束rowkey；\nisBinaryRowkey：指定配置的startRowkey和endRowkey转换为byte[]时的方式，默认值为false,若为true，则调用Bytes.toBytesBinary(rowkey)方法进行转换;若为false：则调用Bytes.toBytes(rowkey)\n配置格式如下： \u0026#34;range\u0026#34;: { \u0026#34;startRowkey\u0026#34;: \u0026#34;aaa\u0026#34;, \u0026#34;endRowkey\u0026#34;: \u0026#34;ccc\u0026#34;, \u0026#34;isBinaryRowkey\u0026#34;:false } ``` * 必选：否 \u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; scanCacheSize\n描述：Hbase client每次rpc从服务器端读取的行数 必选：否\n默认值：256\nscanBatchSize\n描述：Hbase client每次rpc从服务器端读取的列数 必选：否\n默认值：100\n3.3 类型转换 下面列出支持的读取HBase数据类型，HbaseReader 针对 HBase 类型转换列表:\nDataX 内部类型 HBase 数据类型 Long int, short ,long Double float, double String string,binarystring Date date Boolean boolean 请注意:\n除上述罗列字段类型外，其他类型均不支持。 4 性能报告 略\n5 约束限制 略\n6 FAQ ","permalink":"http://121.199.2.5:6080/34ec6d7caba24a86b7a5e9a2f37d8c46/","summary":"Hbase094XReader \u0026amp; Hbase11XReader 插件文档 1 快速介绍 HbaseReader 插件实现了从 Hbase中读取数据。在底层实现上，HbaseReader 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 Scan 方式读取你指定 rowkey 范围内的数据，并将读取的数据使用 DataX 自定义的数据类型拼装为抽象的数据集，并传递给下游 Writer 处理。\n1.1支持的功能 1、目前HbaseReader支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。\n若您的hbase版本为Hbase0.94.x，reader端的插件请选择：hbase094xreader，即：\n\u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase094xreader\u0026#34; } 若您的hbase版本为Hbase1.1.x，reader端的插件请选择：hbase11xreader,即：\n\u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xreader\u0026#34; } 2、目前HbaseReader支持两模式读取：normal 模式、multiVersionFixedColumn模式；\nnormal 模式：把HBase中的表，当成普通二维表（横表）进行读取,读取最新版本数据。如：\nhbase(main):017:0\u0026gt; scan \u0026lsquo;users\u0026rsquo; ROW COLUMN+CELL lisi column=address:city, timestamp=1457101972764, value=beijing lisi column=address:contry, timestamp=1457102773908, value=china lisi column=address:province, timestamp=1457101972736, value=beijing lisi column=info:age, timestamp=1457101972548, value=27 lisi column=info:birthday, timestamp=1457101972604, value=1987-06-17 lisi column=info:company, timestamp=1457101972653, value=baidu xiaoming column=address:city, timestamp=1457082196082, value=hangzhou xiaoming column=address:contry, timestamp=1457082195729, value=china xiaoming column=address:province, timestamp=1457082195773, value=zhejiang xiaoming column=info:age, timestamp=1457082218735, value=29 xiaoming column=info:birthday, timestamp=1457082186830, value=1987-06-17 xiaoming column=info:company, timestamp=1457082189826, value=alibaba 2 row(s) in 0.0580 seconds","title":"Hbase094XReader \u0026 Hbase11XReader 插件文档"},{"content":"Hbase094XReader \u0026amp; Hbase11XReader 插件文档 1 快速介绍 HbaseReader 插件实现了从 Hbase中读取数据。在底层实现上，HbaseReader 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 Scan 方式读取你指定 rowkey 范围内的数据，并将读取的数据使用 DataX 自定义的数据类型拼装为抽象的数据集，并传递给下游 Writer 处理。\n1.1支持的功能 1、目前HbaseReader支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。\n若您的hbase版本为Hbase0.94.x，reader端的插件请选择：hbase094xreader，即：\n\u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase094xreader\u0026#34; } 若您的hbase版本为Hbase1.1.x，reader端的插件请选择：hbase11xreader,即：\n\u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xreader\u0026#34; } 2、目前HbaseReader支持两模式读取：normal 模式、multiVersionFixedColumn模式；\nnormal 模式：把HBase中的表，当成普通二维表（横表）进行读取,读取最新版本数据。如：\nhbase(main):017:0\u0026gt; scan \u0026lsquo;users\u0026rsquo; ROW COLUMN+CELL lisi column=address:city, timestamp=1457101972764, value=beijing lisi column=address:contry, timestamp=1457102773908, value=china lisi column=address:province, timestamp=1457101972736, value=beijing lisi column=info:age, timestamp=1457101972548, value=27 lisi column=info:birthday, timestamp=1457101972604, value=1987-06-17 lisi column=info:company, timestamp=1457101972653, value=baidu xiaoming column=address:city, timestamp=1457082196082, value=hangzhou xiaoming column=address:contry, timestamp=1457082195729, value=china xiaoming column=address:province, timestamp=1457082195773, value=zhejiang xiaoming column=info:age, timestamp=1457082218735, value=29 xiaoming column=info:birthday, timestamp=1457082186830, value=1987-06-17 xiaoming column=info:company, timestamp=1457082189826, value=alibaba 2 row(s) in 0.0580 seconds\n读取后数据 | rowKey | addres:city | address:contry | address:province | info:age| info:birthday | info:company | | --------| ---------------- |----- |----- |--------| ---------------- |----- | | lisi | beijing| china| beijing |27 | 1987-06-17 | baidu| | xiaoming | hangzhou| china | zhejiang|29 | 1987-06-17 | alibaba| * multiVersionFixedColumn模式：把HBase中的表，当成竖表进行读取。读出的每条记录一定是四列形式，依次为：rowKey，family:qualifier，timestamp，value。读取时需要明确指定要读取的列，把每一个 cell 中的值，作为一条记录（record），若有多个版本就有多条记录（record）。如： ``` hbase(main):018:0\u0026gt; scan \u0026#39;users\u0026#39;,{VERSIONS=\u0026gt;5} ROW COLUMN+CELL lisi column=address:city, timestamp=1457101972764, value=beijing lisi column=address:contry, timestamp=1457102773908, value=china lisi column=address:province, timestamp=1457101972736, value=beijing lisi column=info:age, timestamp=1457101972548, value=27 lisi column=info:birthday, timestamp=1457101972604, value=1987-06-17 lisi column=info:company, timestamp=1457101972653, value=baidu xiaoming column=address:city, timestamp=1457082196082, value=hangzhou xiaoming column=address:contry, timestamp=1457082195729, value=china xiaoming column=address:province, timestamp=1457082195773, value=zhejiang xiaoming column=info:age, timestamp=1457082218735, value=29 xiaoming column=info:age, timestamp=1457082178630, value=24 xiaoming column=info:birthday, timestamp=1457082186830, value=1987-06-17 xiaoming column=info:company, timestamp=1457082189826, value=alibaba 2 row(s) in 0.0260 seconds 读取后数据(4列)\n| rowKey | column:qualifier| timestamp | value | | \u0026mdash;\u0026mdash;\u0026ndash;| \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- |\u0026mdash;\u0026ndash; |\u0026mdash;\u0026ndash; | | lisi | address:city| 1457101972764 | beijing | | lisi | address:contry| 1457102773908 | china | | lisi | address:province| 1457101972736 | beijing | | lisi | info:age| 1457101972548 | 27 | | lisi | info:birthday| 1457101972604 | 1987-06-17 | | lisi | info:company| 1457101972653 | beijing | | xiaoming | address:city| 1457082196082 | hangzhou | | xiaoming | address:contry| 1457082195729 | china | | xiaoming | address:province| 1457082195773 | zhejiang | | xiaoming | info:age| 1457082218735 | 29 | | xiaoming | info:age| 1457082178630 | 24 | | xiaoming | info:birthday| 1457082186830 | 1987-06-17 | | xiaoming | info:company| 1457082189826 | alibaba |\n3、HbaseReader中有一个必填配置项是：hbaseConfig，需要你联系 HBase PE，将hbase-site.xml 中与连接 HBase 相关的配置项提取出来，以 json 格式填入，同时可以补充更多HBase client的配置，如：设置scan的cache（hbase.client.scanner.caching）、batch来优化与服务器的交互。\n如：hbase-site.xml的配置内容如下\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://ip:9000/hbase\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.cluster.distributed\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;***\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 转换后的json为：\n\u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.rootdir\u0026#34;: \u0026#34;hdfs: //ip:9000/hbase\u0026#34;, \u0026#34;hbase.cluster.distributed\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;***\u0026#34; } 1.2 限制 1、目前不支持动态列的读取。考虑网络传输流量（支持动态列，需要先将hbase所有列的数据读取出来，再按规则进行过滤），现支持的两种读取模式中需要用户明确指定要读取的列。\n2、关于同步作业的切分：目前的切分方式是根据用户hbase表数据的region分布进行切分。即：在用户填写的［startrowkey，endrowkey］范围内，一个region会切分成一个task，单个region不进行切分。\n3、multiVersionFixedColumn模式下不支持增加常量列\n2 实现原理 简而言之，HbaseReader 通过 HBase 的 Java 客户端，通过 HTable, Scan, ResultScanner 等 API，读取你指定 rowkey 范围内的数据，并将读取的数据使用 DataX 自定义的数据类型拼装为抽象的数据集，并传递给下游 Writer 处理。hbase11xreader与hbase094xreader的主要不同在于API的调用不同，Hbase1.1.x废弃了很多Hbase0.94.x的api。\n3 功能说明 3.1 配置样例 配置一个从 HBase 抽取数据到本地的作业:（normal 模式） { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.rootdir\u0026#34;: \u0026#34;hdfs: //xxx: 9000/hbase\u0026#34;, \u0026#34;hbase.cluster.distributed\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;xxx\u0026#34; }, \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;utf-8\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;rowkey\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: birthday\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;yyyy-MM-dd\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: company\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: contry\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: province\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: city\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;range\u0026#34;: { \u0026#34;startRowkey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endRowkey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;isBinaryRowkey\u0026#34;: true } } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xreader/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;qiran\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34; } } } ] } } 配置一个从 HBase 抽取数据到本地的作业:（ multiVersionFixedColumn 模式） { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.rootdir\u0026#34;: \u0026#34;hdfs: //xxx: 9000/hbase\u0026#34;, \u0026#34;hbase.cluster.distributed\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;xxx\u0026#34; }, \u0026#34;table\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;utf-8\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;multiVersionFixedColumn\u0026#34;, \u0026#34;maxVersion\u0026#34;: \u0026#34;-1\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;rowkey\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: age\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: birthday\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;:\u0026#34;yyyy-MM-dd\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;info: company\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: contry\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: province\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;address: city\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;range\u0026#34;: { \u0026#34;startRowkey\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endRowkey\u0026#34;: \u0026#34;\u0026#34; } } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilewriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xreader/result\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;qiran\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;truncate\u0026#34; } } } ] } } 3.2 参数说明 hbaseConfig\n描述：每个HBase集群提供给DataX客户端连接的配置信息存放在hbase-site.xml，请联系你的HBase PE提供配置信息，并转换为JSON格式。同时可以补充更多HBase client的配置，如：设置scan的cache、batch来优化与服务器的交互。\n必选：是 默认值：无 mode\n描述：读取hbase的模式，支持normal 模式、multiVersionFixedColumn模式，即：normal/multiVersionFixedColumn 必选：是 默认值：无 table\n描述：要读取的 hbase 表名（大小写敏感） 必选：是 默认值：无 encoding\n描述：编码方式，UTF-8 或是 GBK，用于对二进制存储的 HBase byte[] 转为 String 时的编码 必选：否 默认值：UTF-8 column\n描述：要读取的hbase字段，normal 模式与multiVersionFixedColumn 模式下必填项。 (1)、normal 模式下：name指定读取的hbase列，除了rowkey外，必须为 列族:列名 的格式，type指定源数据的类型，format指定日期类型的格式，value指定当前类型为常量，不从hbase读取数据，而是根据value值自动生成对应的列。配置格式如下： \u0026#34;column\u0026#34;: [ { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;rowkey\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; }, { \u0026ldquo;value\u0026rdquo;: \u0026ldquo;test\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; } ]\n``` normal 模式下，对于用户指定Column信息，type必须填写，name/value必须选择其一。 (2)、multiVersionFixedColumn 模式下：name指定读取的hbase列，除了rowkey外，必须为 列族:列名 的格式，type指定源数据的类型，format指定日期类型的格式 。multiVersionFixedColumn模式下不支持常量列。配置格式如下： ``` \u0026quot;column\u0026quot;: [ { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;rowkey\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; }, { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;info: age\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; } ] ```\n* 必选：是\u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; maxVersion\n描述：指定在多版本模式下的hbasereader读取的版本数，取值只能为－1或者大于1的数字，－1表示读取所有版本 必选：multiVersionFixedColumn 模式下必填项\n默认值：无\nrange\n描述：指定hbasereader读取的rowkey范围。\nstartRowkey：指定开始rowkey；\nendRowkey指定结束rowkey；\nisBinaryRowkey：指定配置的startRowkey和endRowkey转换为byte[]时的方式，默认值为false,若为true，则调用Bytes.toBytesBinary(rowkey)方法进行转换;若为false：则调用Bytes.toBytes(rowkey)\n配置格式如下： \u0026#34;range\u0026#34;: { \u0026#34;startRowkey\u0026#34;: \u0026#34;aaa\u0026#34;, \u0026#34;endRowkey\u0026#34;: \u0026#34;ccc\u0026#34;, \u0026#34;isBinaryRowkey\u0026#34;:false } ``` * 必选：否 \u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; scanCacheSize\n描述：Hbase client每次rpc从服务器端读取的行数 必选：否\n默认值：256\nscanBatchSize\n描述：Hbase client每次rpc从服务器端读取的列数 必选：否\n默认值：100\n3.3 类型转换 下面列出支持的读取HBase数据类型，HbaseReader 针对 HBase 类型转换列表:\nDataX 内部类型 HBase 数据类型 Long int, short ,long Double float, double String string,binarystring Date date Boolean boolean 请注意:\n除上述罗列字段类型外，其他类型均不支持。 4 性能报告 略\n5 约束限制 略\n6 FAQ ","permalink":"http://121.199.2.5:6080/7c5fc71d1a124a5396b1a17e8293be71/","summary":"Hbase094XReader \u0026amp; Hbase11XReader 插件文档 1 快速介绍 HbaseReader 插件实现了从 Hbase中读取数据。在底层实现上，HbaseReader 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 Scan 方式读取你指定 rowkey 范围内的数据，并将读取的数据使用 DataX 自定义的数据类型拼装为抽象的数据集，并传递给下游 Writer 处理。\n1.1支持的功能 1、目前HbaseReader支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。\n若您的hbase版本为Hbase0.94.x，reader端的插件请选择：hbase094xreader，即：\n\u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase094xreader\u0026#34; } 若您的hbase版本为Hbase1.1.x，reader端的插件请选择：hbase11xreader,即：\n\u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xreader\u0026#34; } 2、目前HbaseReader支持两模式读取：normal 模式、multiVersionFixedColumn模式；\nnormal 模式：把HBase中的表，当成普通二维表（横表）进行读取,读取最新版本数据。如：\nhbase(main):017:0\u0026gt; scan \u0026lsquo;users\u0026rsquo; ROW COLUMN+CELL lisi column=address:city, timestamp=1457101972764, value=beijing lisi column=address:contry, timestamp=1457102773908, value=china lisi column=address:province, timestamp=1457101972736, value=beijing lisi column=info:age, timestamp=1457101972548, value=27 lisi column=info:birthday, timestamp=1457101972604, value=1987-06-17 lisi column=info:company, timestamp=1457101972653, value=baidu xiaoming column=address:city, timestamp=1457082196082, value=hangzhou xiaoming column=address:contry, timestamp=1457082195729, value=china xiaoming column=address:province, timestamp=1457082195773, value=zhejiang xiaoming column=info:age, timestamp=1457082218735, value=29 xiaoming column=info:birthday, timestamp=1457082186830, value=1987-06-17 xiaoming column=info:company, timestamp=1457082189826, value=alibaba 2 row(s) in 0.0580 seconds","title":"Hbase094XReader \u0026 Hbase11XReader 插件文档"},{"content":"Hbase094XWriter \u0026amp; Hbase11XWriter 插件文档 1 快速介绍 HbaseWriter 插件实现了从向Hbase中写取数据。在底层实现上，HbaseWriter 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 put 方式写入Hbase。\n1.1支持功能 1、目前HbaseWriter支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。\n若您的hbase版本为Hbase0.94.x，writer端的插件请选择：hbase094xwriter，即：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase094xwriter\u0026#34; } 若您的hbase版本为Hbase1.1.x，writer端的插件请选择：hbase11xwriter,即：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xwriter\u0026#34; } 2、目前HbaseWriter支持源端多个字段拼接作为hbase 表的 rowkey，具体配置参考：rowkeyColumn配置；\n3、写入hbase的时间戳（版本）支持：用当前时间作为版本，指定源端列作为版本，指定一个时间 三种方式作为版本；\n4、HbaseWriter中有一个必填配置项是：hbaseConfig，需要你联系 HBase PE，将hbase-site.xml 中与连接 HBase 相关的配置项提取出来，以 json 格式填入，同时可以补充更多HBase client的配置来优化与服务器的交互。\n如：hbase-site.xml的配置内容如下\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://ip:9000/hbase\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.cluster.distributed\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;***\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 转换后的json为：\n\u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.rootdir\u0026#34;: \u0026#34;hdfs: //ip: 9000/hbase\u0026#34;, \u0026#34;hbase.cluster.distributed\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;***\u0026#34; } 1.2 限制 1、目前只支持源端为横表写入，不支持竖表（源端读出的为四元组: rowKey，family:qualifier，timestamp，value）模式的数据写入；本期目标主要是替换DataX2中的habsewriter，下次迭代考虑支持。\n2、目前不支持写入hbase前清空表数据，若需要清空数据请联系HBase PE\n2 实现原理 简而言之，HbaseWriter 通过 HBase 的 Java 客户端，通过 HTable, Put等 API，将从上游Reader读取的数据写入HBase你hbase11xwriter与hbase094xwriter的主要不同在于API的调用不同，Hbase1.1.x废弃了很多Hbase0.94.x的api。\n3 功能说明 3.1 配置样例 配置一个从本地写入hbase1.1.x的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xwriter/txt/normal.txt\u0026#34;, \u0026#34;charset\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.rootdir\u0026#34;: \u0026#34;hdfs: //ip: 9000/hbase\u0026#34;, \u0026#34;hbase.cluster.distributed\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;***\u0026#34; }, \u0026#34;table\u0026#34;: \u0026#34;writer\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;rowkeyColumn\u0026#34;: [ { \u0026#34;index\u0026#34;:0, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:-1, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;_\u0026#34; } ], \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;:1, \u0026#34;name\u0026#34;: \u0026#34;cf1:q1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:2, \u0026#34;name\u0026#34;: \u0026#34;cf1:q2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:3, \u0026#34;name\u0026#34;: \u0026#34;cf1:q3\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:4, \u0026#34;name\u0026#34;: \u0026#34;cf2:q1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:5, \u0026#34;name\u0026#34;: \u0026#34;cf2:q2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:6, \u0026#34;name\u0026#34;: \u0026#34;cf2:q3\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;versionColumn\u0026#34;:{ \u0026#34;index\u0026#34;: -1, \u0026#34;value\u0026#34;:\u0026#34;123456789\u0026#34; }, \u0026#34;encoding\u0026#34;: \u0026#34;utf-8\u0026#34; } } } ] } } 3.2 参数说明 hbaseConfig\n描述：每个HBase集群提供给DataX客户端连接的配置信息存放在hbase-site.xml，请联系你的HBase PE提供配置信息，并转换为JSON格式。同时可以补充更多HBase client的配置，如：设置scan的cache、batch来优化与服务器的交互。\n必选：是 默认值：无 mode\n描述：写hbase的模式，目前只支持normal 模式，后续考虑动态列模式\n必选：是 默认值：无 table\n描述：要写的 hbase 表名（大小写敏感） 必选：是 默认值：无 encoding\n描述：编码方式，UTF-8 或是 GBK，用于 String 转 HBase byte[]时的编码 必选：否 默认值：UTF-8 column\n描述：要写入的hbase字段。index：指定该列对应reader端column的索引，从0开始；name：指定hbase表中的列，必须为 列族:列名 的格式；type：指定写入数据类型，用于转换HBase byte[]。配置格式如下： \u0026ldquo;column\u0026rdquo;: [ { \u0026ldquo;index\u0026rdquo;:1, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;cf1:q1\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; }, { \u0026ldquo;index\u0026rdquo;:2, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;cf1:q2\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; } ］\n``` * 必选：是\u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; rowkeyColumn\n描述：要写入的hbase的rowkey列。index：指定该列对应reader端column的索引，从0开始，若为常量index为－1；type：指定写入数据类型，用于转换HBase byte[]；value：配置常量，常作为多个字段的拼接符。hbasewriter会将rowkeyColumn中所有列按照配置顺序进行拼接作为写入hbase的rowkey，不能全为常量。配置格式如下： \u0026ldquo;rowkeyColumn\u0026rdquo;: [ { \u0026ldquo;index\u0026rdquo;:0, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;string\u0026rdquo; }, { \u0026ldquo;index\u0026rdquo;:-1, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;string\u0026rdquo;, \u0026ldquo;value\u0026rdquo;:\u0026quot;_\u0026quot; } ]\n``` * 必选：是\u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; versionColumn\n描述：指定写入hbase的时间戳。支持：当前时间、指定时间列，指定时间，三者选一。若不配置表示用当前时间。index：指定对应reader端column的索引，从0开始，需保证能转换为long,若是Date类型，会尝试用yyyy-MM-dd HH:mm:ss和yyyy-MM-dd HH:mm:ss SSS去解析；若为指定时间index为－1；value：指定时间的值,long值。配置格式如下： \u0026ldquo;versionColumn\u0026rdquo;:{ \u0026ldquo;index\u0026rdquo;:1 }\n``` 或者 ``` \u0026ldquo;versionColumn\u0026rdquo;:{ \u0026ldquo;index\u0026rdquo;:－1, \u0026ldquo;value\u0026rdquo;:123456789 }\n``` * 必选：否\u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; nullMode\n描述：读取的null值时，如何处理。支持两种方式：（1）skip：表示不向hbase写这列；（2）empty：写入HConstants.EMPTY_BYTE_ARRAY，即new byte [0] 必选：否\n默认值：skip\nwalFlag\n描述：在HBae client向集群中的RegionServer提交数据时（Put/Delete操作），首先会先写WAL（Write Ahead Log）日志（即HLog，一个RegionServer上的所有Region共享一个HLog），只有当WAL日志写成功后，再接着写MemStore，然后客户端被通知提交数据成功；如果写WAL日志失败，客户端则被通知提交失败。关闭（false）放弃写WAL日志，从而提高数据写入的性能。\n必选：否\n默认值：false\nwriteBufferSize\n描述：设置HBae client的写buffer大小，单位字节。配合autoflush使用。autoflush，开启（true）表示Hbase client在写的时候有一条put就执行一次更新；关闭（false），表示Hbase client在写的时候只有当put填满客户端写缓存时，才实际向HBase服务端发起写请求\n必选：否\n默认值：8M\n3.3 HBase支持的列类型 BOOLEAN SHORT INT LONG FLOAT DOUBLE STRING 请注意:\n除上述罗列字段类型外，其他类型均不支持。 4 性能报告 略\n5 约束限制 略\n6 FAQ ","permalink":"http://121.199.2.5:6080/0172fbcbd4674f98a8782403fc6a7ca5/","summary":"Hbase094XWriter \u0026amp; Hbase11XWriter 插件文档 1 快速介绍 HbaseWriter 插件实现了从向Hbase中写取数据。在底层实现上，HbaseWriter 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 put 方式写入Hbase。\n1.1支持功能 1、目前HbaseWriter支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。\n若您的hbase版本为Hbase0.94.x，writer端的插件请选择：hbase094xwriter，即：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase094xwriter\u0026#34; } 若您的hbase版本为Hbase1.1.x，writer端的插件请选择：hbase11xwriter,即：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xwriter\u0026#34; } 2、目前HbaseWriter支持源端多个字段拼接作为hbase 表的 rowkey，具体配置参考：rowkeyColumn配置；\n3、写入hbase的时间戳（版本）支持：用当前时间作为版本，指定源端列作为版本，指定一个时间 三种方式作为版本；\n4、HbaseWriter中有一个必填配置项是：hbaseConfig，需要你联系 HBase PE，将hbase-site.xml 中与连接 HBase 相关的配置项提取出来，以 json 格式填入，同时可以补充更多HBase client的配置来优化与服务器的交互。\n如：hbase-site.xml的配置内容如下\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://ip:9000/hbase\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.cluster.distributed\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;***\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 转换后的json为：\n\u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.rootdir\u0026#34;: \u0026#34;hdfs: //ip: 9000/hbase\u0026#34;, \u0026#34;hbase.cluster.distributed\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;***\u0026#34; } 1.2 限制 1、目前只支持源端为横表写入，不支持竖表（源端读出的为四元组: rowKey，family:qualifier，timestamp，value）模式的数据写入；本期目标主要是替换DataX2中的habsewriter，下次迭代考虑支持。\n2、目前不支持写入hbase前清空表数据，若需要清空数据请联系HBase PE\n2 实现原理 简而言之，HbaseWriter 通过 HBase 的 Java 客户端，通过 HTable, Put等 API，将从上游Reader读取的数据写入HBase你hbase11xwriter与hbase094xwriter的主要不同在于API的调用不同，Hbase1.1.x废弃了很多Hbase0.94.x的api。\n3 功能说明 3.1 配置样例 配置一个从本地写入hbase1.1.x的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xwriter/txt/normal.","title":"Hbase094XWriter \u0026 Hbase11XWriter 插件文档"},{"content":"Hbase094XWriter \u0026amp; Hbase11XWriter 插件文档 1 快速介绍 HbaseWriter 插件实现了从向Hbase中写取数据。在底层实现上，HbaseWriter 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 put 方式写入Hbase。\n1.1支持功能 1、目前HbaseWriter支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。\n若您的hbase版本为Hbase0.94.x，writer端的插件请选择：hbase094xwriter，即：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase094xwriter\u0026#34; } 若您的hbase版本为Hbase1.1.x，writer端的插件请选择：hbase11xwriter,即：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xwriter\u0026#34; } 2、目前HbaseWriter支持源端多个字段拼接作为hbase 表的 rowkey，具体配置参考：rowkeyColumn配置；\n3、写入hbase的时间戳（版本）支持：用当前时间作为版本，指定源端列作为版本，指定一个时间 三种方式作为版本；\n1.2 限制 1、目前只支持源端为横表写入，不支持竖表（源端读出的为四元组: rowKey，family:qualifier，timestamp，value）模式的数据写入；本期目标主要是替换DataX2中的habsewriter，下次迭代考虑支持。\n2、目前不支持写入hbase前清空表数据，若需要清空数据请联系HBase PE\n2 实现原理 简而言之，HbaseWriter 通过 HBase 的 Java 客户端，通过 HTable, Put等 API，将从上游Reader读取的数据写入HBase你hbase11xwriter与hbase094xwriter的主要不同在于API的调用不同，Hbase1.1.x废弃了很多Hbase0.94.x的api。\n3 功能说明 3.1 配置样例 配置一个从本地写入hbase1.1.x的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xwriter/txt/normal.txt\u0026#34;, \u0026#34;charset\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;***\u0026#34; }, \u0026#34;table\u0026#34;: \u0026#34;writer\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;rowkeyColumn\u0026#34;: [ { \u0026#34;index\u0026#34;:0, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:-1, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;_\u0026#34; } ], \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;:1, \u0026#34;name\u0026#34;: \u0026#34;cf1:q1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:2, \u0026#34;name\u0026#34;: \u0026#34;cf1:q2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:3, \u0026#34;name\u0026#34;: \u0026#34;cf1:q3\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:4, \u0026#34;name\u0026#34;: \u0026#34;cf2:q1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:5, \u0026#34;name\u0026#34;: \u0026#34;cf2:q2\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;:6, \u0026#34;name\u0026#34;: \u0026#34;cf2:q3\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;versionColumn\u0026#34;:{ \u0026#34;index\u0026#34;: -1, \u0026#34;value\u0026#34;:\u0026#34;123456789\u0026#34; }, \u0026#34;encoding\u0026#34;: \u0026#34;utf-8\u0026#34; } } } ] } } 3.2 参数说明 hbaseConfig\n描述：连接HBase集群需要的配置信息，JSON格式。必填的项是hbase.zookeeper.quorum，表示HBase的ZK链接地址。同时可以补充更多HBase client的配置，如：设置scan的cache、batch来优化与服务器的交互。\n必选：是 默认值：无 mode\n描述：写hbase的模式，目前只支持normal 模式，后续考虑动态列模式\n必选：是 默认值：无 table\n描述：要写的 hbase 表名（大小写敏感） 必选：是 默认值：无 encoding\n描述：编码方式，UTF-8 或是 GBK，用于 String 转 HBase byte[]时的编码 必选：否 默认值：UTF-8 column\n描述：要写入的hbase字段。index：指定该列对应reader端column的索引，从0开始；name：指定hbase表中的列，必须为 列族:列名 的格式；type：指定写入数据类型，用于转换HBase byte[]。配置格式如下： \u0026ldquo;column\u0026rdquo;: [ { \u0026ldquo;index\u0026rdquo;:1, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;cf1:q1\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; }, { \u0026ldquo;index\u0026rdquo;:2, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;cf1:q2\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; } ］\n``` * 必选：是\u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; rowkeyColumn\n描述：要写入的hbase的rowkey列。index：指定该列对应reader端column的索引，从0开始，若为常量index为－1；type：指定写入数据类型，用于转换HBase byte[]；value：配置常量，常作为多个字段的拼接符。hbasewriter会将rowkeyColumn中所有列按照配置顺序进行拼接作为写入hbase的rowkey，不能全为常量。配置格式如下： \u0026ldquo;rowkeyColumn\u0026rdquo;: [ { \u0026ldquo;index\u0026rdquo;:0, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;string\u0026rdquo; }, { \u0026ldquo;index\u0026rdquo;:-1, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;string\u0026rdquo;, \u0026ldquo;value\u0026rdquo;:\u0026quot;_\u0026quot; } ]\n``` * 必选：是\u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; versionColumn\n描述：指定写入hbase的时间戳。支持：当前时间、指定时间列，指定时间，三者选一。若不配置表示用当前时间。index：指定对应reader端column的索引，从0开始，需保证能转换为long,若是Date类型，会尝试用yyyy-MM-dd HH:mm:ss和yyyy-MM-dd HH:mm:ss SSS去解析；若为指定时间index为－1；value：指定时间的值,long值。配置格式如下： \u0026ldquo;versionColumn\u0026rdquo;:{ \u0026ldquo;index\u0026rdquo;:1 }\n``` 或者 ``` \u0026ldquo;versionColumn\u0026rdquo;:{ \u0026ldquo;index\u0026rdquo;:－1, \u0026ldquo;value\u0026rdquo;:123456789 }\n``` * 必选：否\u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; nullMode\n描述：读取的null值时，如何处理。支持两种方式：（1）skip：表示不向hbase写这列；（2）empty：写入HConstants.EMPTY_BYTE_ARRAY，即new byte [0] 必选：否\n默认值：skip\nwalFlag\n描述：在HBae client向集群中的RegionServer提交数据时（Put/Delete操作），首先会先写WAL（Write Ahead Log）日志（即HLog，一个RegionServer上的所有Region共享一个HLog），只有当WAL日志写成功后，再接着写MemStore，然后客户端被通知提交数据成功；如果写WAL日志失败，客户端则被通知提交失败。关闭（false）放弃写WAL日志，从而提高数据写入的性能。\n必选：否\n默认值：false\nwriteBufferSize\n描述：设置HBae client的写buffer大小，单位字节。配合autoflush使用。autoflush，开启（true）表示Hbase client在写的时候有一条put就执行一次更新；关闭（false），表示Hbase client在写的时候只有当put填满客户端写缓存时，才实际向HBase服务端发起写请求\n必选：否\n默认值：8M\n3.3 HBase支持的列类型 BOOLEAN SHORT INT LONG FLOAT DOUBLE STRING 请注意:\n除上述罗列字段类型外，其他类型均不支持。 4 性能报告 略\n5 约束限制 略\n6 FAQ ","permalink":"http://121.199.2.5:6080/745df21fdbd349ab9a9fc5c94551b336/","summary":"Hbase094XWriter \u0026amp; Hbase11XWriter 插件文档 1 快速介绍 HbaseWriter 插件实现了从向Hbase中写取数据。在底层实现上，HbaseWriter 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 put 方式写入Hbase。\n1.1支持功能 1、目前HbaseWriter支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。\n若您的hbase版本为Hbase0.94.x，writer端的插件请选择：hbase094xwriter，即：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase094xwriter\u0026#34; } 若您的hbase版本为Hbase1.1.x，writer端的插件请选择：hbase11xwriter,即：\n\u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xwriter\u0026#34; } 2、目前HbaseWriter支持源端多个字段拼接作为hbase 表的 rowkey，具体配置参考：rowkeyColumn配置；\n3、写入hbase的时间戳（版本）支持：用当前时间作为版本，指定源端列作为版本，指定一个时间 三种方式作为版本；\n1.2 限制 1、目前只支持源端为横表写入，不支持竖表（源端读出的为四元组: rowKey，family:qualifier，timestamp，value）模式的数据写入；本期目标主要是替换DataX2中的habsewriter，下次迭代考虑支持。\n2、目前不支持写入hbase前清空表数据，若需要清空数据请联系HBase PE\n2 实现原理 简而言之，HbaseWriter 通过 HBase 的 Java 客户端，通过 HTable, Put等 API，将从上游Reader读取的数据写入HBase你hbase11xwriter与hbase094xwriter的主要不同在于API的调用不同，Hbase1.1.x废弃了很多Hbase0.94.x的api。\n3 功能说明 3.1 配置样例 配置一个从本地写入hbase1.1.x的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xwriter/txt/normal.txt\u0026#34;, \u0026#34;charset\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 4, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 5, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 6, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.","title":"Hbase094XWriter \u0026 Hbase11XWriter 插件文档"},{"content":"hbase11xsqlreader 插件文档 1 快速介绍 hbase11xsqlreader插件实现了从Phoenix(HBase SQL)读取数据。在底层实现上，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并执行相应的sql语句将数据从Phoenix库中SELECT出来。\n2 实现原理 简而言之，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并根据用户配置的信息生成查询SELECT 语句，然后发送到HBase集群，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\nhbase11xsqlreader 插件文档 1 快速介绍 hbase11xsqlreader插件实现了从Phoenix(HBase SQL)读取数据。在底层实现上，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并执行相应的sql语句将数据从Phoenix库中SELECT出来。\n2 实现原理 简而言之，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并根据用户配置的信息生成查询SELECT 语句，然后发送到HBase集群，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n3 功能说明 3.1 配置样例 配置一个从Phoenix同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;:10485760 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { //指定插件为hbase11xsqlreader \u0026#34;name\u0026#34;: \u0026#34;hbase11xsqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { //填写连接Phoenix的hbase集群zk地址 \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;hb-proxy-xxx-002.hbase.rds.aliyuncs.com,hb-proxy-xxx-001.hbase.rds.aliyuncs.com,hb-proxy-xxx-003.hbase.rds.aliyuncs.com\u0026#34; }, //填写要读取的phoenix的表名 \u0026#34;table\u0026#34;: \u0026#34;US_POPULATION\u0026#34;, //填写要读取的列名，不填读取所有列 \u0026#34;column\u0026#34;: [ ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 hbaseConfig\n描述：hbase11xsqlreader需要通过Phoenix客户端去连接hbase集群，因此这里需要填写对应hbase集群的zkurl地址，注意不要添加2181。\n必选：是 默认值：无 table\n描述：编写Phoenix中的表名,如果有namespace，该值设置为\u0026rsquo;namespace.tablename'\n必选：是 默认值：无 column\n描述：填写需要从phoenix表中读取的列名集合，使用JSON的数组描述字段信息，空值表示读取所有列。\n必选：是 默认值：无 3.3 类型转换 目前hbase11xsqlreader支持大部分Phoenix类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出MysqlReader针对Mysql类型转换列表:\nDataX 内部类型 Phoenix 数据类型 String CHAR, VARCHAR Bytes BINARY, VARBINARY Bool BOOLEAN Long INTEGER, TINYINT, SMALLINT, BIGINT Double FLOAT, DECIMAL, DOUBLE, Date DATE, TIME, TIMESTAMP 4 性能报告 略\n5 约束限制 略\n6 FAQ 3 功能说明 3.1 配置样例 配置一个从Phoenix同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;:10485760 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { //指定插件为hbase11xsqlreader \u0026#34;name\u0026#34;: \u0026#34;hbase11xsqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { //填写连接Phoenix的hbase集群zk地址 \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;hb-proxy-xxx-002.hbase.rds.aliyuncs.com,hb-proxy-xxx-001.hbase.rds.aliyuncs.com,hb-proxy-xxx-003.hbase.rds.aliyuncs.com\u0026#34; }, //填写要读取的phoenix的表名 \u0026#34;table\u0026#34;: \u0026#34;US_POPULATION\u0026#34;, //填写要读取的列名，不填读取所有列 \u0026#34;column\u0026#34;: [ ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 hbaseConfig\n描述：hbase11xsqlreader需要通过Phoenix客户端去连接hbase集群，因此这里需要填写对应hbase集群的zkurl地址，注意不要添加2181。\n必选：是 默认值：无 table\n描述：编写Phoenix中的表名,如果有namespace，该值设置为\u0026rsquo;namespace.tablename'\n必选：是 默认值：无 column\n描述：填写需要从phoenix表中读取的列名集合，使用JSON的数组描述字段信息，空值表示读取所有列。\n必选：是 默认值：无 3.3 类型转换 目前hbase11xsqlreader支持大部分Phoenix类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出MysqlReader针对Mysql类型转换列表:\nDataX 内部类型 Phoenix 数据类型 String CHAR, VARCHAR Bytes BINARY, VARBINARY Bool BOOLEAN Long INTEGER, TINYINT, SMALLINT, BIGINT Double FLOAT, DECIMAL, DOUBLE, Date DATE, TIME, TIMESTAMP 4 性能报告 略\n5 约束限制 略\n6 FAQ ","permalink":"http://121.199.2.5:6080/283a89fffad54e6f909c2936a1ff08ad/","summary":"hbase11xsqlreader 插件文档 1 快速介绍 hbase11xsqlreader插件实现了从Phoenix(HBase SQL)读取数据。在底层实现上，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并执行相应的sql语句将数据从Phoenix库中SELECT出来。\n2 实现原理 简而言之，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并根据用户配置的信息生成查询SELECT 语句，然后发送到HBase集群，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\nhbase11xsqlreader 插件文档 1 快速介绍 hbase11xsqlreader插件实现了从Phoenix(HBase SQL)读取数据。在底层实现上，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并执行相应的sql语句将数据从Phoenix库中SELECT出来。\n2 实现原理 简而言之，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并根据用户配置的信息生成查询SELECT 语句，然后发送到HBase集群，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n3 功能说明 3.1 配置样例 配置一个从Phoenix同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;:10485760 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { //指定插件为hbase11xsqlreader \u0026#34;name\u0026#34;: \u0026#34;hbase11xsqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { //填写连接Phoenix的hbase集群zk地址 \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;hb-proxy-xxx-002.hbase.rds.aliyuncs.com,hb-proxy-xxx-001.hbase.rds.aliyuncs.com,hb-proxy-xxx-003.hbase.rds.aliyuncs.com\u0026#34; }, //填写要读取的phoenix的表名 \u0026#34;table\u0026#34;: \u0026#34;US_POPULATION\u0026#34;, //填写要读取的列名，不填读取所有列 \u0026#34;column\u0026#34;: [ ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 hbaseConfig\n描述：hbase11xsqlreader需要通过Phoenix客户端去连接hbase集群，因此这里需要填写对应hbase集群的zkurl地址，注意不要添加2181。\n必选：是 默认值：无 table\n描述：编写Phoenix中的表名,如果有namespace，该值设置为\u0026rsquo;namespace.tablename'\n必选：是 默认值：无 column","title":"hbase11xsqlreader  插件文档"},{"content":"HBase11xsqlwriter插件文档 1. 快速介绍 HBase11xsqlwriter实现了向hbase中的SQL表(phoenix)批量导入数据的功能。Phoenix因为对rowkey做了数据编码，所以，直接使用HBaseAPI进行写入会面临手工数据转换的问题，麻烦且易错。本插件提供了单间的SQL表的数据导入方式。\n在底层实现上，通过Phoenix的JDBC驱动，执行UPSERT语句向hbase写入数据。\n1.1 支持的功能 支持带索引的表的数据导入，可以同步更新所有的索引表 1.2 限制 仅支持1.x系列的hbase 仅支持通过phoenix创建的表，不支持原生HBase表 不支持带时间戳的数据导入 2. 实现原理 通过Phoenix的JDBC驱动，执行UPSERT语句向表中批量写入数据。因为使用上层接口，所以，可以同步更新索引表。\n3. 配置说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;entry\u0026#34;: { \u0026#34;jvm\u0026#34;: \u0026#34;-Xms2048m -Xmx2048m\u0026#34; }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xsqlwriter/txt/normal.txt\u0026#34;, \u0026#34;charset\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xsqlwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;batchSize\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;UID\u0026#34;, \u0026#34;TS\u0026#34;, \u0026#34;EVENTID\u0026#34;, \u0026#34;CONTENT\u0026#34; ], \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;目标hbase集群的ZK服务器地址，向PE咨询\u0026#34;, \u0026#34;zookeeper.znode.parent\u0026#34;: \u0026#34;目标hbase集群的znode，向PE咨询\u0026#34; }, \u0026#34;nullMode\u0026#34;: \u0026#34;skip\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;目标hbase表名，大小写有关\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } } } } 3.2 参数说明 name\n描述：插件名字，必须是hbase11xsqlwriter 必选：是 默认值：无 table\n描述：要导入的表名，大小写敏感，通常phoenix表都是大写表名 必选：是 默认值：无 column\n描述：列名，大小写敏感，通常phoenix的列名都是大写。 需要注意列的顺序，必须与reader输出的列的顺序一一对应。 不需要填写数据类型，会自动从phoenix获取列的元数据 必选：是 默认值：无 hbaseConfig\n描述：hbase集群地址，zk为必填项，格式：ip1,ip2,ip3，注意，多个IP之间使用英文的逗号分隔。znode是可选的，默认值是/hbase 必选：是 默认值：无 batchSize\n描述：批量写入的最大行数 必选：否 默认值：256 nullMode\n描述：读取到的列值为null时，如何处理。目前有两种方式： skip：跳过这一列，即不插入这一列(如果该行的这一列之前已经存在，则会被删除) empty：插入空值，值类型的空值是0，varchar的空值是空字符串 必选：否 默认值：skip 4. 性能报告 无\n5. 约束限制 writer中的列的定义顺序必须与reader的列顺序匹配。reader中的列顺序定义了输出的每一行中，列的组织顺序。而writer的列顺序，定义的是在收到的数据中，writer期待的列的顺序。例如：\nreader的列顺序是： c1, c2, c3, c4\nwriter的列顺序是： x1, x2, x3, x4\n则reader输出的列c1就会赋值给writer的列x1。如果writer的列顺序是x1, x2, x4, x3，则c3会赋值给x4，c4会赋值给x3.\n6. FAQ 并发开多少合适？速度慢时增加并发有用吗？ 数据导入进程默认JVM的堆大小是2GB，并发(channel数)是通过多线程实现的，开过多的线程有时并不能提高导入速度，反而可能因为过于频繁的GC导致性能下降。一般建议并发数(channel)为5-10.\nbatchSize设置多少比较合适？ 默认是256，但应根据每行的大小来计算最合适的batchSize。通常一次操作的数据量在2MB-4MB左右，用这个值除以行大小，即可得到batchSize。\n","permalink":"http://121.199.2.5:6080/1c329829c22c499d8e684735252966c1/","summary":"HBase11xsqlwriter插件文档 1. 快速介绍 HBase11xsqlwriter实现了向hbase中的SQL表(phoenix)批量导入数据的功能。Phoenix因为对rowkey做了数据编码，所以，直接使用HBaseAPI进行写入会面临手工数据转换的问题，麻烦且易错。本插件提供了单间的SQL表的数据导入方式。\n在底层实现上，通过Phoenix的JDBC驱动，执行UPSERT语句向hbase写入数据。\n1.1 支持的功能 支持带索引的表的数据导入，可以同步更新所有的索引表 1.2 限制 仅支持1.x系列的hbase 仅支持通过phoenix创建的表，不支持原生HBase表 不支持带时间戳的数据导入 2. 实现原理 通过Phoenix的JDBC驱动，执行UPSERT语句向表中批量写入数据。因为使用上层接口，所以，可以同步更新索引表。\n3. 配置说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;entry\u0026#34;: { \u0026#34;jvm\u0026#34;: \u0026#34;-Xms2048m -Xmx2048m\u0026#34; }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase11xsqlwriter/txt/normal.txt\u0026#34;, \u0026#34;charset\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase11xsqlwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;batchSize\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;UID\u0026#34;, \u0026#34;TS\u0026#34;, \u0026#34;EVENTID\u0026#34;, \u0026#34;CONTENT\u0026#34; ], \u0026#34;hbaseConfig\u0026#34;: { \u0026#34;hbase.zookeeper.quorum\u0026#34;: \u0026#34;目标hbase集群的ZK服务器地址，向PE咨询\u0026#34;, \u0026#34;zookeeper.znode.parent\u0026#34;: \u0026#34;目标hbase集群的znode，向PE咨询\u0026#34; }, \u0026#34;nullMode\u0026#34;: \u0026#34;skip\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;目标hbase表名，大小写有关\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } } } } 3.","title":"HBase11xsqlwriter插件文档"},{"content":"hbase20xsqlreader 插件文档 1 快速介绍 hbase20xsqlreader插件实现了从Phoenix(HBase SQL)读取数据，对应版本为HBase2.X和Phoenix5.X。\n2 实现原理 简而言之，hbase20xsqlreader通过Phoenix轻客户端去连接Phoenix QueryServer，并根据用户配置信息生成查询SELECT 语句，然后发送到QueryServer读取HBase数据，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，最终传递给下游Writer处理。\n3 功能说明 3.1 配置样例 配置一个从Phoenix同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase20xsqlreader\u0026#34;, //指定插件为hbase20xsqlreader \u0026#34;parameter\u0026#34;: { \u0026#34;queryServerAddress\u0026#34;: \u0026#34;http://127.0.0.1:8765\u0026#34;, //填写连接Phoenix QueryServer地址 \u0026#34;serialization\u0026#34;: \u0026#34;PROTOBUF\u0026#34;, //QueryServer序列化格式 \u0026#34;table\u0026#34;: \u0026#34;TEST\u0026#34;, //读取表名 \u0026#34;column\u0026#34;: [\u0026#34;ID\u0026#34;, \u0026#34;NAME\u0026#34;], //所要读取列名 \u0026#34;splitKey\u0026#34;: \u0026#34;ID\u0026#34; //切分列，必须是表主键 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: \u0026#34;3\u0026#34; } } } } 3.2 参数说明 queryServerAddress\n描述：hbase20xsqlreader需要通过Phoenix轻客户端去连接Phoenix QueryServer，因此这里需要填写对应QueryServer地址。 增强版/Lindorm 用户若需透传user, password参数，可以在queryServerAddress后增加对应可选属性. 格式参考：http://127.0.0.1:8765;user=root;password=root\n必选：是 默认值：无 serialization\n描述：QueryServer使用的序列化协议\n必选：否 默认值：PROTOBUF table\n描述：所要读取表名\n必选：是 默认值：无 schema\n描述：表所在的schema\n必选：否 默认值：无 column\n描述：填写需要从phoenix表中读取的列名集合，使用JSON的数组描述字段信息，空值表示读取所有列。\n必选： 否\n默认值：全部列 splitKey\n描述：读取表时对表进行切分并行读取，切分时有两种方式：1.根据该列的最大最小值按照指定channel个数均分，这种方式仅支持整形和字符串类型切分列；2.根据设置的splitPoint进行切分\n必选：是 默认值：无 splitPoints\n描述：由于根据切分列最大最小值切分时不能保证避免数据热点，splitKey支持用户根据数据特征动态指定切分点，对表数据进行切分。建议切分点根据Region的startkey和endkey设置，保证每个查询对应单个Region\n必选： 否\n默认值：无 where\n描述：支持对表查询增加过滤条件，每个切分都会携带该过滤条件。\n必选： 否\n默认值：无\nquerySql\n描述：支持指定多个查询语句，但查询列类型和数目必须保持一致，用户可根据实际情况手动输入表查询语句或多表联合查询语句，设置该参数后，除queryserverAddress参数必须设置外，其余参数将失去作用或可不设置。\n必选： 否\n默认值：无\n3.3 类型转换 目前hbase20xsqlreader支持大部分Phoenix类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出MysqlReader针对Mysql类型转换列表:\nDataX 内部类型 Phoenix 数据类型 String CHAR, VARCHAR Bytes BINARY, VARBINARY Bool BOOLEAN Long INTEGER, TINYINT, SMALLINT, BIGINT Double FLOAT, DECIMAL, DOUBLE, Date DATE, TIME, TIMESTAMP 4 性能报告 略\n5 约束限制 切分表时切分列仅支持单个列，且该列必须是表主键 不设置splitPoint默认使用自动切分，此时切分列仅支持整形和字符型 表名和SCHEMA名及列名大小写敏感，请与Phoenix表实际大小写保持一致 仅支持通过Phoenix QeuryServer读取数据，因此您的Phoenix必须启动QueryServer服务才能使用本插件 6 FAQ ","permalink":"http://121.199.2.5:6080/d74e56a13f78424182e14164c8b4e5e4/","summary":"hbase20xsqlreader 插件文档 1 快速介绍 hbase20xsqlreader插件实现了从Phoenix(HBase SQL)读取数据，对应版本为HBase2.X和Phoenix5.X。\n2 实现原理 简而言之，hbase20xsqlreader通过Phoenix轻客户端去连接Phoenix QueryServer，并根据用户配置信息生成查询SELECT 语句，然后发送到QueryServer读取HBase数据，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，最终传递给下游Writer处理。\n3 功能说明 3.1 配置样例 配置一个从Phoenix同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase20xsqlreader\u0026#34;, //指定插件为hbase20xsqlreader \u0026#34;parameter\u0026#34;: { \u0026#34;queryServerAddress\u0026#34;: \u0026#34;http://127.0.0.1:8765\u0026#34;, //填写连接Phoenix QueryServer地址 \u0026#34;serialization\u0026#34;: \u0026#34;PROTOBUF\u0026#34;, //QueryServer序列化格式 \u0026#34;table\u0026#34;: \u0026#34;TEST\u0026#34;, //读取表名 \u0026#34;column\u0026#34;: [\u0026#34;ID\u0026#34;, \u0026#34;NAME\u0026#34;], //所要读取列名 \u0026#34;splitKey\u0026#34;: \u0026#34;ID\u0026#34; //切分列，必须是表主键 } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: \u0026#34;3\u0026#34; } } } } 3.2 参数说明 queryServerAddress\n描述：hbase20xsqlreader需要通过Phoenix轻客户端去连接Phoenix QueryServer，因此这里需要填写对应QueryServer地址。 增强版/Lindorm 用户若需透传user, password参数，可以在queryServerAddress后增加对应可选属性. 格式参考：http://127.0.0.1:8765;user=root;password=root\n必选：是 默认值：无 serialization\n描述：QueryServer使用的序列化协议\n必选：否 默认值：PROTOBUF table\n描述：所要读取表名\n必选：是 默认值：无 schema\n描述：表所在的schema\n必选：否 默认值：无 column\n描述：填写需要从phoenix表中读取的列名集合，使用JSON的数组描述字段信息，空值表示读取所有列。\n必选： 否\n默认值：全部列 splitKey","title":"hbase20xsqlreader  插件文档"},{"content":"HBase20xsqlwriter插件文档 1. 快速介绍 HBase20xsqlwriter实现了向hbase中的SQL表(phoenix)批量导入数据的功能。Phoenix因为对rowkey做了数据编码，所以，直接使用HBaseAPI进行写入会面临手工数据转换的问题，麻烦且易错。本插件提供了SQL方式直接向Phoenix表写入数据。\n在底层实现上，通过Phoenix QueryServer的轻客户端驱动，执行UPSERT语句向Phoenix写入数据。\n1.1 支持的功能 支持带索引的表的数据导入，可以同步更新所有的索引表 1.2 限制 要求版本为Phoenix5.x及HBase2.x 仅支持通过Phoenix QeuryServer导入数据，因此您Phoenix必须启动QueryServer服务才能使用本插件 不支持清空已有表数据 仅支持通过phoenix创建的表，不支持原生HBase表 不支持带时间戳的数据导入 2. 实现原理 通过Phoenix轻客户端，连接Phoenix QueryServer服务，执行UPSERT语句向表中批量写入数据。因为使用上层接口，所以，可以同步更新索引表。\n3. 配置说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;entry\u0026#34;: { \u0026#34;jvm\u0026#34;: \u0026#34;-Xms2048m -Xmx2048m\u0026#34; }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase20xsqlwriter/txt/normal.txt\u0026#34;, \u0026#34;charset\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase20xsqlwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;batchSize\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;UID\u0026#34;, \u0026#34;TS\u0026#34;, \u0026#34;EVENTID\u0026#34;, \u0026#34;CONTENT\u0026#34; ], \u0026#34;queryServerAddress\u0026#34;: \u0026#34;http://127.0.0.1:8765\u0026#34;, \u0026#34;nullMode\u0026#34;: \u0026#34;skip\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;目标hbase表名，大小写有关\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } } } } 3.2 参数说明 name\n描述：插件名字，必须是hbase11xsqlwriter 必选：是 默认值：无 schema\n描述：表所在的schema\n必选：否 默认值：无 table\n描述：要导入的表名，大小写敏感，通常phoenix表都是大写表名 必选：是 默认值：无 column\n描述：列名，大小写敏感，通常phoenix的列名都是大写。 需要注意列的顺序，必须与reader输出的列的顺序一一对应。 不需要填写数据类型，会自动从phoenix获取列的元数据 必选：是 默认值：无 queryServerAddress\n描述：Phoenix QueryServer地址，为必填项，格式：http://NULL:NULL，如http://172.16.34.58:8765。 增强版/Lindorm 用户若需透传user, password参数，可以在queryServerAddress后增加对应可选属性. 格式参考：http://127.0.0.1:8765;user=root;password=root 必选：是 默认值：无 serialization\n描述：QueryServer使用的序列化协议 必选：否 默认值：PROTOBUF batchSize\n描述：批量写入的最大行数 必选：否 默认值：256 nullMode\n描述：读取到的列值为null时，如何处理。目前有两种方式： skip：跳过这一列，即不插入这一列(如果该行的这一列之前已经存在，则会被删除) empty：插入空值，值类型的空值是0，varchar的空值是空字符串 必选：否 默认值：skip 4. 性能报告 无\n5. 约束限制 writer中的列的定义顺序必须与reader的列顺序匹配。reader中的列顺序定义了输出的每一行中，列的组织顺序。而writer的列顺序，定义的是在收到的数据中，writer期待的列的顺序。例如：\nreader的列顺序是： c1, c2, c3, c4\nwriter的列顺序是： x1, x2, x3, x4\n则reader输出的列c1就会赋值给writer的列x1。如果writer的列顺序是x1, x2, x4, x3，则c3会赋值给x4，c4会赋值给x3.\n6. FAQ 并发开多少合适？速度慢时增加并发有用吗？ 数据导入进程默认JVM的堆大小是2GB，并发(channel数)是通过多线程实现的，开过多的线程有时并不能提高导入速度，反而可能因为过于频繁的GC导致性能下降。一般建议并发数(channel)为5-10.\nbatchSize设置多少比较合适？ 默认是256，但应根据每行的大小来计算最合适的batchSize。通常一次操作的数据量在2MB-4MB左右，用这个值除以行大小，即可得到batchSize。\n","permalink":"http://121.199.2.5:6080/28250260969448579bc8cee8c2963319/","summary":"HBase20xsqlwriter插件文档 1. 快速介绍 HBase20xsqlwriter实现了向hbase中的SQL表(phoenix)批量导入数据的功能。Phoenix因为对rowkey做了数据编码，所以，直接使用HBaseAPI进行写入会面临手工数据转换的问题，麻烦且易错。本插件提供了SQL方式直接向Phoenix表写入数据。\n在底层实现上，通过Phoenix QueryServer的轻客户端驱动，执行UPSERT语句向Phoenix写入数据。\n1.1 支持的功能 支持带索引的表的数据导入，可以同步更新所有的索引表 1.2 限制 要求版本为Phoenix5.x及HBase2.x 仅支持通过Phoenix QeuryServer导入数据，因此您Phoenix必须启动QueryServer服务才能使用本插件 不支持清空已有表数据 仅支持通过phoenix创建的表，不支持原生HBase表 不支持带时间戳的数据导入 2. 实现原理 通过Phoenix轻客户端，连接Phoenix QueryServer服务，执行UPSERT语句向表中批量写入数据。因为使用上层接口，所以，可以同步更新索引表。\n3. 配置说明 3.1 配置样例 { \u0026#34;job\u0026#34;: { \u0026#34;entry\u0026#34;: { \u0026#34;jvm\u0026#34;: \u0026#34;-Xms2048m -Xmx2048m\u0026#34; }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;txtfilereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/Users/shf/workplace/datax_test/hbase20xsqlwriter/txt/normal.txt\u0026#34;, \u0026#34;charset\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;column\u0026#34;: [ { \u0026#34;index\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34; }, { \u0026#34;index\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;index\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ], \u0026#34;fieldDelimiter\u0026#34;: \u0026#34;,\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hbase20xsqlwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;batchSize\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;UID\u0026#34;, \u0026#34;TS\u0026#34;, \u0026#34;EVENTID\u0026#34;, \u0026#34;CONTENT\u0026#34; ], \u0026#34;queryServerAddress\u0026#34;: \u0026#34;http://127.0.0.1:8765\u0026#34;, \u0026#34;nullMode\u0026#34;: \u0026#34;skip\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;目标hbase表名，大小写有关\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } } } } 3.","title":"HBase20xsqlwriter插件文档"},{"content":"KingbaseesReader 插件文档 1 快速介绍 KingbaseesReader插件实现了从KingbaseES读取数据。在底层实现上，KingbaseesReader通过JDBC连接远程KingbaseES数据库，并执行相应的sql语句将数据从KingbaseES库中SELECT出来。\n2 实现原理 简而言之，KingbaseesReader通过JDBC连接器连接到远程的KingbaseES数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程KingbaseES数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，KingbaseesReader将其拼接为SQL语句发送到KingbaseES数据库；对于用户配置querySql信息，KingbaseesReader直接将其发送到KingbaseES数据库。\n3 功能说明 3.1 配置样例 配置一个从KingbaseES数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;: 1048576 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kingbaseesreader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;，\u0026#34;name\u0026#34; ], //切分主键 \u0026#34;splitPk\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:kingbase8://host:port/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: 1048576 }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kingbaseesreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:kingbase8://host:port/database\u0026#34;, \u0026#34;jdbc:kingbase8://host:port/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 jdbcUrl\n描述：描述的是到对端数据库的JDBC连接信息，使用JSON的数组描述，并支持一个库填写多个连接地址。之所以使用JSON数组描述连接信息，是因为阿里集团内部支持多个IP探测，如果配置了多个，KingbaseesReader可以依次探测ip的可连接性，直到选择一个合法的IP。如果全部连接失败，KingbaseesReader报错。 注意，jdbcUrl必须包含在connection配置单元中。对于阿里集团外部使用情况，JSON数组填写一个JDBC连接即可。\njdbcUrl按照KingbaseES官方规范，并可以填写连接附件控制信息。具体请参看KingbaseES官方文档。\n必选：是 默认值：无 username\n描述：数据源的用户名 必选：是 默认值：无 password\n描述：数据源指定用户名的密码 必选：是 默认值：无 table\n描述：所选取的需要同步的表。使用JSON的数组描述，因此支持多张表同时抽取。当配置为多张表时，用户自己需保证多张表是同一schema结构，KingbaseesReader不予检查表是否同一逻辑表。注意，table必须包含在connection配置单元中。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。用户使用*代表默认使用所有列配置，例如[\u0026rsquo;*\u0026rsquo;]。\n支持列裁剪，即列可以挑选部分列进行导出。\n支持列换序，即列可以不按照表schema信息进行导出。\n支持常量配置，用户需要按照KingbaseES语法格式: [\u0026ldquo;id\u0026rdquo;, \u0026ldquo;\u0026lsquo;hello\u0026rsquo;::varchar\u0026rdquo;, \u0026ldquo;true\u0026rdquo;, \u0026ldquo;2.5::real\u0026rdquo;, \u0026ldquo;power(2,3)\u0026rdquo;] id为普通列名，\u0026lsquo;hello\u0026rsquo;::varchar为字符串常量，true为布尔值，2.5为浮点数, power(2,3)为函数。\ncolumn必须用户显示指定同步的列集合，不允许为空！\n必选：是 默认值：无 splitPk\n描述：KingbaseesReader进行数据抽取时，如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。\n推荐splitPk用户使用表主键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。\n目前splitPk仅支持整形数据切分，不支持浮点、字符串型、日期等其他类型。如果用户指定其他非支持类型，KingbaseesReader将报错！\nsplitPk设置为空，底层将视作用户不允许对单表进行切分，因此使用单通道进行抽取。\n必选：否 默认值：空 where\n描述：筛选条件，KingbaseesReader根据指定的column、table、where条件拼接SQL，并根据这个SQL进行数据抽取。在实际业务场景中，往往会选择当天的数据进行同步，可以将where条件指定为gmt_create \u0026gt; $bizdate 。注意：不可以将where条件指定为limit 10，limit不是SQL的合法where子句。\nwhere条件可以有效地进行业务增量同步。\twhere条件不配置或者为空，视作全表同步数据。 必选：否 默认值：无 querySql\n描述：在有些业务场景下，where这一配置项不足以描述所筛选的条件，用户可以通过该配置型来自定义筛选SQL。当用户配置了这一项之后，DataX系统就会忽略table，column这些配置型，直接使用这个配置项的内容对数据进行筛选，例如需要进行多表join后同步数据，使用select a,b from table_a join table_b on table_a.id = table_b.id 当用户配置querySql时，KingbaseesReader直接忽略table、column、where条件的配置。\n必选：否 默认值：无 fetchSize\n描述：该配置项定义了插件和数据库服务器端每次批量数据获取条数，该值决定了DataX和服务器端的网络交互次数，能够较大的提升数据抽取性能。 注意，该值过大(\u0026gt;2048)可能造成DataX进程OOM。。\n必选：否 默认值：1024 3.3 类型转换 目前KingbaseesReader支持大部分KingbaseES类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出KingbaseesReader针对KingbaseES类型转换列表:\nDataX 内部类型 KingbaseES 数据类型 Long bigint, bigserial, integer, smallint, serial Double double precision, money, numeric, real String varchar, char, text, bit, inet Date date, time, timestamp Boolean bool Bytes bytea 请注意:\n除上述罗列字段类型外，其他类型均不支持; money,inet,bit需用户使用a_inet::varchar类似的语法转换。 ","permalink":"http://121.199.2.5:6080/cc7bd03f72154435ae9af2d67a214f6d/","summary":"KingbaseesReader 插件文档 1 快速介绍 KingbaseesReader插件实现了从KingbaseES读取数据。在底层实现上，KingbaseesReader通过JDBC连接远程KingbaseES数据库，并执行相应的sql语句将数据从KingbaseES库中SELECT出来。\n2 实现原理 简而言之，KingbaseesReader通过JDBC连接器连接到远程的KingbaseES数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程KingbaseES数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，KingbaseesReader将其拼接为SQL语句发送到KingbaseES数据库；对于用户配置querySql信息，KingbaseesReader直接将其发送到KingbaseES数据库。\n3 功能说明 3.1 配置样例 配置一个从KingbaseES数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;: 1048576 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kingbaseesreader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;，\u0026#34;name\u0026#34; ], //切分主键 \u0026#34;splitPk\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:kingbase8://host:port/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: 1048576 }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kingbaseesreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:kingbase8://host:port/database\u0026#34;, \u0026#34;jdbc:kingbase8://host:port/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.","title":"KingbaseesReader 插件文档"},{"content":"MysqlReader 插件文档 1 快速介绍 MysqlReader插件实现了从Mysql读取数据。在底层实现上，MysqlReader通过JDBC连接远程Mysql数据库，并执行相应的sql语句将数据从mysql库中SELECT出来。\n不同于其他关系型数据库，MysqlReader不支持FetchSize.\n2 实现原理 简而言之，MysqlReader通过JDBC连接器连接到远程的Mysql数据库，并根据用户配置的信息生成查询SELECT SQL语句，然后发送到远程Mysql数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，MysqlReader将其拼接为SQL语句发送到Mysql数据库；对于用户配置querySql信息，MysqlReader直接将其发送到Mysql数据库。\n3 功能说明 3.1 配置样例 配置一个从Mysql数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 }, \u0026#34;errorLimit\u0026#34;: { \u0026#34;record\u0026#34;: 0, \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mysqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;splitPk\u0026#34;: \u0026#34;db_id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:mysql://127.0.0.1:3306/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;:1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mysqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:mysql://bad_ip:3306/database\u0026#34;, \u0026#34;jdbc:mysql://127.0.0.1:bad_port/database\u0026#34;, \u0026#34;jdbc:mysql://127.0.0.1:3306/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 jdbcUrl\n描述：描述的是到对端数据库的JDBC连接信息，使用JSON的数组描述，并支持一个库填写多个连接地址。之所以使用JSON数组描述连接信息，是因为阿里集团内部支持多个IP探测，如果配置了多个，MysqlReader可以依次探测ip的可连接性，直到选择一个合法的IP。如果全部连接失败，MysqlReader报错。 注意，jdbcUrl必须包含在connection配置单元中。对于阿里集团外部使用情况，JSON数组填写一个JDBC连接即可。\njdbcUrl按照Mysql官方规范，并可以填写连接附件控制信息。具体请参看Mysql官方文档。\n必选：是 默认值：无 username\n描述：数据源的用户名 必选：是 默认值：无 password\n描述：数据源指定用户名的密码 必选：是 默认值：无 table\n描述：所选取的需要同步的表。使用JSON的数组描述，因此支持多张表同时抽取。当配置为多张表时，用户自己需保证多张表是同一schema结构，MysqlReader不予检查表是否同一逻辑表。注意，table必须包含在connection配置单元中。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。用户使用*代表默认使用所有列配置，例如[\u0026rsquo;*\u0026rsquo;]。\n支持列裁剪，即列可以挑选部分列进行导出。\n支持列换序，即列可以不按照表schema信息进行导出。\n支持常量配置，用户需要按照Mysql SQL语法格式: [\u0026ldquo;id\u0026rdquo;, \u0026ldquo;`table`\u0026rdquo;, \u0026ldquo;1\u0026rdquo;, \u0026ldquo;\u0026lsquo;bazhen.csy\u0026rsquo;\u0026rdquo;, \u0026ldquo;null\u0026rdquo;, \u0026ldquo;to_char(a + 1)\u0026rdquo;, \u0026ldquo;2.3\u0026rdquo; , \u0026ldquo;true\u0026rdquo;] id为普通列名，`table`为包含保留在的列名，1为整形数字常量，\u0026lsquo;bazhen.csy\u0026rsquo;为字符串常量，null为空指针，to_char(a + 1)为表达式，2.3为浮点数，true为布尔值。\n必选：是 默认值：无 splitPk\n描述：MysqlReader进行数据抽取时，如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。\n推荐splitPk用户使用表主键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。\n目前splitPk仅支持整形数据切分，不支持浮点、字符串、日期等其他类型。如果用户指定其他非支持类型，MysqlReader将报错！\n如果splitPk不填写，包括不提供splitPk或者splitPk值为空，DataX视作使用单通道同步该表数据。 必选：否 默认值：空 where\n描述：筛选条件，MysqlReader根据指定的column、table、where条件拼接SQL，并根据这个SQL进行数据抽取。在实际业务场景中，往往会选择当天的数据进行同步，可以将where条件指定为gmt_create \u0026gt; $bizdate 。注意：不可以将where条件指定为limit 10，limit不是SQL的合法where子句。\nwhere条件可以有效地进行业务增量同步。如果不填写where语句，包括不提供where的key或者value，DataX均视作同步全量数据。 必选：否 默认值：无 querySql\n描述：在有些业务场景下，where这一配置项不足以描述所筛选的条件，用户可以通过该配置型来自定义筛选SQL。当用户配置了这一项之后，DataX系统就会忽略table，column这些配置型，直接使用这个配置项的内容对数据进行筛选，例如需要进行多表join后同步数据，使用select a,b from table_a join table_b on table_a.id = table_b.id 当用户配置querySql时，MysqlReader直接忽略table、column、where条件的配置，querySql优先级大于table、column、where选项。\n必选：否 默认值：无 3.3 类型转换 目前MysqlReader支持大部分Mysql类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出MysqlReader针对Mysql类型转换列表:\nDataX 内部类型 Mysql 数据类型 Long int, tinyint, smallint, mediumint, int, bigint Double float, double, decimal String varchar, char, tinytext, text, mediumtext, longtext, year Date date, datetime, timestamp, time Boolean bit, bool Bytes tinyblob, mediumblob, blob, longblob, varbinary 请注意:\n除上述罗列字段类型外，其他类型均不支持。 tinyint(1) DataX视作为整形。 year DataX视作为字符串类型 bit DataX属于未定义行为。 4 性能报告 4.1 环境准备 4.1.1 数据特征 建表语句：\nCREATE TABLE `tc_biz_vertical_test_0000` ( `biz_order_id` bigint(20) NOT NULL COMMENT 'id', `key_value` varchar(4000) NOT NULL COMMENT 'Key-value的内容', `gmt_create` datetime NOT NULL COMMENT '创建时间', `gmt_modified` datetime NOT NULL COMMENT '修改时间', `attribute_cc` int(11) DEFAULT NULL COMMENT '防止并发修改的标志', `value_type` int(11) NOT NULL DEFAULT '0' COMMENT '类型', `buyer_id` bigint(20) DEFAULT NULL COMMENT 'buyerid', `seller_id` bigint(20) DEFAULT NULL COMMENT 'seller_id', PRIMARY KEY (`biz_order_id`,`value_type`), KEY `idx_biz_vertical_gmtmodified` (`gmt_modified`) ) ENGINE=InnoDB DEFAULT CHARSET=gbk COMMENT='tc_biz_vertical' 单行记录类似于：\nbiz_order_id: 888888888 key_value: ;orderIds:20148888888,2014888888813800; gmt_create: 2011-09-24 11:07:20 gmt_modified: 2011-10-24 17:56:34 attribute_cc: 1 value_type: 3 buyer_id: 8888888 seller_id: 1 4.1.2 机器参数 执行DataX的机器参数为:\ncpu: 24核 Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz mem: 48GB net: 千兆双网卡 disc: DataX 数据不落磁盘，不统计此项 Mysql数据库机器参数为:\ncpu: 32核 Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz mem: 256GB net: 千兆双网卡 disc: BTWL419303E2800RGN INTEL SSDSC2BB800G4 D2010370 4.1.3 DataX jvm 参数 -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError 4.2 测试报告 4.2.1 单表测试报告 通道数 是否按照主键切分 DataX速度(Rec/s) DataX流量(MB/s) DataX机器网卡进入流量(MB/s) DataX机器运行负载 DB网卡流出流量(MB/s) DB运行负载 1 否 183185 18.11 29 0.6 31 0.6 1 是 183185 18.11 29 0.6 31 0.6 4 否 183185 18.11 29 0.6 31 0.6 4 是 329733 32.60 58 0.8 60 0.76 8 否 183185 18.11 29 0.6 31 0.6 8 是 549556 54.33 115 1.46 120 0.78 说明：\n这里的单表，主键类型为 bigint(20),范围为：190247559466810-570722244711460，从主键范围划分看，数据分布均匀。 对单表如果没有安装主键切分，那么配置通道个数不会提升速度，效果与1个通道一样。 4.2.2 分表测试报告(2个分库，每个分库16张分表，共计32张分表) 通道数 DataX速度(Rec/s) DataX流量(MB/s) DataX机器网卡进入流量(MB/s) DataX机器运行负载 DB网卡流出流量(MB/s) DB运行负载 1 202241 20.06 31.5 1.0 32 1.1 4 726358 72.04 123.9 3.1 132 3.6 8 1074405 106.56 197 5.5 205 5.1 16 1227892 121.79 229.2 8.1 233 7.3 5 约束限制 5.1 主备同步数据恢复问题 主备同步问题指Mysql使用主从灾备，备库从主库不间断通过binlog恢复数据。由于主备数据同步存在一定的时间差，特别在于某些特定情况，例如网络延迟等问题，导致备库同步恢复的数据与主库有较大差别，导致从备库同步的数据不是一份当前时间的完整镜像。\n针对这个问题，我们提供了preSql功能，该功能待补充。\n5.2 一致性约束 Mysql在数据存储划分中属于RDBMS系统，对外可以提供强一致性数据查询接口。例如当一次同步任务启动运行过程中，当该库存在其他数据写入方写入数据时，MysqlReader完全不会获取到写入更新数据，这是由于数据库本身的快照特性决定的。关于数据库快照特性，请参看MVCC Wikipedia\n上述是在MysqlReader单线程模型下数据同步一致性的特性，由于MysqlReader可以根据用户配置信息使用了并发数据抽取，因此不能严格保证数据一致性：当MysqlReader根据splitPk进行数据切分后，会先后启动多个并发任务完成数据同步。由于多个并发任务相互之间不属于同一个读事务，同时多个并发任务存在时间间隔。因此这份数据并不是完整的、一致的数据快照信息。\n针对多线程的一致性快照需求，在技术上目前无法实现，只能从工程角度解决，工程化的方式存在取舍，我们提供几个解决思路给用户，用户可以自行选择：\n使用单线程同步，即不再进行数据切片。缺点是速度比较慢，但是能够很好保证一致性。\n关闭其他数据写入方，保证当前数据为静态数据，例如，锁表、关闭备库同步等等。缺点是可能影响在线业务。\n5.3 数据库编码问题 Mysql本身的编码设置非常灵活，包括指定编码到库、表、字段级别，甚至可以均不同编码。优先级从高到低为字段、表、库、实例。我们不推荐数据库用户设置如此混乱的编码，最好在库级别就统一到UTF-8。\nMysqlReader底层使用JDBC进行数据抽取，JDBC天然适配各类编码，并在底层进行了编码转换。因此MysqlReader不需用户指定编码，可以自动获取编码并转码。\n对于Mysql底层写入编码和其设定的编码不一致的混乱情况，MysqlReader对此无法识别，对此也无法提供解决方案，对于这类情况，导出有可能为乱码。\n5.4 增量数据同步 MysqlReader使用JDBC SELECT语句完成数据抽取工作，因此可以使用SELECT\u0026hellip;WHERE\u0026hellip;进行增量数据抽取，方式有多种：\n数据库在线应用写入数据库时，填充modify字段为更改时间戳，包括新增、更新、删除(逻辑删)。对于这类应用，MysqlReader只需要WHERE条件跟上一同步阶段时间戳即可。 对于新增流水型数据，MysqlReader可以WHERE条件后跟上一阶段最大自增ID即可。 对于业务上无字段区分新增、修改数据情况，MysqlReader也无法进行增量数据同步，只能同步全量数据。\n5.5 Sql安全性 MysqlReader提供querySql语句交给用户自己实现SELECT抽取语句，MysqlReader本身对querySql不做任何安全性校验。这块交由DataX用户方自己保证。\n6 FAQ Q: MysqlReader同步报错，报错信息为XXX\nA: 网络或者权限问题，请使用mysql命令行测试：\nmysql -u\u0026lt;username\u0026gt; -p\u0026lt;password\u0026gt; -h\u0026lt;ip\u0026gt; -D\u0026lt;database\u0026gt; -e \u0026quot;select * from \u0026lt;表名\u0026gt;\u0026quot; 如果上述命令也报错，那可以证实是环境问题，请联系你的DBA。\n","permalink":"http://121.199.2.5:6080/29875f5b780c46929670f9b8699ed462/","summary":"MysqlReader 插件文档 1 快速介绍 MysqlReader插件实现了从Mysql读取数据。在底层实现上，MysqlReader通过JDBC连接远程Mysql数据库，并执行相应的sql语句将数据从mysql库中SELECT出来。\n不同于其他关系型数据库，MysqlReader不支持FetchSize.\n2 实现原理 简而言之，MysqlReader通过JDBC连接器连接到远程的Mysql数据库，并根据用户配置的信息生成查询SELECT SQL语句，然后发送到远程Mysql数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，MysqlReader将其拼接为SQL语句发送到Mysql数据库；对于用户配置querySql信息，MysqlReader直接将其发送到Mysql数据库。\n3 功能说明 3.1 配置样例 配置一个从Mysql数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 }, \u0026#34;errorLimit\u0026#34;: { \u0026#34;record\u0026#34;: 0, \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mysqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;, \u0026#34;name\u0026#34; ], \u0026#34;splitPk\u0026#34;: \u0026#34;db_id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:mysql://127.0.0.1:3306/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;:1 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mysqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:mysql://bad_ip:3306/database\u0026#34;, \u0026#34;jdbc:mysql://127.","title":"MysqlReader 插件文档"},{"content":"OpenTSDBReader 插件文档 1 快速介绍 OpenTSDBReader 插件实现了从 OpenTSDB 读取数据。OpenTSDB 是主要由 Yahoo 维护的、可扩展的、分布式时序数据库，与阿里巴巴自研 TSDB 的关系与区别详见阿里云官网：《相比 OpenTSDB 优势》\n2 实现原理 在底层实现上，OpenTSDBReader 通过 HTTP 请求链接到 OpenTSDB 实例，利用 /api/config 接口获取到其底层存储 HBase 的连接信息，再利用 AsyncHBase 框架连接 HBase，通过 Scan 的方式将数据点扫描出来。整个同步的过程通过 metric 和时间段进行切分，即某个 metric 在某一个小时内的数据迁移，组合成一个迁移 Task。\n3 功能说明 3.1 配置样例 配置一个从 OpenTSDB 数据库同步抽取数据到本地的作业： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;opentsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:4242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 03:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } } } } 3.2 参数说明 name\n描述：本插件的名称 必选：是 默认值：opentsdbreader parameter\nendpoint 描述：OpenTSDB 的 HTTP 连接地址 必选：是 格式：http://IP:Port 默认值：无\ncolumn 描述：数据迁移任务需要迁移的 Metric 列表 必选：是 默认值：无 beginDateTime\n描述：和 endDateTime 配合使用，用于指定哪个时间段内的数据点，需要被迁移 必选：是 格式：yyyy-MM-dd HH:mm:ss 默认值：无 注意：指定起止时间会自动忽略分钟和秒，转为整点时刻，例如 2019-4-18 的 [3:35, 4:55) 会被转为 [3:00, 4:00) endDateTime\n描述：和 beginDateTime 配合使用，用于指定哪个时间段内的数据点，需要被迁移 必选：是 格式：yyyy-MM-dd HH:mm:ss 默认值：无 注意：指定起止时间会自动忽略分钟和秒，转为整点时刻，例如 2019-4-18 的 [3:35, 4:55) 会被转为 [3:00, 4:00) 3.3 类型转换 DataX 内部类型 TSDB 数据类型 String TSDB 数据点序列化字符串，包括 timestamp、metric、tags 和 value 4 性能报告 4.1 环境准备 4.1.1 数据特征 从 Metric、时间线、Value 和 采集周期 四个方面来描述：\nmetric 固定指定一个 metric 为 m。\ntagkv 前四个 tagkv 全排列，形成 10 * 20 * 100 * 100 = 2000000 条时间线，最后 IP 对应 2000000 条时间线从 1 开始自增。\ntag_k tag_v zone z1~z10 cluster c1~c20 group g1~100 app a1~a100 ip ip1~ip2000000 value 度量值为 [1, 100] 区间内的随机值\ninterval 采集周期为 10 秒，持续摄入 3 小时，总数据量为 3 * 60 * 60 / 10 * 2000000 = 2,160,000,000 个数据点。\n4.1.2 机器参数 OpenTSDB Reader 机型: 64C256G\nHBase 机型： 8C16G * 5\n4.1.3 DataX jvm 参数 \u0026ldquo;-Xms4096m -Xmx4096m\u0026rdquo;\n4.2 测试报告 通道数 DataX 速度 (Rec/s) DataX 流量 (MB/s) 1 215428 25.65 2 424994 50.60 3 603132 71.81 5 约束限制 5.1 需要确保与 OpenTSDB 底层存储的网络是连通的 具体缘由详见 6.1\n5.2 如果存在某一个 Metric 下在一个小时范围内的数据量过大，可能需要通过 -j 参数调整 JVM 内存大小 考虑到下游 Writer 如果写入速度不及 OpenTSDB reader 的查询数据，可能会存在积压的情况，因此需要适当地调整 JVM 参数。以\u0026quot;从 OpenTSDB 数据库同步抽取数据到本地的作业\u0026quot;为例，启动命令如下：\npython datax/bin/datax.py opentsdb2stream.json -j \u0026#34;-Xms4096m -Xmx4096m\u0026#34; 5.3 指定起止时间会自动被转为整点时刻 指定起止时间会自动被转为整点时刻，例如 2019-4-18 的 [3:35, 3:55) 会被转为 [3:00, 4:00)\n5.4 目前只支持兼容 OpenTSDB 2.3.x 其他版本暂不保证兼容\n6 FAQ Q：为什么需要连接 OpenTSDB 的底层存储，为什么不直接使用 /api/query 查询获取数据点？\nA：因为通过 OpenTSDB 的 HTTP 接口（/api/query）来读取数据的话，经内部压测发现，在大数据量的情况下，会导致 OpenTSDB 的异步框架会报 CallBack 过多的问题；所以，采用了直连底层 HBase 存储，通过 Scan 的方式来扫描数据点，来避免这个问题。另外，还考虑到，可以通过指定 metric 和时间范围，可以顺序地 Scan HBase 表，提高查询效率。\n","permalink":"http://121.199.2.5:6080/4e39a8595b6247fabfd6f625f053b0b6/","summary":"OpenTSDBReader 插件文档 1 快速介绍 OpenTSDBReader 插件实现了从 OpenTSDB 读取数据。OpenTSDB 是主要由 Yahoo 维护的、可扩展的、分布式时序数据库，与阿里巴巴自研 TSDB 的关系与区别详见阿里云官网：《相比 OpenTSDB 优势》\n2 实现原理 在底层实现上，OpenTSDBReader 通过 HTTP 请求链接到 OpenTSDB 实例，利用 /api/config 接口获取到其底层存储 HBase 的连接信息，再利用 AsyncHBase 框架连接 HBase，通过 Scan 的方式将数据点扫描出来。整个同步的过程通过 metric 和时间段进行切分，即某个 metric 在某一个小时内的数据迁移，组合成一个迁移 Task。\n3 功能说明 3.1 配置样例 配置一个从 OpenTSDB 数据库同步抽取数据到本地的作业： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;opentsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:4242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 03:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } } } } 3.2 参数说明 name\n描述：本插件的名称 必选：是 默认值：opentsdbreader parameter","title":"OpenTSDBReader 插件文档"},{"content":"OracleReader 插件文档 1 快速介绍 OracleReader插件实现了从Oracle读取数据。在底层实现上，OracleReader通过JDBC连接远程Oracle数据库，并执行相应的sql语句将数据从Oracle库中SELECT出来。\n2 实现原理 简而言之，OracleReader通过JDBC连接器连接到远程的Oracle数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程Oracle数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，OracleReader将其拼接为SQL语句发送到Oracle数据库；对于用户配置querySql信息，Oracle直接将其发送到Oracle数据库。\n3 功能说明 3.1 配置样例 配置一个从Oracle数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度 byte/s 尽量逼近这个速度但是不高于它. // channel 表示通道数量，byte表示通道速度，如果单通道速度1MB，配置byte为1048576表示一个channel \u0026#34;byte\u0026#34;: 1048576 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //先选择record \u0026#34;record\u0026#34;: 0, //百分比 1表示100% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;oraclereader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;,\u0026#34;name\u0026#34; ], //切分主键 \u0026#34;splitPk\u0026#34;: \u0026#34;db_id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, // 是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;oraclereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;visible\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 jdbcUrl\n描述：描述的是到对端数据库的JDBC连接信息，使用JSON的数组描述，并支持一个库填写多个连接地址。之所以使用JSON数组描述连接信息，是因为阿里集团内部支持多个IP探测，如果配置了多个，OracleReader可以依次探测ip的可连接性，直到选择一个合法的IP。如果全部连接失败，OracleReader报错。 注意，jdbcUrl必须包含在connection配置单元中。对于阿里集团外部使用情况，JSON数组填写一个JDBC连接即可。\njdbcUrl按照Oracle官方规范，并可以填写连接附件控制信息。具体请参看Oracle官方文档。\n必选：是 默认值：无 username\n描述：数据源的用户名 必选：是 默认值：无 password\n描述：数据源指定用户名的密码 必选：是 默认值：无 table\n描述：所选取的需要同步的表。使用JSON的数组描述，因此支持多张表同时抽取。当配置为多张表时，用户自己需保证多张表是同一schema结构，OracleReader不予检查表是否同一逻辑表。注意，table必须包含在connection配置单元中。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。用户使用*代表默认使用所有列配置，例如[\u0026rsquo;*\u0026rsquo;]。\n支持列裁剪，即列可以挑选部分列进行导出。\n支持列换序，即列可以不按照表schema信息进行导出。\n支持常量配置，用户需要按照JSON格式: [\u0026ldquo;id\u0026rdquo;, \u0026ldquo;table\u0026rdquo;, \u0026ldquo;1\u0026rdquo;, \u0026ldquo;\u0026lsquo;bazhen.csy\u0026rsquo;\u0026rdquo;, \u0026ldquo;null\u0026rdquo;, \u0026ldquo;to_char(a + 1)\u0026rdquo;, \u0026ldquo;2.3\u0026rdquo; , \u0026ldquo;true\u0026rdquo;] id为普通列名，`table`为包含保留在的列名，1为整形数字常量，\u0026lsquo;bazhen.csy\u0026rsquo;为字符串常量，null为空指针，to_char(a + 1)为表达式，2.3为浮点数，true为布尔值。\nColumn必须显示填写，不允许为空！\n必选：是 默认值：无 splitPk\n描述：OracleReader进行数据抽取时，如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。\n推荐splitPk用户使用表主键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。\n目前splitPk仅支持整形、字符串型数据切分，不支持浮点、日期等其他类型。如果用户指定其他非支持类型，OracleReader将报错！\nsplitPk如果不填写，将视作用户不对单表进行切分，OracleReader使用单通道同步全量数据。\n必选：否 默认值：无 where\n描述：筛选条件，MysqlReader根据指定的column、table、where条件拼接SQL，并根据这个SQL进行数据抽取。在实际业务场景中，往往会选择当天的数据进行同步，可以将where条件指定为gmt_create \u0026gt; $bizdate 。注意：不可以将where条件指定为limit 10，limit不是SQL的合法where子句。\nwhere条件可以有效地进行业务增量同步。 必选：否 默认值：无 querySql\n描述：在有些业务场景下，where这一配置项不足以描述所筛选的条件，用户可以通过该配置型来自定义筛选SQL。当用户配置了这一项之后，DataX系统就会忽略table，column这些配置型，直接使用这个配置项的内容对数据进行筛选，例如需要进行多表join后同步数据，使用select a,b from table_a join table_b on table_a.id = table_b.id 当用户配置querySql时，OracleReader直接忽略table、column、where条件的配置。\n必选：否 默认值：无 fetchSize\n描述：该配置项定义了插件和数据库服务器端每次批量数据获取条数，该值决定了DataX和服务器端的网络交互次数，能够较大的提升数据抽取性能。 注意，该值过大(\u0026gt;2048)可能造成DataX进程OOM。。\n必选：否 默认值：1024 session\n描述：控制写入数据的时间格式，时区等的配置，如果表中有时间字段，配置该值以明确告知写入 oracle 的时间格式。通常配置的参数为：NLS_DATE_FORMAT,NLS_TIME_FORMAT。其配置的值为 json 格式，例如： \u0026#34;session\u0026#34;: [ \u0026#34;alter session set NLS_DATE_FORMAT=\u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;\u0026#34;, \u0026#34;alter session set NLS_TIMESTAMP_FORMAT=\u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;\u0026#34;, \u0026#34;alter session set NLS_TIMESTAMP_TZ_FORMAT=\u0026#39;yyyy-mm-dd hh24:mi:ss\u0026#39;\u0026#34;, \u0026#34;alter session set TIME_ZONE=\u0026#39;US/Pacific\u0026#39;\u0026#34; ] (注意\u0026amp;quot;是 \u0026quot; 的转义字符串)。\n* 必选：否 \u0026lt;br /\u0026gt; * 默认值：无 \u0026lt;br /\u0026gt; 3.3 类型转换 目前OracleReader支持大部分Oracle类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出OracleReader针对Oracle类型转换列表:\nDataX 内部类型 Oracle 数据类型 Long NUMBER,INTEGER,INT,SMALLINT Double NUMERIC,DECIMAL,FLOAT,DOUBLE PRECISION,REAL String LONG,CHAR,NCHAR,VARCHAR,VARCHAR2,NVARCHAR2,CLOB,NCLOB,CHARACTER,CHARACTER VARYING,CHAR VARYING,NATIONAL CHARACTER,NATIONAL CHAR,NATIONAL CHARACTER VARYING,NATIONAL CHAR VARYING,NCHAR VARYING Date TIMESTAMP,DATE Boolean bit, bool Bytes BLOB,BFILE,RAW,LONG RAW 请注意:\n除上述罗列字段类型外，其他类型均不支持。 4 性能报告 4.1 环境准备 4.1.1 数据特征 为了模拟线上真实数据，我们设计两个Oracle数据表，分别为:\n4.1.2 机器参数 执行DataX的机器参数为:\nOracle数据库机器参数为:\n4.2 测试报告 4.2.1 表1测试报告 并发任务数 DataX速度(Rec/s) DataX流量 网卡流量 DataX运行负载 DB运行负载 1 DataX 统计速度(Rec/s) DataX统计流量 网卡流量 DataX运行负载 DB运行负载 5 约束限制 5.1 主备同步数据恢复问题 主备同步问题指Oracle使用主从灾备，备库从主库不间断通过binlog恢复数据。由于主备数据同步存在一定的时间差，特别在于某些特定情况，例如网络延迟等问题，导致备库同步恢复的数据与主库有较大差别，导致从备库同步的数据不是一份当前时间的完整镜像。\n针对这个问题，我们提供了preSql功能，该功能待补充。\n5.2 一致性约束 Oracle在数据存储划分中属于RDBMS系统，对外可以提供强一致性数据查询接口。例如当一次同步任务启动运行过程中，当该库存在其他数据写入方写入数据时，OracleReader完全不会获取到写入更新数据，这是由于数据库本身的快照特性决定的。关于数据库快照特性，请参看MVCC Wikipedia\n上述是在OracleReader单线程模型下数据同步一致性的特性，由于OracleReader可以根据用户配置信息使用了并发数据抽取，因此不能严格保证数据一致性：当OracleReader根据splitPk进行数据切分后，会先后启动多个并发任务完成数据同步。由于多个并发任务相互之间不属于同一个读事务，同时多个并发任务存在时间间隔。因此这份数据并不是完整的、一致的数据快照信息。\n针对多线程的一致性快照需求，在技术上目前无法实现，只能从工程角度解决，工程化的方式存在取舍，我们提供几个解决思路给用户，用户可以自行选择：\n使用单线程同步，即不再进行数据切片。缺点是速度比较慢，但是能够很好保证一致性。\n关闭其他数据写入方，保证当前数据为静态数据，例如，锁表、关闭备库同步等等。缺点是可能影响在线业务。\n5.3 数据库编码问题 OracleReader底层使用JDBC进行数据抽取，JDBC天然适配各类编码，并在底层进行了编码转换。因此OracleReader不需用户指定编码，可以自动获取编码并转码。\n对于Oracle底层写入编码和其设定的编码不一致的混乱情况，OracleReader对此无法识别，对此也无法提供解决方案，对于这类情况，导出有可能为乱码。\n5.4 增量数据同步 OracleReader使用JDBC SELECT语句完成数据抽取工作，因此可以使用SELECT\u0026hellip;WHERE\u0026hellip;进行增量数据抽取，方式有多种：\n数据库在线应用写入数据库时，填充modify字段为更改时间戳，包括新增、更新、删除(逻辑删)。对于这类应用，OracleReader只需要WHERE条件跟上一同步阶段时间戳即可。 对于新增流水型数据，OracleReader可以WHERE条件后跟上一阶段最大自增ID即可。 对于业务上无字段区分新增、修改数据情况，OracleReader也无法进行增量数据同步，只能同步全量数据。\n5.5 Sql安全性 OracleReader提供querySql语句交给用户自己实现SELECT抽取语句，OracleReader本身对querySql不做任何安全性校验。这块交由DataX用户方自己保证。\n6 FAQ Q: OracleReader同步报错，报错信息为XXX\nA: 网络或者权限问题，请使用Oracle命令行测试： sqlplus username/password@//host:port/sid\n如果上述命令也报错，那可以证实是环境问题，请联系你的DBA。\nQ: OracleReader抽取速度很慢怎么办？\nA: 影响抽取时间的原因大概有如下几个：(来自专业 DBA 卫绾)\n由于SQL的plan异常，导致的抽取时间长； 在抽取时，尽可能使用全表扫描代替索引扫描; 合理sql的并发度，减少抽取时间；根据表的大小， \u0026lt;50G可以不用并发， \u0026lt;100G添加如下hint: parallel(a,2）, 100G添加如下hint : parallel(a,4);\n抽取sql要简单，尽量不用replace等函数，这个非常消耗cpu，会严重影响抽取速度; ","permalink":"http://121.199.2.5:6080/0ffdd59e828b4bb0a53cef538910db12/","summary":"OracleReader 插件文档 1 快速介绍 OracleReader插件实现了从Oracle读取数据。在底层实现上，OracleReader通过JDBC连接远程Oracle数据库，并执行相应的sql语句将数据从Oracle库中SELECT出来。\n2 实现原理 简而言之，OracleReader通过JDBC连接器连接到远程的Oracle数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程Oracle数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，OracleReader将其拼接为SQL语句发送到Oracle数据库；对于用户配置querySql信息，Oracle直接将其发送到Oracle数据库。\n3 功能说明 3.1 配置样例 配置一个从Oracle数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度 byte/s 尽量逼近这个速度但是不高于它. // channel 表示通道数量，byte表示通道速度，如果单通道速度1MB，配置byte为1048576表示一个channel \u0026#34;byte\u0026#34;: 1048576 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //先选择record \u0026#34;record\u0026#34;: 0, //百分比 1表示100% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;oraclereader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;,\u0026#34;name\u0026#34; ], //切分主键 \u0026#34;splitPk\u0026#34;: \u0026#34;db_id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, // 是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;oraclereader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;visible\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.","title":"OracleReader 插件文档"},{"content":"OTSReader 插件文档 1 快速介绍 OTSReader插件实现了从OTS读取数据，并可以通过用户指定抽取数据范围可方便的实现数据增量抽取的需求。目前支持三种抽取方式：\n全表抽取 范围抽取 指定分片抽取 OTS是构建在阿里云飞天分布式系统之上的 NoSQL数据库服务，提供海量结构化数据的存储和实时访问。OTS 以实例和表的形式组织数据，通过数据分片和负载均衡技术，实现规模上的无缝扩展。\n2 实现原理 简而言之，OTSReader通过OTS官方Java SDK连接到OTS服务端，获取并按照DataX官方协议标准转为DataX字段信息传递给下游Writer端。\nOTSReader会根据OTS的表范围，按照Datax并发的数目N，将范围等分为N份Task。每个Task都会有一个OTSReader线程来执行。\n3 功能说明 3.1 配置样例 配置一个从OTS全表同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;otsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { /* ----------- 必填 --------------*/ \u0026#34;endpoint\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessId\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;instanceName\u0026#34;:\u0026#34;\u0026#34;, // 导出数据表的表名 \u0026#34;table\u0026#34;:\u0026#34;\u0026#34;, // 需要导出的列名，支持重复列和常量列，区分大小写 // 常量列：类型支持STRING，INT，DOUBLE，BOOL和BINARY // 备注：BINARY需要通过Base64转换为对应的字符串传入插件 \u0026#34;column\u0026#34;:[ {\u0026#34;name\u0026#34;:\u0026#34;col1\u0026#34;}, // 普通列 {\u0026#34;name\u0026#34;:\u0026#34;col2\u0026#34;}, // 普通列 {\u0026#34;name\u0026#34;:\u0026#34;col3\u0026#34;}, // 普通列 {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;bazhen\u0026#34;}, // 常量列(字符串) {\u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(整形) {\u0026#34;type\u0026#34;:\u0026#34;DOUBLE\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(浮点) {\u0026#34;type\u0026#34;:\u0026#34;BOOL\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(布尔) {\u0026#34;type\u0026#34;:\u0026#34;BINARY\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;Base64(bin)\u0026#34;} // 常量列(二进制),使用Base64编码完成 ], \u0026#34;range\u0026#34;:{ // 导出数据的起始范围 // 支持INF_MIN, INF_MAX, STRING, INT \u0026#34;begin\u0026#34;:[ {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;}, ], // 导出数据的结束范围 // 支持INF_MIN, INF_MAX, STRING, INT \u0026#34;end\u0026#34;:[ {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;}, ] } } }, \u0026#34;writer\u0026#34;: {} } ] } } 配置一个定义抽取范围的OTSReader： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;byte\u0026#34;:10485760 }, \u0026#34;errorLimit\u0026#34;:0.0 }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;otsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessId\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;instanceName\u0026#34;:\u0026#34;\u0026#34;, // 导出数据表的表名 \u0026#34;table\u0026#34;:\u0026#34;\u0026#34;, // 需要导出的列名，支持重复类和常量列，区分大小写 // 常量列：类型支持STRING，INT，DOUBLE，BOOL和BINARY // 备注：BINARY需要通过Base64转换为对应的字符串传入插件 \u0026#34;column\u0026#34;:[ {\u0026#34;name\u0026#34;:\u0026#34;col1\u0026#34;}, // 普通列 {\u0026#34;name\u0026#34;:\u0026#34;col2\u0026#34;}, // 普通列 {\u0026#34;name\u0026#34;:\u0026#34;col3\u0026#34;}, // 普通列 {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;,\u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(字符串) {\u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;,\u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(整形) {\u0026#34;type\u0026#34;:\u0026#34;DOUBLE\u0026#34;,\u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(浮点) {\u0026#34;type\u0026#34;:\u0026#34;BOOL\u0026#34;,\u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(布尔) {\u0026#34;type\u0026#34;:\u0026#34;BINARY\u0026#34;,\u0026#34;value\u0026#34; : \u0026#34;Base64(bin)\u0026#34;} // 常量列(二进制) ], \u0026#34;range\u0026#34;:{ // 导出数据的起始范围 // 支持INF_MIN, INF_MAX, STRING, INT \u0026#34;begin\u0026#34;:[ {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;hello\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;2999\u0026#34;}, ], // 导出数据的结束范围 // 支持INF_MIN, INF_MAX, STRING, INT \u0026#34;end\u0026#34;:[ {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;hello\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;2999\u0026#34;}, ] } } }, \u0026#34;writer\u0026#34;: {} } ] } } 3.2 参数说明 endpoint\n描述：OTS Server的EndPoint地址，例如http://bazhen.cn−hangzhou.ots.aliyuncs.com。\n必选：是 默认值：无 accessId\n描述：OTS的accessId 必选：是 默认值：无 accessKey\n描述：OTS的accessKey 必选：是 默认值：无 instanceName\n描述：OTS的实例名称，实例是用户使用和管理 OTS 服务的实体，用户在开通 OTS 服务之后，需要通过管理控制台来创建实例，然后在实例内进行表的创建和管理。实例是 OTS 资源管理的基础单元，OTS 对应用程序的访问控制和资源计量都在实例级别完成。 必选：是 默认值：无 table\n描述：所选取的需要抽取的表名称，这里有且只能填写一张表。在OTS不存在多表同步的需求。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。由于OTS本身是NoSQL系统，在OTSReader抽取数据过程中，必须指定相应地字段名称。\n支持普通的列读取，例如: {\u0026ldquo;name\u0026rdquo;:\u0026ldquo;col1\u0026rdquo;}\n支持部分列读取，如用户不配置该列，则OTSReader不予读取。\n支持常量列读取，例如: {\u0026ldquo;type\u0026rdquo;:\u0026ldquo;STRING\u0026rdquo;, \u0026ldquo;value\u0026rdquo; : \u0026ldquo;DataX\u0026rdquo;}。使用type描述常量类型，目前支持STRING、INT、DOUBLE、BOOL、BINARY(用户使用Base64编码填写)、INF_MIN(OTS的系统限定最小值，使用该值用户不能填写value属性，否则报错)、INF_MAX(OTS的系统限定最大值，使用该值用户不能填写value属性，否则报错)。\n不支持函数或者自定义表达式，由于OTS本身不提供类似SQL的函数或者表达式功能，OTSReader也不能提供函数或表达式列功能。\n必选：是 默认值：无 begin/end\n描述：该配置项必须配对使用，用于支持OTS表范围抽取。begin/end中描述的是OTS PrimaryKey的区间分布状态，而且必须保证区间覆盖到所有的PrimaryKey，需要指定该表下所有的PrimaryKey范围，不能遗漏任意一个PrimaryKey，对于无限大小的区间，可以使用{\u0026ldquo;type\u0026rdquo;:\u0026ldquo;INF_MIN\u0026rdquo;}，{\u0026ldquo;type\u0026rdquo;:\u0026ldquo;INF_MAX\u0026rdquo;}指代。例如对一张主键为 [DeviceID, SellerID]的OTS进行抽取任务，begin/end可以配置为: \u0026#34;range\u0026#34;: { \u0026#34;begin\u0026#34;: { {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;}, //指定deviceID最小值 {\u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;0\u0026#34;} //指定deviceID最小值 }, \u0026#34;end\u0026#34;: { {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;}, //指定deviceID抽取最大值 {\u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;9999\u0026#34;} //指定deviceID抽取最大值 } } 如果要对上述表抽取全表，可以使用如下配置： \u0026#34;range\u0026#34;: { \u0026#34;begin\u0026#34;: [ {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;}, //指定deviceID最小值 {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;} //指定SellerID最小值 ], \u0026#34;end\u0026#34;: [ {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;}, //指定deviceID抽取最大值 {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;} //指定SellerID抽取最大值 ] } 必选：是 默认值：空 split\n描述：该配置项属于高级配置项，是用户自己定义切分配置信息，普通情况下不建议用户使用。适用场景通常在OTS数据存储发生热点，使用OTSReader自动切分的策略不能生效情况下，使用用户自定义的切分规则。split指定是的在Begin、End区间内的切分点，且只能是partitionKey的切分点信息，即在split仅配置partitionKey，而不需要指定全部的PrimaryKey。\n例如对一张主键为 [DeviceID, SellerID]的OTS进行抽取任务，可以配置为:\n\u0026#34;range\u0026#34;: { \u0026#34;begin\u0026#34;: { {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;}, //指定deviceID最小值 {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;} //指定deviceID最小值 }, \u0026#34;end\u0026#34;: { {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;}, //指定deviceID抽取最大值 {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;} //指定deviceID抽取最大值 }， // 用户指定的切分点，如果指定了切分点，Job将按照begin、end和split进行Task的切分， // 切分的列只能是Partition Key（ParimaryKey的第一列） // 支持INF_MIN, INF_MAX, STRING, INT \u0026#34;split\u0026#34;:[ {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;1\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;2\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;3\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;4\u0026#34;}, {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;5\u0026#34;} ] } 必选：否 默认值：无 3.3 类型转换 目前OTSReader支持所有OTS类型，下面列出OTSReader针对OTS类型转换列表:\nDataX 内部类型 OTS 数据类型 Long Integer Double Double String String Boolean Boolean Bytes Binary 注意，OTS本身不支持日期型类型。应用层一般使用Long报错时间的Unix TimeStamp。 4 性能报告 4.1 环境准备 4.1.1 数据特征 15列String(10 Byte), 2两列Integer(8 Byte)，总计168Byte/r。\n4.1.2 机器参数 OTS端：3台前端机，5台后端机\nDataX运行端: 24核CPU， 98GB内存\n4.1.3 DataX jvm 参数 -Xms1024m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError 4.2 测试报告 4.2.1 测试报告 并发数 DataX CPU OTS 流量 DATAX流量 前端QPS 前端延时 2 36% 6.3M/s 12739 rec/s 4.7 308ms 11 155% 32M/s 60732 rec/s 23.9 412ms 50 377% 73M/s 145139 rec/s 54 874ms 100 448% 82M/s 156262 rec/s 60 1570ms 5 约束限制 5.1 一致性约束 OTS是类BigTable的存储系统，OTS本身能够保证单行写事务性，无法提供跨行级别的事务。对于OTSReader而言也无法提供全表的一致性视图。例如对于OTSReader在0点启动的数据同步任务，在整个表数据同步过程中，OTSReader同样会抽取到后续更新的数据，无法提供准确的0点时刻该表一致性视图。\n5.2 增量数据同步 OTS本质上KV存储，目前只能针对PK进行范围查询，暂不支持按照字段范围抽取数据。因此只能对于增量查询，如果PK能够表示范围信息，例如自增ID，或者时间戳。\n自增ID，OTSReader可以通过记录上次最大的ID信息，通过指定Range范围进行增量抽取。这样使用的前提是OTS中的PrimaryKey必须包含主键自增列(自增主键需要使用OTS应用方生成。)\n时间戳，\tOTSReader可以通过PK过滤时间戳，通过制定Range范围进行增量抽取。这样使用的前提是OTS中的PrimaryKey必须包含主键时间列(时间主键需要使用OTS应用方生成。)\n6 FAQ ","permalink":"http://121.199.2.5:6080/2dc5ba5f4d58498e8453bd2d8efde079/","summary":"OTSReader 插件文档 1 快速介绍 OTSReader插件实现了从OTS读取数据，并可以通过用户指定抽取数据范围可方便的实现数据增量抽取的需求。目前支持三种抽取方式：\n全表抽取 范围抽取 指定分片抽取 OTS是构建在阿里云飞天分布式系统之上的 NoSQL数据库服务，提供海量结构化数据的存储和实时访问。OTS 以实例和表的形式组织数据，通过数据分片和负载均衡技术，实现规模上的无缝扩展。\n2 实现原理 简而言之，OTSReader通过OTS官方Java SDK连接到OTS服务端，获取并按照DataX官方协议标准转为DataX字段信息传递给下游Writer端。\nOTSReader会根据OTS的表范围，按照Datax并发的数目N，将范围等分为N份Task。每个Task都会有一个OTSReader线程来执行。\n3 功能说明 3.1 配置样例 配置一个从OTS全表同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;otsreader\u0026#34;, \u0026#34;parameter\u0026#34;: { /* ----------- 必填 --------------*/ \u0026#34;endpoint\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessId\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;instanceName\u0026#34;:\u0026#34;\u0026#34;, // 导出数据表的表名 \u0026#34;table\u0026#34;:\u0026#34;\u0026#34;, // 需要导出的列名，支持重复列和常量列，区分大小写 // 常量列：类型支持STRING，INT，DOUBLE，BOOL和BINARY // 备注：BINARY需要通过Base64转换为对应的字符串传入插件 \u0026#34;column\u0026#34;:[ {\u0026#34;name\u0026#34;:\u0026#34;col1\u0026#34;}, // 普通列 {\u0026#34;name\u0026#34;:\u0026#34;col2\u0026#34;}, // 普通列 {\u0026#34;name\u0026#34;:\u0026#34;col3\u0026#34;}, // 普通列 {\u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;bazhen\u0026#34;}, // 常量列(字符串) {\u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(整形) {\u0026#34;type\u0026#34;:\u0026#34;DOUBLE\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(浮点) {\u0026#34;type\u0026#34;:\u0026#34;BOOL\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;\u0026#34;}, // 常量列(布尔) {\u0026#34;type\u0026#34;:\u0026#34;BINARY\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;Base64(bin)\u0026#34;} // 常量列(二进制),使用Base64编码完成 ], \u0026#34;range\u0026#34;:{ // 导出数据的起始范围 // 支持INF_MIN, INF_MAX, STRING, INT \u0026#34;begin\u0026#34;:[ {\u0026#34;type\u0026#34;:\u0026#34;INF_MIN\u0026#34;}, ], // 导出数据的结束范围 // 支持INF_MIN, INF_MAX, STRING, INT \u0026#34;end\u0026#34;:[ {\u0026#34;type\u0026#34;:\u0026#34;INF_MAX\u0026#34;}, ] } } }, \u0026#34;writer\u0026#34;: {} } ] } } 配置一个定义抽取范围的OTSReader： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;byte\u0026#34;:10485760 }, \u0026#34;errorLimit\u0026#34;:0.","title":"OTSReader 插件文档"},{"content":"OTSWriter 插件文档 1 快速介绍 OTSWriter插件实现了向OTS写入数据，目前支持三种写入方式：\nPutRow，对应于OTS API PutRow，插入数据到指定的行，如果该行不存在，则新增一行；若该行存在，则覆盖原有行。\nUpdateRow，对应于OTS API UpdateRow，更新指定行的数据，如果该行不存在，则新增一行；若该行存在，则根据请求的内容在这一行中新增、修改或者删除指定列的值。\nDeleteRow，对应于OTS API DeleteRow，删除指定行的数据。\nOTS是构建在阿里云飞天分布式系统之上的 NoSQL数据库服务，提供海量结构化数据的存储和实时访问。OTS 以实例和表的形式组织数据，通过数据分片和负载均衡技术，实现规模上的无缝扩展。\n2 实现原理 简而言之，OTSWriter通过OTS官方Java SDK连接到OTS服务端，并通过SDK写入OTS服务端。OTSWriter本身对于写入过程做了很多优化，包括写入超时重试、异常写入重试、批量提交等Feature。\n3 功能说明 3.1 配置样例 配置一个写入OTS作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: {}, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;otswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessId\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;instanceName\u0026#34;:\u0026#34;\u0026#34;, // 导出数据表的表名 \u0026#34;table\u0026#34;:\u0026#34;\u0026#34;, // Writer支持不同类型之间进行相互转换 // 如下类型转换不支持: // ================================ // int -\u0026gt; binary // double -\u0026gt; bool, binary // bool -\u0026gt; binary // bytes -\u0026gt; int, double, bool // ================================ // 需要导入的PK列名，区分大小写 // 类型支持：STRING，INT // 1. 支持类型转换，注意类型转换时的精度丢失 // 2. 顺序不要求和表的Meta一致 \u0026#34;primaryKey\u0026#34; : [ {\u0026#34;name\u0026#34;:\u0026#34;pk1\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;pk2\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;int\u0026#34;} ], // 需要导入的列名，区分大小写 // 类型支持STRING，INT，DOUBLE，BOOL和BINARY \u0026#34;column\u0026#34; : [ {\u0026#34;name\u0026#34;:\u0026#34;col2\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;col3\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;col4\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;col5\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;BINARY\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;col6\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;DOUBLE\u0026#34;} ], // 写入OTS的方式 // PutRow : 等同于OTS API中PutRow操作，检查条件是ignore // UpdateRow : 等同于OTS API中UpdateRow操作，检查条件是ignore // DeleteRow: 等同于OTS API中DeleteRow操作，检查条件是ignore \u0026#34;writeMode\u0026#34; : \u0026#34;PutRow\u0026#34; } } } ] } } 3.2 参数说明 endpoint\n描述：OTS Server的EndPoint(服务地址)，例如http://bazhen.cn−hangzhou.ots.aliyuncs.com。\n必选：是 默认值：无 accessId\n描述：OTS的accessId 必选：是 默认值：无 accessKey\n描述：OTS的accessKey 必选：是 默认值：无 instanceName\n描述：OTS的实例名称，实例是用户使用和管理 OTS 服务的实体，用户在开通 OTS 服务之后，需要通过管理控制台来创建实例，然后在实例内进行表的创建和管理。实例是 OTS 资源管理的基础单元，OTS 对应用程序的访问控制和资源计量都在实例级别完成。 必选：是 默认值：无 table\n描述：所选取的需要抽取的表名称，这里有且只能填写一张表。在OTS不存在多表同步的需求。\n必选：是 默认值：无 primaryKey\n描述: OTS的主键信息，使用JSON的数组描述字段信息。OTS本身是NoSQL系统，在OTSWriter导入数据过程中，必须指定相应地字段名称。\nOTS的PrimaryKey只能支持STRING，INT两种类型，因此OTSWriter本身也限定填写上述两种类型。\nDataX本身支持类型转换的，因此对于源头数据非String/Int，OTSWriter会进行数据类型转换。\n配置实例:\n\u0026#34;primaryKey\u0026#34; : [ {\u0026#34;name\u0026#34;:\u0026#34;pk1\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;pk2\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;int\u0026#34;} ], 必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。使用格式为\n{\u0026#34;name\u0026#34;:\u0026#34;col2\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;}, 其中的name指定写入的OTS列名，type指定写入的类型。OTS类型支持STRING，INT，DOUBLE，BOOL和BINARY几种类型 。\n写入过程不支持常量、函数或者自定义表达式。\n必选：是 默认值：无 writeMode\n描述：写入模式，目前支持两种模式，\nPutRow，对应于OTS API PutRow，插入数据到指定的行，如果该行不存在，则新增一行；若该行存在，则覆盖原有行。\nUpdateRow，对应于OTS API UpdateRow，更新指定行的数据，如果该行不存在，则新增一行；若该行存在，则根据请求的内容在这一行中新增、修改或者删除指定列的值。\nDeleteRow，对应于OTS API DeleteRow，删除指定行的数据。\n必选：是 默认值：无 3.3 类型转换 目前OTSWriter支持所有OTS类型，下面列出OTSWriter针对OTS类型转换列表:\nDataX 内部类型 OTS 数据类型 Long Integer Double Double String String Boolean Boolean Bytes Binary 注意，OTS本身不支持日期型类型。应用层一般使用Long报错时间的Unix TimeStamp。 4 性能报告 4.1 环境准备 4.1.1 数据特征 2列PK（10 + 8），15列String(10 Byte), 2两列Integer(8 Byte)，算上Column Name每行大概327Byte，每次BatchWriteRow写入100行数据，所以当个请求的数据大小是32KB。\n4.1.2 机器参数 OTS端：3台前端机，5台后端机\nDataX运行端: 24核CPU， 98GB内存\n4.2 测试报告 4.2.1 测试报告 并发数 DataX CPU DATAX流量 OTS 流量 BatchWrite前端QPS BatchWriteRow前端延时 40 1027% Speed 22.13MB/s, 112640 records/s 65.8M/s 42 153ms 50 1218% Speed 24.11MB/s, 122700 records/s 73.5M/s 47 174ms 60 1355% Speed 25.31MB/s, 128854 records/s 78.1M/s 50 190ms 70 1578% Speed 26.35MB/s, 134121 records/s 80.8M/s 52 210ms 80 1771% Speed 26.55MB/s, 135161 records/s 82.7M/s 53 230ms 5 约束限制 5.1 写入幂等性 OTS写入本身是支持幂等性的，也就是使用OTS SDK同一条数据写入OTS系统，一次和多次请求的结果可以理解为一致的。因此对于OTSWriter多次尝试写入同一条数据与写入一条数据结果是等同的。\n5.2 单任务FailOver 由于OTS写入本身是幂等性的，因此可以支持单任务FailOver。即一旦写入Fail，DataX会重新启动相关子任务进行重试。\n6 FAQ ","permalink":"http://121.199.2.5:6080/7d2496e6853044daa824d7a78226b07d/","summary":"OTSWriter 插件文档 1 快速介绍 OTSWriter插件实现了向OTS写入数据，目前支持三种写入方式：\nPutRow，对应于OTS API PutRow，插入数据到指定的行，如果该行不存在，则新增一行；若该行存在，则覆盖原有行。\nUpdateRow，对应于OTS API UpdateRow，更新指定行的数据，如果该行不存在，则新增一行；若该行存在，则根据请求的内容在这一行中新增、修改或者删除指定列的值。\nDeleteRow，对应于OTS API DeleteRow，删除指定行的数据。\nOTS是构建在阿里云飞天分布式系统之上的 NoSQL数据库服务，提供海量结构化数据的存储和实时访问。OTS 以实例和表的形式组织数据，通过数据分片和负载均衡技术，实现规模上的无缝扩展。\n2 实现原理 简而言之，OTSWriter通过OTS官方Java SDK连接到OTS服务端，并通过SDK写入OTS服务端。OTSWriter本身对于写入过程做了很多优化，包括写入超时重试、异常写入重试、批量提交等Feature。\n3 功能说明 3.1 配置样例 配置一个写入OTS作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: {}, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;otswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessId\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;accessKey\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;instanceName\u0026#34;:\u0026#34;\u0026#34;, // 导出数据表的表名 \u0026#34;table\u0026#34;:\u0026#34;\u0026#34;, // Writer支持不同类型之间进行相互转换 // 如下类型转换不支持: // ================================ // int -\u0026gt; binary // double -\u0026gt; bool, binary // bool -\u0026gt; binary // bytes -\u0026gt; int, double, bool // ================================ // 需要导入的PK列名，区分大小写 // 类型支持：STRING，INT // 1. 支持类型转换，注意类型转换时的精度丢失 // 2. 顺序不要求和表的Meta一致 \u0026#34;primaryKey\u0026#34; : [ {\u0026#34;name\u0026#34;:\u0026#34;pk1\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;pk2\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;int\u0026#34;} ], // 需要导入的列名，区分大小写 // 类型支持STRING，INT，DOUBLE，BOOL和BINARY \u0026#34;column\u0026#34; : [ {\u0026#34;name\u0026#34;:\u0026#34;col2\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;INT\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;col3\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;col4\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;STRING\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;col5\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;BINARY\u0026#34;}, {\u0026#34;name\u0026#34;:\u0026#34;col6\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;DOUBLE\u0026#34;} ], // 写入OTS的方式 // PutRow : 等同于OTS API中PutRow操作，检查条件是ignore // UpdateRow : 等同于OTS API中UpdateRow操作，检查条件是ignore // DeleteRow: 等同于OTS API中DeleteRow操作，检查条件是ignore \u0026#34;writeMode\u0026#34; : \u0026#34;PutRow\u0026#34; } } } ] } } 3.","title":"OTSWriter 插件文档"},{"content":"PostgresqlReader 插件文档 1 快速介绍 PostgresqlReader插件实现了从PostgreSQL读取数据。在底层实现上，PostgresqlReader通过JDBC连接远程PostgreSQL数据库，并执行相应的sql语句将数据从PostgreSQL库中SELECT出来。\n2 实现原理 简而言之，PostgresqlReader通过JDBC连接器连接到远程的PostgreSQL数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程PostgreSQL数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，PostgresqlReader将其拼接为SQL语句发送到PostgreSQL数据库；对于用户配置querySql信息，PostgresqlReader直接将其发送到PostgreSQL数据库。\n3 功能说明 3.1 配置样例 配置一个从PostgreSQL数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;: 1048576 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;postgresqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;，\u0026#34;name\u0026#34; ], //切分主键 \u0026#34;splitPk\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:postgresql://host:port/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: 1048576 }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;postgresqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:postgresql://host:port/database\u0026#34;, \u0026#34;jdbc:postgresql://host:port/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 jdbcUrl\n描述：描述的是到对端数据库的JDBC连接信息，使用JSON的数组描述，并支持一个库填写多个连接地址。之所以使用JSON数组描述连接信息，是因为阿里集团内部支持多个IP探测，如果配置了多个，PostgresqlReader可以依次探测ip的可连接性，直到选择一个合法的IP。如果全部连接失败，PostgresqlReader报错。 注意，jdbcUrl必须包含在connection配置单元中。对于阿里集团外部使用情况，JSON数组填写一个JDBC连接即可。\njdbcUrl按照PostgreSQL官方规范，并可以填写连接附件控制信息。具体请参看PostgreSQL官方文档。\n必选：是 默认值：无 username\n描述：数据源的用户名 必选：是 默认值：无 password\n描述：数据源指定用户名的密码 必选：是 默认值：无 table\n描述：所选取的需要同步的表。使用JSON的数组描述，因此支持多张表同时抽取。当配置为多张表时，用户自己需保证多张表是同一schema结构，PostgresqlReader不予检查表是否同一逻辑表。注意，table必须包含在connection配置单元中。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。用户使用*代表默认使用所有列配置，例如[\u0026rsquo;*\u0026rsquo;]。\n支持列裁剪，即列可以挑选部分列进行导出。\n支持列换序，即列可以不按照表schema信息进行导出。\n支持常量配置，用户需要按照PostgreSQL语法格式: [\u0026ldquo;id\u0026rdquo;, \u0026ldquo;\u0026lsquo;hello\u0026rsquo;::varchar\u0026rdquo;, \u0026ldquo;true\u0026rdquo;, \u0026ldquo;2.5::real\u0026rdquo;, \u0026ldquo;power(2,3)\u0026rdquo;] id为普通列名，\u0026lsquo;hello\u0026rsquo;::varchar为字符串常量，true为布尔值，2.5为浮点数, power(2,3)为函数。\ncolumn必须用户显示指定同步的列集合，不允许为空！\n必选：是 默认值：无 splitPk\n描述：PostgresqlReader进行数据抽取时，如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。\n推荐splitPk用户使用表主键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。\n目前splitPk仅支持整形数据切分，不支持浮点、字符串型、日期等其他类型。如果用户指定其他非支持类型，PostgresqlReader将报错！\nsplitPk设置为空，底层将视作用户不允许对单表进行切分，因此使用单通道进行抽取。\n必选：否 默认值：空 where\n描述：筛选条件，MysqlReader根据指定的column、table、where条件拼接SQL，并根据这个SQL进行数据抽取。在实际业务场景中，往往会选择当天的数据进行同步，可以将where条件指定为gmt_create \u0026gt; $bizdate 。注意：不可以将where条件指定为limit 10，limit不是SQL的合法where子句。\nwhere条件可以有效地进行业务增量同步。\twhere条件不配置或者为空，视作全表同步数据。 必选：否 默认值：无 querySql\n描述：在有些业务场景下，where这一配置项不足以描述所筛选的条件，用户可以通过该配置型来自定义筛选SQL。当用户配置了这一项之后，DataX系统就会忽略table，column这些配置型，直接使用这个配置项的内容对数据进行筛选，例如需要进行多表join后同步数据，使用select a,b from table_a join table_b on table_a.id = table_b.id 当用户配置querySql时，PostgresqlReader直接忽略table、column、where条件的配置。\n必选：否 默认值：无 fetchSize\n描述：该配置项定义了插件和数据库服务器端每次批量数据获取条数，该值决定了DataX和服务器端的网络交互次数，能够较大的提升数据抽取性能。 注意，该值过大(\u0026gt;2048)可能造成DataX进程OOM。。\n必选：否 默认值：1024 3.3 类型转换 目前PostgresqlReader支持大部分PostgreSQL类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出PostgresqlReader针对PostgreSQL类型转换列表:\nDataX 内部类型 PostgreSQL 数据类型 Long bigint, bigserial, integer, smallint, serial Double double precision, money, numeric, real String varchar, char, text, bit, inet Date date, time, timestamp Boolean bool Bytes bytea 请注意:\n除上述罗列字段类型外，其他类型均不支持; money,inet,bit需用户使用a_inet::varchar类似的语法转换。 4 性能报告 4.1 环境准备 4.1.1 数据特征 建表语句：\ncreate table pref_test( id serial, a_bigint bigint, a_bit bit(10), a_boolean boolean, a_char character(5), a_date date, a_double double precision, a_integer integer, a_money money, a_num numeric(10,2), a_real real, a_smallint smallint, a_text text, a_time time, a_timestamp timestamp )\n4.1.2 机器参数 执行DataX的机器参数为:\ncpu: 16核 Intel(R) Xeon(R) CPU E5620 @ 2.40GHz mem: MemTotal: 24676836kB MemFree: 6365080kB net: 百兆双网卡 PostgreSQL数据库机器参数为: D12 24逻辑核 192G内存 12*480G SSD 阵列\n4.2 测试报告 4.2.1 单表测试报告 通道数 是否按照主键切分 DataX速度(Rec/s) DataX流量(MB/s) DataX机器运行负载 1 否 10211 0.63 0.2 1 是 10211 0.63 0.2 4 否 10211 0.63 0.2 4 是 40000 2.48 0.5 8 否 10211 0.63 0.2 8 是 78048 4.84 0.8 说明：\n这里的单表，主键类型为 serial，数据分布均匀。 对单表如果没有按照主键切分，那么配置通道个数不会提升速度，效果与1个通道一样。 ","permalink":"http://121.199.2.5:6080/0adf103bd70440a1978ecbcc7dd63787/","summary":"PostgresqlReader 插件文档 1 快速介绍 PostgresqlReader插件实现了从PostgreSQL读取数据。在底层实现上，PostgresqlReader通过JDBC连接远程PostgreSQL数据库，并执行相应的sql语句将数据从PostgreSQL库中SELECT出来。\n2 实现原理 简而言之，PostgresqlReader通过JDBC连接器连接到远程的PostgreSQL数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程PostgreSQL数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，PostgresqlReader将其拼接为SQL语句发送到PostgreSQL数据库；对于用户配置querySql信息，PostgresqlReader直接将其发送到PostgreSQL数据库。\n3 功能说明 3.1 配置样例 配置一个从PostgreSQL数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. \u0026#34;byte\u0026#34;: 1048576 }, //出错限制 \u0026#34;errorLimit\u0026#34;: { //出错的record条数上限，当大于该值即报错。 \u0026#34;record\u0026#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% \u0026#34;percentage\u0026#34;: 0.02 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;postgresqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34;，\u0026#34;name\u0026#34; ], //切分主键 \u0026#34;splitPk\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:postgresql://host:port/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { //writer类型 \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, //是否打印内容 \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: 1048576 }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;postgresqlreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xx\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:postgresql://host:port/database\u0026#34;, \u0026#34;jdbc:postgresql://host:port/database\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.","title":"PostgresqlReader 插件文档"},{"content":"RDBMSReader 插件文档 1 快速介绍 RDBMSReader插件实现了从RDBMS读取数据。在底层实现上，RDBMSReader通过JDBC连接远程RDBMS数据库，并执行相应的sql语句将数据从RDBMS库中SELECT出来。目前支持达梦、db2、PPAS、Sybase数据库的读取。RDBMSReader是一个通用的关系数据库读插件，您可以通过注册数据库驱动等方式增加任意多样的关系数据库读支持。\n2 实现原理 简而言之，RDBMSReader通过JDBC连接器连接到远程的RDBMS数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程RDBMS数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，RDBMSReader将其拼接为SQL语句发送到RDBMS数据库；对于用户配置querySql信息，RDBMS直接将其发送到RDBMS数据库。\n3 功能说明 3.1 配置样例 配置一个从RDBMS数据库同步抽取数据作业: {\r\u0026#34;job\u0026#34;: {\r\u0026#34;setting\u0026#34;: {\r\u0026#34;speed\u0026#34;: {\r\u0026#34;byte\u0026#34;: 1048576\r},\r\u0026#34;errorLimit\u0026#34;: {\r\u0026#34;record\u0026#34;: 0,\r\u0026#34;percentage\u0026#34;: 0.02\r}\r},\r\u0026#34;content\u0026#34;: [\r{\r\u0026#34;reader\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;rdbmsreader\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;username\u0026#34;: \u0026#34;xxx\u0026#34;,\r\u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;,\r\u0026#34;column\u0026#34;: [\r\u0026#34;id\u0026#34;,\r\u0026#34;name\u0026#34;\r],\r\u0026#34;splitPk\u0026#34;: \u0026#34;pk\u0026#34;,\r\u0026#34;connection\u0026#34;: [\r{\r\u0026#34;table\u0026#34;: [\r\u0026#34;table\u0026#34;\r],\r\u0026#34;jdbcUrl\u0026#34;: [\r\u0026#34;jdbc:dm://ip:port/database\u0026#34;\r]\r}\r],\r\u0026#34;fetchSize\u0026#34;: 1024,\r\u0026#34;where\u0026#34;: \u0026#34;1 = 1\u0026#34;\r}\r},\r\u0026#34;writer\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;print\u0026#34;: true\r}\r}\r}\r]\r}\r} 配置一个自定义SQL的数据库同步任务到ODPS的作业： {\r\u0026#34;job\u0026#34;: {\r\u0026#34;setting\u0026#34;: {\r\u0026#34;speed\u0026#34;: {\r\u0026#34;byte\u0026#34;: 1048576\r},\r\u0026#34;errorLimit\u0026#34;: {\r\u0026#34;record\u0026#34;: 0,\r\u0026#34;percentage\u0026#34;: 0.02\r}\r},\r\u0026#34;content\u0026#34;: [\r{\r\u0026#34;reader\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;rdbmsreader\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;username\u0026#34;: \u0026#34;xxx\u0026#34;,\r\u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;,\r\u0026#34;column\u0026#34;: [\r\u0026#34;id\u0026#34;,\r\u0026#34;name\u0026#34;\r],\r\u0026#34;splitPk\u0026#34;: \u0026#34;pk\u0026#34;,\r\u0026#34;connection\u0026#34;: [\r{\r\u0026#34;querySql\u0026#34;: [\r\u0026#34;SELECT * from dual\u0026#34;\r],\r\u0026#34;jdbcUrl\u0026#34;: [\r\u0026#34;jdbc:dm://ip:port/database\u0026#34;\r]\r}\r],\r\u0026#34;fetchSize\u0026#34;: 1024,\r\u0026#34;where\u0026#34;: \u0026#34;1 = 1\u0026#34;\r}\r},\r\u0026#34;writer\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;print\u0026#34;: true\r}\r}\r}\r]\r}\r} 3.2 参数说明 jdbcUrl\n描述：描述的是到对端数据库的JDBC连接信息，jdbcUrl按照RDBMS官方规范，并可以填写连接附件控制信息。请注意不同的数据库jdbc的格式是不同的，DataX会根据具体jdbc的格式选择合适的数据库驱动完成数据读取。\n达梦 jdbc:dm://ip:port/database db2格式 jdbc:db2://ip:port/database PPAS格式 jdbc:edb://ip:port/database rdbmswriter如何增加新的数据库支持:\n进入rdbmsreader对应目录，这里${DATAX_HOME}为DataX主目录，即: ${DATAX_HOME}/plugin/reader/rdbmswriter 在rdbmsreader插件目录下有plugin.json配置文件，在此文件中注册您具体的数据库驱动，具体放在drivers数组中。rdbmsreader插件在任务执行时会动态选择合适的数据库驱动连接数据库。 {\r\u0026#34;name\u0026#34;: \u0026#34;rdbmsreader\u0026#34;,\r\u0026#34;class\u0026#34;: \u0026#34;com.alibaba.datax.plugin.reader.rdbmsreader.RdbmsReader\u0026#34;,\r\u0026#34;description\u0026#34;: \u0026#34;useScene: prod. mechanism: Jdbc connection using the database, execute select sql, retrieve data from the ResultSet. warn: The more you know about the database, the less problems you encounter.\u0026#34;,\r\u0026#34;developer\u0026#34;: \u0026#34;alibaba\u0026#34;,\r\u0026#34;drivers\u0026#34;: [\r\u0026#34;dm.jdbc.driver.DmDriver\u0026#34;,\r\u0026#34;com.ibm.db2.jcc.DB2Driver\u0026#34;,\r\u0026#34;com.sybase.jdbc3.jdbc.SybDriver\u0026#34;,\r\u0026#34;com.edb.Driver\u0026#34;\r]\r} 在rdbmsreader插件目录下有libs子目录，您需要将您具体的数据库驱动放到libs目录下。 $tree\r.\r|-- libs\r| |-- Dm7JdbcDriver16.jar\r| |-- commons-collections-3.0.jar\r| |-- commons-io-2.4.jar\r| |-- commons-lang3-3.3.2.jar\r| |-- commons-math3-3.1.1.jar\r| |-- datax-common-0.0.1-SNAPSHOT.jar\r| |-- datax-service-face-1.0.23-20160120.024328-1.jar\r| |-- db2jcc4.jar\r| |-- druid-1.0.15.jar\r| |-- edb-jdbc16.jar\r| |-- fastjson-1.1.46.sec01.jar\r| |-- guava-r05.jar\r| |-- hamcrest-core-1.3.jar\r| |-- jconn3-1.0.0-SNAPSHOT.jar\r| |-- logback-classic-1.0.13.jar\r| |-- logback-core-1.0.13.jar\r| |-- plugin-rdbms-util-0.0.1-SNAPSHOT.jar\r| `-- slf4j-api-1.7.10.jar\r|-- plugin.json\r|-- plugin_job_template.json\r`-- rdbmsreader-0.0.1-SNAPSHOT.jar 必选：是 默认值：无 username\n描述：数据源的用户名。 必选：是 默认值：无 password\n描述：数据源指定用户名的密码。 必选：是 默认值：无 table\n描述：所选取的需要同步的表名。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。用户使用代表默认使用所有列配置，例如[\u0026rsquo;\u0026rsquo;]。\n支持列裁剪，即列可以挑选部分列进行导出。\n支持列换序，即列可以不按照表schema信息进行导出。\n支持常量配置，用户需要按照JSON格式: [\u0026ldquo;id\u0026rdquo;, \u0026ldquo;1\u0026rdquo;, \u0026ldquo;\u0026lsquo;bazhen.csy\u0026rsquo;\u0026rdquo;, \u0026ldquo;null\u0026rdquo;, \u0026ldquo;to_char(a + 1)\u0026rdquo;, \u0026ldquo;2.3\u0026rdquo; , \u0026ldquo;true\u0026rdquo;] id为普通列名，1为整形数字常量，\u0026lsquo;bazhen.csy\u0026rsquo;为字符串常量，null为空指针，to_char(a + 1)为表达式，2.3为浮点数，true为布尔值。\nColumn必须显示填写，不允许为空！\n必选：是 默认值：无 splitPk\n描述：RDBMSReader进行数据抽取时，如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。\n推荐splitPk用户使用表主键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。\n目前splitPk仅支持整形数据切分，不支持浮点、字符串型、日期等其他类型。如果用户指定其他非支持类型，RDBMSReader将报错！\nsplitPk如果不填写，将视作用户不对单表进行切分，RDBMSReader使用单通道同步全量数据。\n必选：否 默认值：空 where\n描述：筛选条件，RDBMSReader根据指定的column、table、where条件拼接SQL，并根据这个SQL进行数据抽取。例如在做测试时，可以将where条件指定为limit 10；在实际业务场景中，往往会选择当天的数据进行同步，可以将where条件指定为gmt_create \u0026gt; $bizdate 。。\nwhere条件可以有效地进行业务增量同步。where条件不配置或者为空，视作全表同步数据。\r必选：否 默认值：无 querySql\n描述：在有些业务场景下，where这一配置项不足以描述所筛选的条件，用户可以通过该配置型来自定义筛选SQL。当用户配置了这一项之后，DataX系统就会忽略table，column这些配置型，直接使用这个配置项的内容对数据进行筛选，例如需要进行多表join后同步数据，使用select a,b from table_a join table_b on table_a.id = table_b.id 当用户配置querySql时，RDBMSReader直接忽略table、column、where条件的配置。\n必选：否 默认值：无 fetchSize\n描述：该配置项定义了插件和数据库服务器端每次批量数据获取条数，该值决定了DataX和服务器端的网络交互次数，能够较大的提升数据抽取性能。 注意，该值过大(\u0026gt;2048)可能造成DataX进程OOM。。\n必选：否 默认值：1024 3.3 类型转换 目前RDBMSReader支持大部分通用得关系数据库类型如数字、字符等，但也存在部分个别类型没有支持的情况，请注意检查你的类型，根据具体的数据库做选择。\n","permalink":"http://121.199.2.5:6080/34c79077064f4502a247f840e8b64c46/","summary":"RDBMSReader 插件文档 1 快速介绍 RDBMSReader插件实现了从RDBMS读取数据。在底层实现上，RDBMSReader通过JDBC连接远程RDBMS数据库，并执行相应的sql语句将数据从RDBMS库中SELECT出来。目前支持达梦、db2、PPAS、Sybase数据库的读取。RDBMSReader是一个通用的关系数据库读插件，您可以通过注册数据库驱动等方式增加任意多样的关系数据库读支持。\n2 实现原理 简而言之，RDBMSReader通过JDBC连接器连接到远程的RDBMS数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程RDBMS数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，RDBMSReader将其拼接为SQL语句发送到RDBMS数据库；对于用户配置querySql信息，RDBMS直接将其发送到RDBMS数据库。\n3 功能说明 3.1 配置样例 配置一个从RDBMS数据库同步抽取数据作业: {\r\u0026#34;job\u0026#34;: {\r\u0026#34;setting\u0026#34;: {\r\u0026#34;speed\u0026#34;: {\r\u0026#34;byte\u0026#34;: 1048576\r},\r\u0026#34;errorLimit\u0026#34;: {\r\u0026#34;record\u0026#34;: 0,\r\u0026#34;percentage\u0026#34;: 0.02\r}\r},\r\u0026#34;content\u0026#34;: [\r{\r\u0026#34;reader\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;rdbmsreader\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;username\u0026#34;: \u0026#34;xxx\u0026#34;,\r\u0026#34;password\u0026#34;: \u0026#34;xxx\u0026#34;,\r\u0026#34;column\u0026#34;: [\r\u0026#34;id\u0026#34;,\r\u0026#34;name\u0026#34;\r],\r\u0026#34;splitPk\u0026#34;: \u0026#34;pk\u0026#34;,\r\u0026#34;connection\u0026#34;: [\r{\r\u0026#34;table\u0026#34;: [\r\u0026#34;table\u0026#34;\r],\r\u0026#34;jdbcUrl\u0026#34;: [\r\u0026#34;jdbc:dm://ip:port/database\u0026#34;\r]\r}\r],\r\u0026#34;fetchSize\u0026#34;: 1024,\r\u0026#34;where\u0026#34;: \u0026#34;1 = 1\u0026#34;\r}\r},\r\u0026#34;writer\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;print\u0026#34;: true\r}\r}\r}\r]\r}\r} 配置一个自定义SQL的数据库同步任务到ODPS的作业： {\r\u0026#34;job\u0026#34;: {\r\u0026#34;setting\u0026#34;: {\r\u0026#34;speed\u0026#34;: {\r\u0026#34;byte\u0026#34;: 1048576\r},\r\u0026#34;errorLimit\u0026#34;: {\r\u0026#34;record\u0026#34;: 0,\r\u0026#34;percentage\u0026#34;: 0.","title":"RDBMSReader 插件文档"},{"content":"RDBMSWriter 插件文档 1 快速介绍 RDBMSWriter 插件实现了写入数据到 RDBMS 主库的目的表的功能。在底层实现上， RDBMSWriter 通过 JDBC 连接远程 RDBMS 数据库，并执行相应的 insert into \u0026hellip; 的 sql 语句将数据写入 RDBMS。 RDBMSWriter是一个通用的关系数据库写插件，您可以通过注册数据库驱动等方式增加任意多样的关系数据库写支持。\nRDBMSWriter 面向ETL开发工程师，他们使用 RDBMSWriter 从数仓导入数据到 RDBMS。同时 RDBMSWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 RDBMSWriter 通过 DataX 框架获取 Reader 生成的协议数据，RDBMSWriter 通过 JDBC 连接远程 RDBMS 数据库，并执行相应的 insert into \u0026hellip; 的 sql 语句将数据写入 RDBMS。\n3 功能说明 3.1 配置样例 配置一个写入RDBMS的作业。 {\r\u0026#34;job\u0026#34;: {\r\u0026#34;setting\u0026#34;: {\r\u0026#34;speed\u0026#34;: {\r\u0026#34;channel\u0026#34;: 1\r}\r},\r\u0026#34;content\u0026#34;: [\r{\r\u0026#34;reader\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;column\u0026#34;: [\r{\r\u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;\r},\r{\r\u0026#34;value\u0026#34;: 19880808,\r\u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;\r},\r{\r\u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;\r},\r{\r\u0026#34;value\u0026#34;: true,\r\u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;\r},\r{\r\u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34;\r}\r],\r\u0026#34;sliceRecordCount\u0026#34;: 1000\r}\r},\r\u0026#34;writer\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;rdbmswriter\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;connection\u0026#34;: [\r{\r\u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:dm://ip:port/database\u0026#34;,\r\u0026#34;table\u0026#34;: [\r\u0026#34;table\u0026#34;\r]\r}\r],\r\u0026#34;username\u0026#34;: \u0026#34;username\u0026#34;,\r\u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;,\r\u0026#34;table\u0026#34;: \u0026#34;table\u0026#34;,\r\u0026#34;column\u0026#34;: [\r\u0026#34;*\u0026#34;\r],\r\u0026#34;preSql\u0026#34;: [\r\u0026#34;delete from XXX;\u0026#34;\r]\r}\r}\r}\r]\r}\r} 3.2 参数说明 jdbcUrl\n描述：描述的是到对端数据库的JDBC连接信息，jdbcUrl按照RDBMS官方规范，并可以填写连接附件控制信息。请注意不同的数据库jdbc的格式是不同的，DataX会根据具体jdbc的格式选择合适的数据库驱动完成数据读取。\n达梦 jdbc:dm://ip:port/database db2格式 jdbc:db2://ip:port/database PPAS格式 jdbc:edb://ip:port/database rdbmswriter如何增加新的数据库支持:\n进入rdbmswriter对应目录，这里${DATAX_HOME}为DataX主目录，即: ${DATAX_HOME}/plugin/writer/rdbmswriter 在rdbmswriter插件目录下有plugin.json配置文件，在此文件中注册您具体的数据库驱动，具体放在drivers数组中。rdbmswriter插件在任务执行时会动态选择合适的数据库驱动连接数据库。 { \u0026#34;name\u0026#34;: \u0026#34;rdbmswriter\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;com.alibaba.datax.plugin.reader.rdbmswriter.RdbmsWriter\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;useScene: prod. mechanism: Jdbc connection using the database, execute select sql, retrieve data from the ResultSet. warn: The more you know about the database, the less problems you encounter.\u0026#34;, \u0026#34;developer\u0026#34;: \u0026#34;alibaba\u0026#34;, \u0026#34;drivers\u0026#34;: [ \u0026#34;dm.jdbc.driver.DmDriver\u0026#34;, \u0026#34;com.ibm.db2.jcc.DB2Driver\u0026#34;, \u0026#34;com.sybase.jdbc3.jdbc.SybDriver\u0026#34;, \u0026#34;com.edb.Driver\u0026#34; ] } 在rdbmswriter插件目录下有libs子目录，您需要将您具体的数据库驱动放到libs目录下。 $tree\r.\r|-- libs\r| |-- Dm7JdbcDriver16.jar\r| |-- commons-collections-3.0.jar\r| |-- commons-io-2.4.jar\r| |-- commons-lang3-3.3.2.jar\r| |-- commons-math3-3.1.1.jar\r| |-- datax-common-0.0.1-SNAPSHOT.jar\r| |-- datax-service-face-1.0.23-20160120.024328-1.jar\r| |-- db2jcc4.jar\r| |-- druid-1.0.15.jar\r| |-- edb-jdbc16.jar\r| |-- fastjson-1.1.46.sec01.jar\r| |-- guava-r05.jar\r| |-- hamcrest-core-1.3.jar\r| |-- jconn3-1.0.0-SNAPSHOT.jar\r| |-- logback-classic-1.0.13.jar\r| |-- logback-core-1.0.13.jar\r| |-- plugin-rdbms-util-0.0.1-SNAPSHOT.jar\r| `-- slf4j-api-1.7.10.jar\r|-- plugin.json\r|-- plugin_job_template.json\r`-- rdbmswriter-0.0.1-SNAPSHOT.jar 必选：是 默认值：无 username\n描述：数据源的用户名 必选：是 默认值：无 password\n描述：数据源指定用户名的密码 必选：是 默认值：无 table\n描述：目标表名称，如果表的schema信息和上述配置username不一致，请使用schema.table的格式填写table信息。 必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合。以英文逗号（,）进行分隔。我们强烈不推荐用户使用默认列情况 必选：是 默认值：无 preSql\n描述：执行数据同步任务之前率先执行的sql语句，目前只允许执行一条SQL语句，例如清除旧数据。 必选：否 默认值：无 postSql\n描述：执行数据同步任务之后执行的sql语句，目前只允许执行一条SQL语句，例如加上某一个时间戳。 必选：否 默认值：无 batchSize\n描述：一次性批量提交的记录数大小，该值可以极大减少DataX与RDBMS的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。\n必选：否 默认值：1024 3.3 类型转换 目前RDBMSReader支持大部分通用得关系数据库类型如数字、字符等，但也存在部分个别类型没有支持的情况，请注意检查你的类型，根据具体的数据库做选择。\n","permalink":"http://121.199.2.5:6080/5d9d57b2e00d4d8ca994dc15ece38148/","summary":"RDBMSWriter 插件文档 1 快速介绍 RDBMSWriter 插件实现了写入数据到 RDBMS 主库的目的表的功能。在底层实现上， RDBMSWriter 通过 JDBC 连接远程 RDBMS 数据库，并执行相应的 insert into \u0026hellip; 的 sql 语句将数据写入 RDBMS。 RDBMSWriter是一个通用的关系数据库写插件，您可以通过注册数据库驱动等方式增加任意多样的关系数据库写支持。\nRDBMSWriter 面向ETL开发工程师，他们使用 RDBMSWriter 从数仓导入数据到 RDBMS。同时 RDBMSWriter 亦可以作为数据迁移工具为DBA等用户提供服务。\n2 实现原理 RDBMSWriter 通过 DataX 框架获取 Reader 生成的协议数据，RDBMSWriter 通过 JDBC 连接远程 RDBMS 数据库，并执行相应的 insert into \u0026hellip; 的 sql 语句将数据写入 RDBMS。\n3 功能说明 3.1 配置样例 配置一个写入RDBMS的作业。 {\r\u0026#34;job\u0026#34;: {\r\u0026#34;setting\u0026#34;: {\r\u0026#34;speed\u0026#34;: {\r\u0026#34;channel\u0026#34;: 1\r}\r},\r\u0026#34;content\u0026#34;: [\r{\r\u0026#34;reader\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;streamreader\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;column\u0026#34;: [\r{\r\u0026#34;value\u0026#34;: \u0026#34;DataX\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;\r},\r{\r\u0026#34;value\u0026#34;: 19880808,\r\u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;\r},\r{\r\u0026#34;value\u0026#34;: \u0026#34;1988-08-08 08:08:08\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;\r},\r{\r\u0026#34;value\u0026#34;: true,\r\u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;\r},\r{\r\u0026#34;value\u0026#34;: \u0026#34;test\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;bytes\u0026#34;\r}\r],\r\u0026#34;sliceRecordCount\u0026#34;: 1000\r}\r},\r\u0026#34;writer\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;rdbmswriter\u0026#34;,\r\u0026#34;parameter\u0026#34;: {\r\u0026#34;connection\u0026#34;: [\r{\r\u0026#34;jdbcUrl\u0026#34;: \u0026#34;jdbc:dm://ip:port/database\u0026#34;,\r\u0026#34;table\u0026#34;: [\r\u0026#34;table\u0026#34;\r]\r}\r],\r\u0026#34;username\u0026#34;: \u0026#34;username\u0026#34;,\r\u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;,\r\u0026#34;table\u0026#34;: \u0026#34;table\u0026#34;,\r\u0026#34;column\u0026#34;: [\r\u0026#34;*\u0026#34;\r],\r\u0026#34;preSql\u0026#34;: [\r\u0026#34;delete from XXX;\u0026#34;\r]\r}\r}\r}\r]\r}\r} 3.","title":"RDBMSWriter 插件文档"},{"content":"some script here.\n","permalink":"http://121.199.2.5:6080/b007b0bc6dca40458f17c7c1826e9da5/","summary":"some script here.","title":"Readme.md"},{"content":"本插件仅在Elasticsearch 5.x上测试\n","permalink":"http://121.199.2.5:6080/9dbd1273dcb848bbaa69f53c0105d5c4/","summary":"本插件仅在Elasticsearch 5.x上测试","title":"README.md"},{"content":"SqlServerReader 插件文档 1 快速介绍 SqlServerReader插件实现了从SqlServer读取数据。在底层实现上，SqlServerReader通过JDBC连接远程SqlServer数据库，并执行相应的sql语句将数据从SqlServer库中SELECT出来。\n2 实现原理 简而言之，SqlServerReader通过JDBC连接器连接到远程的SqlServer数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程SqlServer数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，SqlServerReader将其拼接为SQL语句发送到SqlServer数据库；对于用户配置querySql信息，SqlServer直接将其发送到SqlServer数据库。\n3 功能说明 3.1 配置样例 配置一个从SqlServer数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;byte\u0026#34;: 1048576 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sqlserverreader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34; ], \u0026#34;splitPk\u0026#34;: \u0026#34;db_id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:sqlserver://localhost:3433;DatabaseName=dbname\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: 1048576 }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sqlserverreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:sqlserver://bad_ip:3433;DatabaseName=dbname\u0026#34;, \u0026#34;jdbc:sqlserver://127.0.0.1:bad_port;DatabaseName=dbname\u0026#34;, \u0026#34;jdbc:sqlserver://127.0.0.1:3306;DatabaseName=dbname\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;visible\u0026#34;: false, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 3.2 参数说明 jdbcUrl\n描述：描述的是到对端数据库的JDBC连接信息，使用JSON的数组描述，并支持一个库填写多个连接地址。之所以使用JSON数组描述连接信息，是因为阿里集团内部支持多个IP探测，如果配置了多个，SqlServerReader可以依次探测ip的可连接性，直到选择一个合法的IP。如果全部连接失败，SqlServerReader报错。 注意，jdbcUrl必须包含在connection配置单元中。对于阿里集团外部使用情况，JSON数组填写一个JDBC连接即可。\njdbcUrl按照SqlServer官方规范，并可以填写连接附件控制信息。具体请参看SqlServer官方文档。\n必选：是 默认值：无 username\n描述：数据源的用户名 必选：是 默认值：无 password\n描述：数据源指定用户名的密码 必选：是 默认值：无 table\n描述：所选取的需要同步的表。使用JSON的数组描述，因此支持多张表同时抽取。当配置为多张表时，用户自己需保证多张表是同一schema结构，SqlServerReader不予检查表是否同一逻辑表。注意，table必须包含在connection配置单元中。\n必选：是 默认值：无 column\n描述：所配置的表中需要同步的列名集合，使用JSON的数组描述字段信息。用户使用*代表默认使用所有列配置，例如[\u0026quot;*\u0026quot;]。\n支持列裁剪，即列可以挑选部分列进行导出。\n支持列换序，即列可以不按照表schema信息进行导出。\n支持常量配置，用户需要按照JSON格式: [\u0026ldquo;id\u0026rdquo;, \u0026ldquo;[table]\u0026rdquo;, \u0026ldquo;1\u0026rdquo;, \u0026ldquo;\u0026lsquo;bazhen.csy\u0026rsquo;\u0026rdquo;, \u0026ldquo;null\u0026rdquo;, \u0026ldquo;COUNT(*)\u0026rdquo;, \u0026ldquo;2.3\u0026rdquo; , \u0026ldquo;true\u0026rdquo;] id为普通列名，[table]为包含保留在的列名，1为整形数字常量，\u0026lsquo;bazhen.csy\u0026rsquo;为字符串常量，null为空指针，to_char(a + 1)为表达式，2.3为浮点数，true为布尔值。\ncolumn必须用户显示指定同步的列集合，不允许为空！\n必选：是 默认值：无 splitPk\n描述：SqlServerReader进行数据抽取时，如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。\n推荐splitPk用户使用表主键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。\n目前splitPk仅支持整形型数据切分，不支持浮点、字符串、日期等其他类型。如果用户指定其他非支持类型，SqlServerReader将报错！\nsplitPk设置为空，底层将视作用户不允许对单表进行切分，因此使用单通道进行抽取。\n必选：否 默认值：无 where\n描述：筛选条件，MysqlReader根据指定的column、table、where条件拼接SQL，并根据这个SQL进行数据抽取。在实际业务场景中，往往会选择当天的数据进行同步，可以将where条件指定为gmt_create \u0026gt; $bizdate 。注意：不可以将where条件指定为limit 10，limit不是SQL的合法where子句。\nwhere条件可以有效地进行业务增量同步。如果该值为空，代表同步全表所有的信息。 必选：否 默认值：无 querySql\n描述：在有些业务场景下，where这一配置项不足以描述所筛选的条件，用户可以通过该配置型来自定义筛选SQL。当用户配置了这一项之后，DataX系统就会忽略table，column这些配置型，直接使用这个配置项的内容对数据进行筛选，例如需要进行多表join后同步数据，使用select a,b from table_a join table_b on table_a.id = table_b.id 当用户配置querySql时，SqlServerReader直接忽略table、column、where条件的配置。\n必选：否 默认值：无 fetchSize\n描述：该配置项定义了插件和数据库服务器端每次批量数据获取条数，该值决定了DataX和服务器端的网络交互次数，能够较大的提升数据抽取性能。 注意，该值过大(\u0026gt;2048)可能造成DataX进程OOM。。\n必选：否 默认值：1024 3.3 类型转换 目前SqlServerReader支持大部分SqlServer类型，但也存在部分个别类型没有支持的情况，请注意检查你的类型。\n下面列出SqlServerReader针对SqlServer类型转换列表:\nDataX 内部类型 SqlServer 数据类型 Long bigint, int, smallint, tinyint Double float, decimal, real, numeric String char,nchar,ntext,nvarchar,text,varchar,nvarchar(MAX),varchar(MAX) Date date, datetime, time Boolean bit Bytes binary,varbinary,varbinary(MAX),timestamp 请注意:\n除上述罗列字段类型外，其他类型均不支持。 timestamp类型作为二进制类型。 4 性能报告 暂无\n5 约束限制 5.1 主备同步数据恢复问题 主备同步问题指SqlServer使用主从灾备，备库从主库不间断通过binlog恢复数据。由于主备数据同步存在一定的时间差，特别在于某些特定情况，例如网络延迟等问题，导致备库同步恢复的数据与主库有较大差别，导致从备库同步的数据不是一份当前时间的完整镜像。\n针对这个问题，我们提供了preSql功能，该功能待补充。\n5.2 一致性约束 SqlServer在数据存储划分中属于RDBMS系统，对外可以提供强一致性数据查询接口。例如当一次同步任务启动运行过程中，当该库存在其他数据写入方写入数据时，SqlServerReader完全不会获取到写入更新数据，这是由于数据库本身的快照特性决定的。关于数据库快照特性，请参看MVCC Wikipedia\n上述是在SqlServerReader单线程模型下数据同步一致性的特性，由于SqlServerReader可以根据用户配置信息使用了并发数据抽取，因此不能严格保证数据一致性：当SqlServerReader根据splitPk进行数据切分后，会先后启动多个并发任务完成数据同步。由于多个并发任务相互之间不属于同一个读事务，同时多个并发任务存在时间间隔。因此这份数据并不是完整的、一致的数据快照信息。\n针对多线程的一致性快照需求，在技术上目前无法实现，只能从工程角度解决，工程化的方式存在取舍，我们提供几个解决思路给用户，用户可以自行选择：\n使用单线程同步，即不再进行数据切片。缺点是速度比较慢，但是能够很好保证一致性。\n关闭其他数据写入方，保证当前数据为静态数据，例如，锁表、关闭备库同步等等。缺点是可能影响在线业务。\n5.3 数据库编码问题 SqlServerReader底层使用JDBC进行数据抽取，JDBC天然适配各类编码，并在底层进行了编码转换。因此SqlServerReader不需用户指定编码，可以自动识别编码并转码。\n5.4 增量数据同步 SqlServerReader使用JDBC SELECT语句完成数据抽取工作，因此可以使用SELECT\u0026hellip;WHERE\u0026hellip;进行增量数据抽取，方式有多种：\n数据库在线应用写入数据库时，填充modify字段为更改时间戳，包括新增、更新、删除(逻辑删)。对于这类应用，SqlServerReader只需要WHERE条件跟上一同步阶段时间戳即可。 对于新增流水型数据，SqlServerReader可以WHERE条件后跟上一阶段最大自增ID即可。 对于业务上无字段区分新增、修改数据情况，SqlServerReader也无法进行增量数据同步，只能同步全量数据。\n5.5 Sql安全性 SqlServerReader提供querySql语句交给用户自己实现SELECT抽取语句，SqlServerReader本身对querySql不做任何安全性校验。这块交由DataX用户方自己保证。\n6 FAQ ","permalink":"http://121.199.2.5:6080/e30dc23cda1d465fa0872475e5c976a7/","summary":"SqlServerReader 插件文档 1 快速介绍 SqlServerReader插件实现了从SqlServer读取数据。在底层实现上，SqlServerReader通过JDBC连接远程SqlServer数据库，并执行相应的sql语句将数据从SqlServer库中SELECT出来。\n2 实现原理 简而言之，SqlServerReader通过JDBC连接器连接到远程的SqlServer数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程SqlServer数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。\n对于用户配置Table、Column、Where的信息，SqlServerReader将其拼接为SQL语句发送到SqlServer数据库；对于用户配置querySql信息，SqlServer直接将其发送到SqlServer数据库。\n3 功能说明 3.1 配置样例 配置一个从SqlServer数据库同步抽取数据到本地的作业: { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;byte\u0026#34;: 1048576 } }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sqlserverreader\u0026#34;, \u0026#34;parameter\u0026#34;: { // 数据库连接用户名 \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, // 数据库连接密码 \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;id\u0026#34; ], \u0026#34;splitPk\u0026#34;: \u0026#34;db_id\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;table\u0026#34;: [ \u0026#34;table\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:sqlserver://localhost:3433;DatabaseName=dbname\u0026#34; ] } ] } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;print\u0026#34;: true, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { \u0026#34;job\u0026#34;: { \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: 1048576 }, \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sqlserverreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;where\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;connection\u0026#34;: [ { \u0026#34;querySql\u0026#34;: [ \u0026#34;select db_id,on_line_flag from db_info where db_id \u0026lt; 10;\u0026#34; ], \u0026#34;jdbcUrl\u0026#34;: [ \u0026#34;jdbc:sqlserver://bad_ip:3433;DatabaseName=dbname\u0026#34;, \u0026#34;jdbc:sqlserver://127.","title":"SqlServerReader 插件文档"},{"content":"TableStore增量数据导出通道：TableStoreStreamReader 快速介绍 TableStoreStreamReader插件主要用于TableStore的增量数据导出，增量数据可以看作操作日志，除了数据本身外还附有操作信息。\n与全量导出插件不同，增量导出插件只有多版本模式，同时不支持指定列。这是与增量导出的原理有关的，导出的格式下面有详细介绍。\n使用插件前必须确保表上已经开启Stream功能，可以在建表的时候指定开启，或者使用SDK的UpdateTable接口开启。\n开启Stream的方法： SyncClient client = new SyncClient(\u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;); 1. 建表的时候开启： CreateTableRequest createTableRequest = new CreateTableRequest(tableMeta); createTableRequest.setStreamSpecification(new StreamSpecification(true, 24)); // 24代表增量数据保留24小时 client.createTable(createTableRequest); 2. 如果建表时未开启，可以通过UpdateTable开启: UpdateTableRequest updateTableRequest = new UpdateTableRequest(\u0026quot;tableName\u0026quot;); updateTableRequest.setStreamSpecification(new StreamSpecification(true, 24)); client.updateTable(updateTableRequest); 实现原理 首先用户使用SDK的UpdateTable功能，指定开启Stream并设置过期时间，即开启了增量功能。\n开启后，TableStore服务端就会将用户的操作日志额外保存起来， 每个分区有一个有序的操作日志队列，每条操作日志会在一定时间后被垃圾回收，这个时间即用户指定的过期时间。\nTableStore的SDK提供了几个Stream相关的API用于将这部分操作日志读取出来，增量插件也是通过TableStore SDK的接口获取到增量数据的，并将 增量数据转化为多个6元组的形式(pk, colName, version, colValue, opType, sequenceInfo)导入到ODPS中。\nReader的配置模版： \u0026quot;reader\u0026quot;: { \u0026quot;name\u0026quot; : \u0026quot;otsstreamreader\u0026quot;, \u0026quot;parameter\u0026quot; : { \u0026quot;endpoint\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;accessId\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;accessKey\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;instanceName\u0026quot; : \u0026quot;\u0026quot;, //dataTable即需要导出数据的表。 \u0026quot;dataTable\u0026quot; : \u0026quot;\u0026quot;, //statusTable是Reader用于保存状态的表，若该表不存在，Reader会自动创建该表。 //一次离线导出任务完成后，用户不应删除该表，该表中记录的状态可用于下次导出任务中。 \u0026quot;statusTable\u0026quot; : \u0026quot;TableStoreStreamReaderStatusTable\u0026quot;, //增量数据的时间范围（左闭右开）的左边界。 \u0026quot;startTimestampMillis\u0026quot; : \u0026quot;\u0026quot;, //增量数据的时间范围（左闭右开）的右边界。 \u0026quot;endTimestampMillis\u0026quot; : \u0026quot;\u0026quot;, //采云间调度只支持天级别，所以提供该配置，作用与startTimestampMillis和endTimestampMillis类似。 \u0026quot;date\u0026quot;: \u0026quot;\u0026quot;, //是否导出时序信息。 \u0026quot;isExportSequenceInfo\u0026quot;: true, //从TableStore中读增量数据时，每次请求的最大重试次数，默认为30。 \u0026quot;maxRetries\u0026quot; : 30 } } 参数说明 名称 说明 类型 必选 endpoint TableStoreServer的Endpoint地址。 String 是 accessId 用于访问TableStore服务的accessId。 String 是 accessKey 用于访问TableStore服务的accessKey。 String 是 instanceName TableStore的实例名称。 String 是 dataTable 需要导出增量数据的表的名称。该表需要开启Stream，可以在建表时开启，或者使用UpdateTable接口开启。 String 是 statusTable Reader插件用于记录状态的表的名称，这些状态可用于减少对非目标范围内的数据的扫描，从而加快导出速度。 1. 用户不需要创建该表，只需要给出一个表名。Reader插件会尝试在用户的instance下创建该表，若该表不存在即创建新表，若该表已存在，会判断该表的Meta是否与期望一致，若不一致会抛出异常。 2. 在一次导出完成之后，用户不应删除该表，该表的状态可用于下次导出任务。 3. 该表会开启TTL，数据自动过期，因此可认为其数据量很小。 4. 针对同一个instance下的多个不同的dataTable的Reader配置，可以使用同一个statusTable，记录的状态信息互不影响。 综上，用户配置一个类似TableStoreStreamReaderStatusTable之类的名称即可，注意不要与业务相关的表重名。 String 是 startTimestampMillis 增量数据的时间范围（左闭右开）的左边界，单位毫秒。 1. Reader插件会从statusTable中找对应startTimestampMillis的位点，从该点开始读取开始导出数据。 2. 若statusTable中找不到对应的位点，则从系统保留的增量数据的第一条开始读取，并跳过写入时间小于startTimestampMillis的数据。 Long 否 endTimestampMillis 增量数据的时间范围（左闭右开）的右边界，单位毫秒。 1. Reader插件从startTimestampMillis位置开始导出数据后，当遇到第一条时间戳大于等于endTimestampMillis的数据时，结束导出数据，导出完成。 2. 当读取完当前全部的增量数据时，结束读取，即使未达到endTimestampMillis。 Long 否 date 日期格式为yyyyMMdd，如20151111，表示导出该日的数据。 若没有指定date，则必须指定startTimestampMillis和endTimestampMillis，反之也成立。 String 否 isExportSequenceInfo 是否导出时序信息，时序信息包含了数据的写入时间等。默认该值为false，即不导出。 Boolean 否 maxRetries 从TableStore中读增量数据时，每次请求的最大重试次数，默认为30，重试之间有间隔，30次重试总时间约为5分钟，一般无需更改。 Int 否 导出的数据格式 首先，在TableStore多版本模型下，表中的数据组织为“行－列－版本”三级的模式， 一行可以有任意列，列名也并非固定的，每一列可以含有多个版本，每个版本都有一个特定的时间戳（版本号）。\n用户可以通过TableStore的API进行一系列读写操作， TableStore通过记录用户最近对表的一系列写操作（或称为数据更改操作）来实现记录增量数据的目的， 所以也可以把增量数据看作一批操作记录。\nTableStore有三类数据更改操作：PutRow、UpdateRow、DeleteRow。\nPutRow的语义是写入一行，若该行已存在即覆盖该行。\nUpdateRow的语义是更新一行，对原行其他数据不做更改， 更新可能包括新增或覆盖（若对应列的对应版本已存在）一些列值、删除某一列的全部版本、删除某一列的某个版本。\nDeleteRow的语义是删除一行。\nTableStore会根据每种操作生成对应的增量数据记录，Reader插件会读出这些记录，并导出成Datax的数据格式。\n同时，由于TableStore具有动态列、多版本的特性，所以Reader插件导出的一行不对应TableStore中的一行，而是对应TableStore中的一列的一个版本。 即TableStore中的一行可能会导出很多行，每行包含主键值、该列的列名、该列下该版本的时间戳（版本号）、该版本的值、操作类型。若设置isExportSequenceInfo为true，还会包括时序信息。\n转换为Datax的数据格式后，我们定义了四种操作类型，分别为:\nU（UPDATE）: 写入一列的一个版本\nDO（DELETE_ONE_VERSION）: 删除某一列的某个版本\nDA（DELETE_ALL_VERSION）: 删除某一列的全部版本，此时需要根据主键和列名，将对应列的全部版本删除\nDR（DELETE_ROW）: 删除某一行，此时需要根据主键，将该行数据全部删除\n举例如下，假设该表有两个主键列，主键列名分别为pkName1, pkName2：\npkName1 pkName2 columnName timestamp columnValue opType pk1_V1 pk2_V1 col_a 1441803688001 col_val1 U pk1_V1 pk2_V1 col_a 1441803688002 col_val2 U pk1_V1 pk2_V1 col_b 1441803688003 col_val3 U pk1_V2 pk2_V2 col_a 1441803688000 DO pk1_V2 pk2_V2 col_b DA pk1_V3 pk2_V3 DR pk1_V3 pk2_V3 col_a 1441803688005 col_val1 U 假设导出的数据如上，共7行，对应TableStore表内的3行，主键分别是(pk1_V1,pk2_V1), (pk1_V2, pk2_V2), (pk1_V3, pk2_V3)。\n对于主键为(pk1_V1, pk2_V1)的一行，包含三个操作，分别是写入col_a列的两个版本和col_b列的一个版本。\n对于主键为(pk1_V2, pk2_V2)的一行，包含两个操作，分别是删除col_a列的一个版本、删除col_b列的全部版本。\n对于主键为(pk1_V3, pk2_V3)的一行，包含两个操作，分别是删除整行、写入col_a列的一个版本。\n","permalink":"http://121.199.2.5:6080/6851df08c78c4eb1aab41d312b920bf9/","summary":"TableStore增量数据导出通道：TableStoreStreamReader 快速介绍 TableStoreStreamReader插件主要用于TableStore的增量数据导出，增量数据可以看作操作日志，除了数据本身外还附有操作信息。\n与全量导出插件不同，增量导出插件只有多版本模式，同时不支持指定列。这是与增量导出的原理有关的，导出的格式下面有详细介绍。\n使用插件前必须确保表上已经开启Stream功能，可以在建表的时候指定开启，或者使用SDK的UpdateTable接口开启。\n开启Stream的方法： SyncClient client = new SyncClient(\u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;); 1. 建表的时候开启： CreateTableRequest createTableRequest = new CreateTableRequest(tableMeta); createTableRequest.setStreamSpecification(new StreamSpecification(true, 24)); // 24代表增量数据保留24小时 client.createTable(createTableRequest); 2. 如果建表时未开启，可以通过UpdateTable开启: UpdateTableRequest updateTableRequest = new UpdateTableRequest(\u0026quot;tableName\u0026quot;); updateTableRequest.setStreamSpecification(new StreamSpecification(true, 24)); client.updateTable(updateTableRequest); 实现原理 首先用户使用SDK的UpdateTable功能，指定开启Stream并设置过期时间，即开启了增量功能。\n开启后，TableStore服务端就会将用户的操作日志额外保存起来， 每个分区有一个有序的操作日志队列，每条操作日志会在一定时间后被垃圾回收，这个时间即用户指定的过期时间。\nTableStore的SDK提供了几个Stream相关的API用于将这部分操作日志读取出来，增量插件也是通过TableStore SDK的接口获取到增量数据的，并将 增量数据转化为多个6元组的形式(pk, colName, version, colValue, opType, sequenceInfo)导入到ODPS中。\nReader的配置模版： \u0026quot;reader\u0026quot;: { \u0026quot;name\u0026quot; : \u0026quot;otsstreamreader\u0026quot;, \u0026quot;parameter\u0026quot; : { \u0026quot;endpoint\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;accessId\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;accessKey\u0026quot; : \u0026quot;\u0026quot;, \u0026quot;instanceName\u0026quot; : \u0026quot;\u0026quot;, //dataTable即需要导出数据的表。 \u0026quot;dataTable\u0026quot; : \u0026quot;\u0026quot;, //statusTable是Reader用于保存状态的表，若该表不存在，Reader会自动创建该表。 //一次离线导出任务完成后，用户不应删除该表，该表中记录的状态可用于下次导出任务中。 \u0026quot;statusTable\u0026quot; : \u0026quot;TableStoreStreamReaderStatusTable\u0026quot;, //增量数据的时间范围（左闭右开）的左边界。 \u0026quot;startTimestampMillis\u0026quot; : \u0026quot;\u0026quot;, //增量数据的时间范围（左闭右开）的右边界。 \u0026quot;endTimestampMillis\u0026quot; : \u0026quot;\u0026quot;, //采云间调度只支持天级别，所以提供该配置，作用与startTimestampMillis和endTimestampMillis类似。 \u0026quot;date\u0026quot;: \u0026quot;\u0026quot;, //是否导出时序信息。 \u0026quot;isExportSequenceInfo\u0026quot;: true, //从TableStore中读增量数据时，每次请求的最大重试次数，默认为30。 \u0026quot;maxRetries\u0026quot; : 30 } } 参数说明 名称 说明 类型 必选 endpoint TableStoreServer的Endpoint地址。 String 是 accessId 用于访问TableStore服务的accessId。 String 是 accessKey 用于访问TableStore服务的accessKey。 String 是 instanceName TableStore的实例名称。 String 是 dataTable 需要导出增量数据的表的名称。该表需要开启Stream，可以在建表时开启，或者使用UpdateTable接口开启。 String 是 statusTable Reader插件用于记录状态的表的名称，这些状态可用于减少对非目标范围内的数据的扫描，从而加快导出速度。 1.","title":"TableStore增量数据导出通道：TableStoreStreamReader"},{"content":"TSDBReader 插件文档 1 快速介绍 TSDBReader 插件实现了从阿里云 TSDB 读取数据。阿里云时间序列数据库 ( Time Series Database , 简称 TSDB) 是一种集时序数据高效读写，压缩存储，实时计算能力为一体的数据库服务，可广泛应用于物联网和互联网领域，实现对设备及业务服务的实时监控，实时预测告警。详见 TSDB 的阿里云官网。\n2 实现原理 在底层实现上，TSDBReader 通过 HTTP 请求链接到 阿里云 TSDB 实例，利用 /api/query 或者 /api/mquery 接口将数据点扫描出来（更多细节详见：时序数据库 TSDB - HTTP API 概览）。而整个同步的过程，是通过时间线和查询时间线范围进行切分。\n3 功能说明 3.1 配置样例 配置一个从 阿里云 TSDB 数据库同步抽取数据到本地的作业，并以时序数据的格式输出： 时序数据样例：\n{\u0026#34;metric\u0026#34;:\u0026#34;m\u0026#34;,\u0026#34;tags\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;a19\u0026#34;,\u0026#34;cluster\u0026#34;:\u0026#34;c5\u0026#34;,\u0026#34;group\u0026#34;:\u0026#34;g10\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;i999\u0026#34;,\u0026#34;zone\u0026#34;:\u0026#34;z1\u0026#34;},\u0026#34;timestamp\u0026#34;:1546272263,\u0026#34;value\u0026#34;:1} { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;TSDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取数据到本地的作业，并以关系型数据的格式输出： 关系型数据样例：\nm\t1546272125\ta1\tc1\tg2\ti3021\tz4\t1.0 { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;RDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;__metric__\u0026#34;, \u0026#34;__ts__\u0026#34;, \u0026#34;app\u0026#34;, \u0026#34;cluster\u0026#34;, \u0026#34;group\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;zone\u0026#34;, \u0026#34;__value__\u0026#34; ], \u0026#34;metric\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取单值数据到 ADB 的作业： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;RDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;__metric__\u0026#34;, \u0026#34;__ts__\u0026#34;, \u0026#34;app\u0026#34;, \u0026#34;cluster\u0026#34;, \u0026#34;group\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;zone\u0026#34;, \u0026#34;__value__\u0026#34; ], \u0026#34;metric\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;`metric`\u0026#34;, \u0026#34;`ts`\u0026#34;, \u0026#34;`app`\u0026#34;, \u0026#34;`cluster`\u0026#34;, \u0026#34;`group`\u0026#34;, \u0026#34;`ip`\u0026#34;, \u0026#34;`zone`\u0026#34;, \u0026#34;`value`\u0026#34; ], \u0026#34;url\u0026#34;: \u0026#34;http://localhost:3306\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;datax_test\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;datax_test\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;opIndex\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;batchSize\u0026#34;: \u0026#34;2\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取多值数据到 ADB 的作业： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;RDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;__metric__\u0026#34;, \u0026#34;__ts__\u0026#34;, \u0026#34;app\u0026#34;, \u0026#34;cluster\u0026#34;, \u0026#34;group\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;zone\u0026#34;, \u0026#34;load\u0026#34;, \u0026#34;memory\u0026#34;, \u0026#34;cpu\u0026#34; ], \u0026#34;metric\u0026#34;: [ \u0026#34;m_field\u0026#34; ], \u0026#34;field\u0026#34;: { \u0026#34;m_field\u0026#34;: [ \u0026#34;load\u0026#34;, \u0026#34;memory\u0026#34;, \u0026#34;cpu\u0026#34; ] }, \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;`metric`\u0026#34;, \u0026#34;`ts`\u0026#34;, \u0026#34;`app`\u0026#34;, \u0026#34;`cluster`\u0026#34;, \u0026#34;`group`\u0026#34;, \u0026#34;`ip`\u0026#34;, \u0026#34;`zone`\u0026#34;, \u0026#34;`load`\u0026#34;, \u0026#34;`memory`\u0026#34;, \u0026#34;`cpu`\u0026#34; ], \u0026#34;url\u0026#34;: \u0026#34;http://localhost:3306\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;datax_test\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;datax_test_multi_field\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;opIndex\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;batchSize\u0026#34;: \u0026#34;2\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取单值数据到 ADB 的作业，并指定过滤部分时间线： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;RDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;__metric__\u0026#34;, \u0026#34;__ts__\u0026#34;, \u0026#34;app\u0026#34;, \u0026#34;cluster\u0026#34;, \u0026#34;group\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;zone\u0026#34;, \u0026#34;__value__\u0026#34; ], \u0026#34;metric\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;tag\u0026#34;: { \u0026#34;m\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;a1\u0026#34;, \u0026#34;cluster\u0026#34;: \u0026#34;c1\u0026#34; } }, \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;`metric`\u0026#34;, \u0026#34;`ts`\u0026#34;, \u0026#34;`app`\u0026#34;, \u0026#34;`cluster`\u0026#34;, \u0026#34;`group`\u0026#34;, \u0026#34;`ip`\u0026#34;, \u0026#34;`zone`\u0026#34;, \u0026#34;`value`\u0026#34; ], \u0026#34;url\u0026#34;: \u0026#34;http://localhost:3306\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;datax_test\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;datax_test\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;opIndex\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;batchSize\u0026#34;: \u0026#34;2\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取多值数据到 ADB 的作业，并指定过滤部分时间线： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;RDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;__metric__\u0026#34;, \u0026#34;__ts__\u0026#34;, \u0026#34;app\u0026#34;, \u0026#34;cluster\u0026#34;, \u0026#34;group\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;zone\u0026#34;, \u0026#34;load\u0026#34;, \u0026#34;memory\u0026#34;, \u0026#34;cpu\u0026#34; ], \u0026#34;metric\u0026#34;: [ \u0026#34;m_field\u0026#34; ], \u0026#34;field\u0026#34;: { \u0026#34;m_field\u0026#34;: [ \u0026#34;load\u0026#34;, \u0026#34;memory\u0026#34;, \u0026#34;cpu\u0026#34; ] }, \u0026#34;tag\u0026#34;: { \u0026#34;m_field\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;i999\u0026#34; } }, \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;adswriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;******\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;`metric`\u0026#34;, \u0026#34;`ts`\u0026#34;, \u0026#34;`app`\u0026#34;, \u0026#34;`cluster`\u0026#34;, \u0026#34;`group`\u0026#34;, \u0026#34;`ip`\u0026#34;, \u0026#34;`zone`\u0026#34;, \u0026#34;`load`\u0026#34;, \u0026#34;`memory`\u0026#34;, \u0026#34;`cpu`\u0026#34; ], \u0026#34;url\u0026#34;: \u0026#34;http://localhost:3306\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;datax_test\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;datax_test_multi_field\u0026#34;, \u0026#34;writeMode\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;opIndex\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;batchSize\u0026#34;: \u0026#34;2\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取单值数据到另一个 阿里云 TSDB 数据库 的作业： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;TSDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8240\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取多值数据到另一个 阿里云 TSDB 数据库 的作业： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;TSDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;m_field\u0026#34; ], \u0026#34;field\u0026#34;: { \u0026#34;m_field\u0026#34;: [ \u0026#34;load\u0026#34;, \u0026#34;memory\u0026#34;, \u0026#34;cpu\u0026#34; ] }, \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;multiField\u0026#34;: true, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8240\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 3.2 参数说明 name\n描述：本插件的名称 必选：是 默认值：tsdbreader parameter\nsinkDbType\n描述：目标数据库的类型 必选：否 默认值：TSDB 注意：目前支持 TSDB 和 RDB 两个取值。其中，TSDB 包括 阿里云 TSDB、OpenTSDB、InfluxDB、Prometheus 和 TimeScale。RDB 包括 ADB、MySQL、Oracle、PostgreSQL 和 DRDS 等。 endpoint\n描述：阿里云 TSDB 的 HTTP 连接地址 必选：是 格式：http://IP:Port 默认值：无 column\n描述：TSDB 场景下：数据迁移任务需要迁移的 Metric 列表；RDB 场景下：映射到关系型数据库中的表字段，且增加 __metric__、__ts__ 和 __value__ 三个字段，其中 __metric__ 用于映射度量字段，__ts__ 用于映射 timestamp 字段，而 __value__ 仅适用于单值场景，用于映射度量值，多值场景下，直接指定 field 字段即可 必选：是 默认值：无 metric\n描述：仅适用于 RDB 场景下，表示数据迁移任务需要迁移的 Metric 列表 必选：否 默认值：无 field\n描述：仅适用于多值场景下，表示数据迁移任务需要迁移的 Field 列表 必选：否 默认值：无 tag\n描述：数据迁移任务需要迁移的 TagK 和 TagV，用于进一步过滤时间线 必选：否 默认值：无 splitIntervalMs\n描述：用于 DataX 内部切分 Task，每个 Task 只查询一小部分的时间段 必选：是 默认值：无 注意：单位是 ms 毫秒 beginDateTime\n描述：和 endDateTime 配合使用，用于指定哪个时间段内的数据点，需要被迁移 必选：是 格式：yyyy-MM-dd HH:mm:ss 默认值：无 注意：指定起止时间会自动忽略分钟和秒，转为整点时刻，例如 2019-4-18 的 [3:35, 4:55) 会被转为 [3:00, 4:00) endDateTime\n描述：和 beginDateTime 配合使用，用于指定哪个时间段内的数据点，需要被迁移 必选：是 格式：yyyy-MM-dd HH:mm:ss 默认值：无 注意：指定起止时间会自动忽略分钟和秒，转为整点时刻，例如 2019-4-18 的 [3:35, 4:55) 会被转为 [3:00, 4:00) 3.3 类型转换 DataX 内部类型 TSDB 数据类型 String TSDB 数据点序列化字符串，包括 timestamp、metric、tags、fields 和 value 4 约束限制 4.2 如果存在某一个 Metric 下在一个小时范围内的数据量过大，可能需要通过 -j 参数调整 JVM 内存大小 考虑到下游 Writer 如果写入速度不及 TSDB Reader 的查询数据，可能会存在积压的情况，因此需要适当地调整 JVM 参数。以\u0026quot;从 阿里云 TSDB 数据库同步抽取数据到本地的作业\u0026quot;为例，启动命令如下：\npython datax/bin/datax.py tsdb2stream.json -j \u0026#34;-Xms4096m -Xmx4096m\u0026#34; 4.3 指定起止时间会自动被转为整点时刻 指定起止时间会自动被转为整点时刻，例如 2019-4-18 的 [3:35, 3:55) 会被转为 [3:00, 4:00)\n","permalink":"http://121.199.2.5:6080/66980d1599cc4aec9a2d6616d1f0c5e7/","summary":"TSDBReader 插件文档 1 快速介绍 TSDBReader 插件实现了从阿里云 TSDB 读取数据。阿里云时间序列数据库 ( Time Series Database , 简称 TSDB) 是一种集时序数据高效读写，压缩存储，实时计算能力为一体的数据库服务，可广泛应用于物联网和互联网领域，实现对设备及业务服务的实时监控，实时预测告警。详见 TSDB 的阿里云官网。\n2 实现原理 在底层实现上，TSDBReader 通过 HTTP 请求链接到 阿里云 TSDB 实例，利用 /api/query 或者 /api/mquery 接口将数据点扫描出来（更多细节详见：时序数据库 TSDB - HTTP API 概览）。而整个同步的过程，是通过时间线和查询时间线范围进行切分。\n3 功能说明 3.1 配置样例 配置一个从 阿里云 TSDB 数据库同步抽取数据到本地的作业，并以时序数据的格式输出： 时序数据样例：\n{\u0026#34;metric\u0026#34;:\u0026#34;m\u0026#34;,\u0026#34;tags\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;a19\u0026#34;,\u0026#34;cluster\u0026#34;:\u0026#34;c5\u0026#34;,\u0026#34;group\u0026#34;:\u0026#34;g10\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;i999\u0026#34;,\u0026#34;zone\u0026#34;:\u0026#34;z1\u0026#34;},\u0026#34;timestamp\u0026#34;:1546272263,\u0026#34;value\u0026#34;:1} { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;sinkDbType\u0026#34;: \u0026#34;TSDB\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;splitIntervalMs\u0026#34;: 60000, \u0026#34;beginDateTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endDateTime\u0026#34;: \u0026#34;2019-01-01 01:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;streamwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;print\u0026#34;: true } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取数据到本地的作业，并以关系型数据的格式输出： 关系型数据样例：","title":"TSDBReader 插件文档"},{"content":"TSDBWriter 插件文档 1 快速介绍 TSDBWriter 插件实现了将数据点写入到阿里巴巴自研 TSDB 数据库中（后续简称 TSDB）。\n时间序列数据库（Time Series Database , 简称 TSDB）是一种高性能，低成本，稳定可靠的在线时序数据库服务；提供高效读写，高压缩比存储、时序数据插值及聚合计算，广泛应用于物联网（IoT）设备监控系统 ，企业能源管理系统（EMS），生产安全监控系统，电力检测系统等行业场景。 TSDB 提供百万级时序数据秒级写入，高压缩比低成本存储、预降采样、插值、多维聚合计算，查询结果可视化功能；解决由于设备采集点数量巨大，数据采集频率高，造成的存储成本高，写入和查询分析效率低的问题。更多关于 TSDB 的介绍，详见阿里云 TSDB 官网。\n2 实现原理 通过 HTTP 连接 TSDB 实例，并通过 /api/put 接口将数据点写入。关于写入接口详见 TSDB 的接口说明文档。\n3 功能说明 3.1 配置样例 配置一个从 OpenTSDB 数据库同步抽取数据到 TSDB： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;opentsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:4242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;startTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2019-01-01 03:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbhttpwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } } } } 3.2 参数说明 name\n描述：本插件的名称 必选：是 默认值：tsdbhttpwriter parameter\nendpoint 描述：TSDB 的 HTTP 连接地址 必选：是 格式：http://IP:Port 默认值：无 batchSize\n描述：每次批量数据的条数 必选：否 格式：int，需要保证大于 0 默认值：100 maxRetryTime\n描述：失败后重试的次数 必选：否 格式：int，需要保证大于 1 默认值：3 ignoreWriteError\n描述：如果设置为 true，则忽略写入错误，继续写入；否则，多次重试后仍写入失败的话，则会终止写入任务 必选：否 格式：bool 默认值：false 3.3 类型转换 DataX 内部类型 TSDB 数据类型 String TSDB 数据点序列化字符串，包括 timestamp、metric、tags 和 value 4 性能报告 4.1 环境准备 4.1.1 数据特征 从 Metric、时间线、Value 和 采集周期 四个方面来描述：\nmetric 固定指定一个 metric 为 m。\ntagkv 前四个 tagkv 全排列，形成 10 * 20 * 100 * 100 = 2000000 条时间线，最后 IP 对应 2000000 条时间线从 1 开始自增。\ntag_k tag_v zone z1~z10 cluster c1~c20 group g1~100 app a1~a100 ip ip1~ip2000000 value 度量值为 [1, 100] 区间内的随机值\ninterval 采集周期为 10 秒，持续摄入 3 小时，总数据量为 3 * 60 * 60 / 10 * 2000000 = 2,160,000,000 个数据点。\n4.1.2 机器参数 TSDB Writer 机型: 64C256G\nHBase 机型： 8C16G * 5\n4.1.3 DataX jvm 参数 \u0026ldquo;-Xms4096m -Xmx4096m\u0026rdquo;\n4.2 测试报告 通道数 DataX 速度 (Rec/s) DataX 流量 (MB/s) 1 129753 15.45 2 284953 33.70 3 385868 45.71 5 约束限制 5.1 目前只支持兼容 TSDB 2.4.x 及以上版本 其他版本暂不保证兼容\n6 FAQ ","permalink":"http://121.199.2.5:6080/2a52431a701d4468a150a7986b3b4752/","summary":"TSDBWriter 插件文档 1 快速介绍 TSDBWriter 插件实现了将数据点写入到阿里巴巴自研 TSDB 数据库中（后续简称 TSDB）。\n时间序列数据库（Time Series Database , 简称 TSDB）是一种高性能，低成本，稳定可靠的在线时序数据库服务；提供高效读写，高压缩比存储、时序数据插值及聚合计算，广泛应用于物联网（IoT）设备监控系统 ，企业能源管理系统（EMS），生产安全监控系统，电力检测系统等行业场景。 TSDB 提供百万级时序数据秒级写入，高压缩比低成本存储、预降采样、插值、多维聚合计算，查询结果可视化功能；解决由于设备采集点数量巨大，数据采集频率高，造成的存储成本高，写入和查询分析效率低的问题。更多关于 TSDB 的介绍，详见阿里云 TSDB 官网。\n2 实现原理 通过 HTTP 连接 TSDB 实例，并通过 /api/put 接口将数据点写入。关于写入接口详见 TSDB 的接口说明文档。\n3 功能说明 3.1 配置样例 配置一个从 OpenTSDB 数据库同步抽取数据到 TSDB： { \u0026#34;job\u0026#34;: { \u0026#34;content\u0026#34;: [ { \u0026#34;reader\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;opentsdbreader\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:4242\u0026#34;, \u0026#34;column\u0026#34;: [ \u0026#34;m\u0026#34; ], \u0026#34;startTime\u0026#34;: \u0026#34;2019-01-01 00:00:00\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2019-01-01 03:00:00\u0026#34; } }, \u0026#34;writer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;tsdbhttpwriter\u0026#34;, \u0026#34;parameter\u0026#34;: { \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:8242\u0026#34; } } } ], \u0026#34;setting\u0026#34;: { \u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 1 } } } } 3.2 参数说明 name\n描述：本插件的名称 必选：是 默认值：tsdbhttpwriter parameter\nendpoint 描述：TSDB 的 HTTP 连接地址 必选：是 格式：http://IP:Port 默认值：无 batchSize","title":"TSDBWriter 插件文档"},{"content":"阿里云开源离线同步工具DataX3.0介绍 一. DataX\u00083.0概览 ​\tDataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。\n设计理念 为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。\n当前使用现状 DataX在阿里巴巴集团内被广泛使用，承担了所有大数据的离线同步业务，并已持续稳定运行了6年之久。目前每天完成同步8w多道作业，每日传输数据量超过300TB。\n此前已经开源DataX1.0版本，此次介绍为阿里云开源全新版本DataX3.0，有了更多更强大的功能和更好的使用体验。Github主页地址：https://github.com/alibaba/DataX\n二、DataX3.0框架设计 DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。\nReader：Reader\u0008为数据采集模块，负责采集数据源的数据，将数据发送给Framework。 Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。 Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。 三. DataX3.0插件体系 ​\t经过几年积累，DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入。DataX目前支持数据如下：\n类型 数据源 Reader(读) Writer(写) 文档 RDBMS 关系型数据库 MySQL √ √ 读 、写 Oracle √ √ 读 、写 SQLServer √ √ 读 、写 PostgreSQL √ √ 读 、写 DRDS √ √ 读 、写 达梦 √ √ 读 、写 通用RDBMS(支持所有关系型数据库) √ √ 读 、写 阿里云数仓数据存储 ODPS √ √ 读 、写 ADS √ 写 OSS √ √ 读 、写 OCS √ √ 读 、写 NoSQL数据存储 OTS √ √ 读 、写 Hbase0.94 √ √ 读 、写 Hbase1.1 √ √ 读 、写 MongoDB √ √ 读 、写 Hive √ √ 读 、写 无结构化数据存储 TxtFile √ √ 读 、写 FTP √ √ 读 、写 HDFS √ √ 读 、写 Elasticsearch √ 写 DataX Framework提供了简单的接口与插件交互，提供简单的插件接入机制，只需要任意加上一种插件，就能无缝对接其他数据源。详情请看：DataX数据源指南\n\u0008四、DataX3.0核心架构 DataX 3.0 开源版本支持单机多线程模式完成同步作业运行，本小节按一个DataX作业生命周期的时序图，从整体架构设计非常简要说明DataX各个模块相互关系。\n核心模块介绍： DataX完成单个数据同步的作业，我们称之为Job，DataX接受到一个Job之后，将启动一个进程来完成整个作业同步过程。DataX Job模块是单个作业的中枢管理节点，承担了数据清理、子任务切分(将单一作业计算转化为多个子Task)、TaskGroup管理等功能。 DataXJob启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。Task便是DataX作业的最小单元，每一个Task都会负责一部分数据的同步工作。 切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup(任务组)。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，默认单个任务组的并发数量为5。 每一个Task都由TaskGroup负责启动，Task启动后，会固定启动Reader—\u0026gt;Channel—\u0026gt;Writer的线程来完成任务同步工作。 DataX作业运行起来之后， Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出，进程退出值非0 DataX调度流程： 举例来说，用户提交了一个DataX作业，并且配置了20个并发，目的是将一个100张分表的mysql数据同步到odps里面。\tDataX的调度决策思路是：\nDataXJob根据分库分表切分成了100个Task。 根据20个并发，DataX计算共需要分配4个TaskGroup。 4个TaskGroup平分切分好的100个Task，每一个TaskGroup负责以5个并发共计运行25个Task。 五、DataX 3.0六大核心优势 可靠的数据质量监控 完美解决数据传输个别类型失真问题\nDataX旧版对于部分数据类型(比如时间戳)传输一直存在毫秒阶段等数据失真情况，新版本DataX3.0已经做到支持所有的强数据类型，每一种插件都有自己的数据类型转换策略，让数据可以完整无损的传输到目的端。\n提供作业全链路的流量、数据量\u0008运行时监控\nDataX3.0运行过程中可以将作业本身状态、数据流量、数据速度、执行进度等信息进行全面的展示，让用户可以实时了解作业状态。并可在作业执行过程中智能判断源端和目的端的速度对比情况，给予用户更多性能排查信息。\n提供脏数据探测\n在大量数据的传输过程中，必定会由于各种原因导致很多数据传输报错(比如类型转换错误)，这种数据DataX认为就是脏数据。DataX目前可以实现脏数据精确过滤、识别、采集、展示，为用户提供多种的脏数据处理模式，让用户准确把控数据质量大关！\n丰富的数据转换功能 DataX作为一个服务于大数据的ETL工具，除了提供数据快照搬迁功能之外，还提供了丰富数据转换的功能，让数据在传输过程中可以轻松完成数据脱敏，补全，过滤等数据转换功能，另外还提供了自动groovy函数，让用户自定义转换函数。详情请看DataX3的transformer详细介绍。\n精准的速度控制 还在为同步过程对在线存储压力影响而担心吗？新版本DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在库可以承受的范围内达到最佳的同步速度。\n\u0026#34;speed\u0026#34;: { \u0026#34;channel\u0026#34;: 5, \u0026#34;byte\u0026#34;: 1048576, \u0026#34;record\u0026#34;: 10000 } 强劲的同步性能 DataX3.0每一种读插件都有一种或多种切分策略，都能将作业合理切分成多个Task并行执行，单机多线程执行模型可以让DataX速度随并发成线性增长。在源端和目的端性能都足够的情况下，单个作业一定可以打满网卡。另外，DataX团队对所有的已经接入的插件都做了极致的性能优化，并且做了完整的性能测试。性能测试相关详情可以参照每单个数据源的详细介绍：DataX数据源指南\n健壮的容错机制 DataX作业是极易受外部因素的干扰，网络闪断、数据源不稳定等因素很容易让同步到一半的作业报错停止。因此稳定性是DataX的基本要求，在DataX 3.0的设计中，重点完善了框架和插件的稳定性。目前DataX3.0可以做到线程级别、进程级别(暂时未开放)、作业级别多层次局部/全局的重试，保证用户的作业稳定运行。\n线程内部重试\nDataX的核心插件都经过团队的全盘review，不同的网络交互方式都有不同的重试策略。\n线程级别重试\n目前DataX已经可以实现TaskFailover，针对于中间失败的Task，DataX框架可以做到整个Task级别的重新调度。\n极简的使用体验 易用\n下载即可用，支持linux和windows，只需要短短几步骤就可以完成数据的传输。请点击：Quick Start\n详细\nDataX在运行日志中打印了大量信息，其中包括传输速度，Reader、Writer性能，进程CPU，JVM和GC情况等等。\n传输过程中打印传输速度、进度等\n传输过程中会打印进程相关的CPU、JVM等\n在任务结束之后，打印总体运行情况\n","permalink":"http://121.199.2.5:6080/408be8b90df543789f79c7b43375e5a3/","summary":"阿里云开源离线同步工具DataX3.0介绍 一. DataX\u00083.0概览 ​\tDataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。\n设计理念 为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。\n当前使用现状 DataX在阿里巴巴集团内被广泛使用，承担了所有大数据的离线同步业务，并已持续稳定运行了6年之久。目前每天完成同步8w多道作业，每日传输数据量超过300TB。\n此前已经开源DataX1.0版本，此次介绍为阿里云开源全新版本DataX3.0，有了更多更强大的功能和更好的使用体验。Github主页地址：https://github.com/alibaba/DataX\n二、DataX3.0框架设计 DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。\nReader：Reader\u0008为数据采集模块，负责采集数据源的数据，将数据发送给Framework。 Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。 Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。 三. DataX3.0插件体系 ​\t经过几年积累，DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入。DataX目前支持数据如下：\n类型 数据源 Reader(读) Writer(写) 文档 RDBMS 关系型数据库 MySQL √ √ 读 、写 Oracle √ √ 读 、写 SQLServer √ √ 读 、写 PostgreSQL √ √ 读 、写 DRDS √ √ 读 、写 达梦 √ √ 读 、写 通用RDBMS(支持所有关系型数据库) √ √ 读 、写 阿里云数仓数据存储 ODPS √ √ 读 、写 ADS √ 写 OSS √ √ 读 、写 OCS √ √ 读 、写 NoSQL数据存储 OTS √ √ 读 、写 Hbase0.94 √ √ 读 、写 Hbase1.1 √ √ 读 、写 MongoDB √ √ 读 、写 Hive √ √ 读 、写 无结构化数据存储 TxtFile √ √ 读 、写 FTP √ √ 读 、写 HDFS √ √ 读 、写 Elasticsearch √ 写 DataX Framework提供了简单的接口与插件交互，提供简单的插件接入机制，只需要任意加上一种插件，就能无缝对接其他数据源。详情请看：DataX数据源指南","title":"阿里云开源离线同步工具DataX3.0介绍"},{"content":"mysql replace 可以将字段的字符串内容替换，语法：\nUpdate `table_name` SET `field_name` = replace (`field_name`,’from_str’,\u0026#39;to_str’) Where `field_name` LIKE ‘%from_str%’ 使用样例：\n# 将 blog 表的字段 coentent 里的 \u0026#34;https://haokiu.com\u0026#34; 替换为 \u0026#34;http://haokiu.com\u0026#34; update blog set content = replace(content, \u0026#34;https://haokiu.com\u0026#34;, \u0026#34;http://haokiu.com\u0026#34;) where content like \u0026#39;%https://haokiu.com%\u0026#39; ","permalink":"http://121.199.2.5:6080/nfvw4R/","summary":"mysql replace 可以将字段的字符串内容替换，语法：\nUpdate `table_name` SET `field_name` = replace (`field_name`,’from_str’,\u0026#39;to_str’) Where `field_name` LIKE ‘%from_str%’ 使用样例：\n# 将 blog 表的字段 coentent 里的 \u0026#34;https://haokiu.com\u0026#34; 替换为 \u0026#34;http://haokiu.com\u0026#34; update blog set content = replace(content, \u0026#34;https://haokiu.com\u0026#34;, \u0026#34;http://haokiu.com\u0026#34;) where content like \u0026#39;%https://haokiu.com%\u0026#39; ","title":"mysql replace "},{"content":"什么是 Vim？ Vim 是一个历史悠久的文本编辑器，可以追溯到 qed。 Bram Moolenaar 于 1991 年发布初始版本。\nLinux、Mac 用户，可以使用包管理器安装 Vim，对于 Windows 用户，可以从 我的网盘 下载。 该版本可轻易添加 python 、python3 、lua 等支持，只需要安装 python、lua 即可。\n项目在 Github 上开发，项目讨论请订阅 vim_dev 邮件列表。\n通过阅读 Why, oh WHY, do those #?@! nutheads use vi? 来对 Vim 进行大致的了解。\nVim 哲学 Vim 采用模式编辑的理念，即它提供了多种模式，按键在不同的模式下作用不同。 你可以在普通模式 下浏览文件，在插入模式下插入文本， 在可视模式下选择行，在命令模式下执行命令等等。起初这听起来可能很复杂， 但是这有一个很大的优点：不需要通过同时按住多个键来完成操作， 大多数时候你只需要依次按下这些按键即可。越常用的操作，所需要的按键数量越少。\n和模式编辑紧密相连的概念是 操作符 和 动作。操作符 指的是开始某个行为， 例如：修改、删除或者选择文本，之后你要用一个 动作 来指定需要操作的文本区域。 比如，要改变括号内的文本，需要执行 ci( （读做 change inner parentheses）； 删除整个段落的内容，需要执行 dap （读做：delete around paragraph）。\n如果你能看见 Vim 老司机操作，你会发现他们使用 Vim 脚本语言就如同钢琴师弹钢琴一样。复杂的操作只需要几个按键就能完成。他们甚至不用刻意去想，因为这已经成为肌肉记忆了。这减少认识负荷并帮助人们专注于实际任务。\n入门 Vim 自带一个交互式的教程，内含你需要了解的最基础的信息，你可以通过终端运行以下命令打开教程：\n$ vimtutor 不要因为这个看上去很无聊而跳过，按照此教程多练习。你以前用的 IDE 或者其他编辑器很少是有“模式”概念的，因此一开始你会很难适应模式切换。但是你 Vim 使用的越多，肌肉记忆 将越容易形成。\nVim 基于一个 vi 克隆，叫做 Stevie，支持两种运行模式：\u0026ldquo;compatible\u0026rdquo; 和 \u0026ldquo;nocompatible\u0026rdquo;。在兼容模式下运行 Vim 意味着使用 vi 的默认设置，而不是 Vim 的默认设置。除非你新建一个用户的 vimrc 或者使用 vim -N 命令启动 Vim，否则就是在兼容模式下运行 Vim！请大家不要在兼容模式下运行 Vim。\n下一步\n创建你自己的 vimrc。 在第一周准备备忘录。 通读基础章节了解 Vim 还有哪些功能。 按需学习！Vim 是学不完的。如果你遇到了问题，先上网寻找解决方案，你的问题可能已经被解决了。Vim 拥有大量的参考文档，知道如何利用这些参考文档很有必要：获取离线帮助。 浏览附加资源。 最后一个建议：使用插件之前，请先掌握 Vim 的基本操作。很多插件都只是对 Vim 自带功能的封装。\n返回主目录 ⤴️\n精简的 vimrc Vim 启动是会按照一定的优先顺序来搜索配置文件，这个顺序，可以通过 :version 命令查看。下面分 Windows 系统， 和 *niux 系统分别来说明 Vim 是如何载入配置文件的。\nWindows 系统 system vimrc file: \u0026#34;$VIM\\vimrc\u0026#34; user vimrc file: \u0026#34;$HOME\\_vimrc\u0026#34; 2nd user vimrc file: \u0026#34;$HOME\\vimfiles\\vimrc\u0026#34; 3rd user vimrc file: \u0026#34;$VIM\\_vimrc\u0026#34; user exrc file: \u0026#34;$HOME\\_exrc\u0026#34; 2nd user exrc file: \u0026#34;$VIM\\_exrc\u0026#34; system gvimrc file: \u0026#34;$VIM\\gvimrc\u0026#34; user gvimrc file: \u0026#34;$HOME\\_gvimrc\u0026#34; 2nd user gvimrc file: \u0026#34;$HOME\\vimfiles\\gvimrc\u0026#34; 3rd user gvimrc file: \u0026#34;$VIM\\_gvimrc\u0026#34; defaults file: \u0026#34;$VIMRUNTIME\\defaults.vim\u0026#34; system menu file: \u0026#34;$VIMRUNTIME\\menu.vim\u0026#34; 我们只看上面这一段，Vim 会优先读取 user vimrc file: $HOME\\_vimrc, 当这一文件不存在是， Vim 再去寻找 2nd user vimrc file: $HOME\\vimfiles\\vimrc; 倘若这个文件还是不存在，那么 Vim 会去继续寻找 3rd user vimrc file: $VIM\\_vimrc。 了解以上顺序后，就不会再因为 Vim 总是不读取配置文件而感到烦恼了。\nLinux 或者 Mac OS 同 Windows 系统类似，也可以使用 :version 命令查看 vim 载入配置的优先顺序。\n系统 vimrc 文件: \u0026#34;/etc/vimrc\u0026#34; 用户 vimrc 文件: \u0026#34;$HOME/.vimrc\u0026#34; 第二用户 vimrc 文件: \u0026#34;~/.vim/vimrc\u0026#34; 用户 exrc 文件: \u0026#34;$HOME/.exrc\u0026#34; defaults file: \u0026#34;$VIMRUNTIME/defaults.vim\u0026#34; $VIM 预设值: \u0026#34;/etc\u0026#34; $VIMRUNTIME 预设值: \u0026#34;/usr/share/vim/vim81\u0026#34; 你可以在网上找到许多精简的 vimrc 配置文件，我的版本可能并不是最简单的版本，但是我的版本提供了一套我认为良好的，非常适合入门的设置。\n最终你需要阅读完那些设置，然后自行决定需要使用哪些。:-)\n精简的 vimrc 地址：minimal-vimrc\n如果你有兴趣，这里是我（原作者）的 vimrc。\n建议：大多数插件作者都维护不止一个插件并且将他们的 vimrc 放在 Github 上展示（通常放在叫做 \u0026ldquo;vim-config\u0026rdquo; 或者 \u0026ldquo;dotfiles\u0026rdquo; 的仓库中），所以当你发现你喜欢的插件时，去插件维护者的 Github 主页看看有没有这样的仓库。\n返回主目录 ⤴️\n我正在使用什么样的 Vim 使用 :version 命令将向你展示当前正在运行的 Vim 的所有相关信息，包括它是如何编译的。\n第一行告诉你这个二进制文件的编译时间和版本号，比如：7.4。接下来的一行呈现 Included patches: 1-1051，这是补丁版本包。因此你 Vim 确切的版本号是 7.4.1051。\n另一行显示着一些像 Tiny version without GUI 或者 Huge version with GUI 的信息。很显然这些信息告诉你当前的 Vim 是否支持 GUI，例如：从终端中运行 gvim 或者从终端模拟器中的 Vim 内运行 :gui 命令。另一个重要的信息是 Tiny 和 Huge。Vim 的特性集区分被叫做 tiny，small，normal，big and huge，所有的都实现不同的功能子集。\n:version 主要的输出内容是特性列表。+clipboard 意味这剪贴板功能被编译支持了，-clipboard 意味着剪贴板特性没有被编译支持。\n一些功能特性需要编译支持才能正常工作。例如：为了让 :prof 工作，你需要使用 huge 模式编译的 Vim，因为那种模式启用了 +profile 特性。\n如果你的输出情况并不是那样，并且你是从包管理器安装 Vim 的，确保你安装了 vim-x，vim-x11，vim-gtk，vim-gnome 这些包或者相似的，因为这些包通常都是 huge 模式编译的。\n你也可以运行下面这段代码来测试 Vim 版本以及功能支持：\n\u0026#34; Do something if running at least Vim 7.4.42 with +profile enabled. if (v:version \u0026gt; 704 || v:version == 704 \u0026amp;\u0026amp; has(\u0026#39;patch42\u0026#39;)) \u0026amp;\u0026amp; has(\u0026#39;profile\u0026#39;) \u0026#34; do stuff endif 相关帮助：\n:h :version :h feature-list :h +feature-list :h has-patch 返回主目录 ⤴️\n备忘录 为了避免版权问题，我只贴出链接：\nhttp://people.csail.mit.edu/vgod/vim/vim-cheat-sheet-en.png https://cdn.shopify.com/s/files/1/0165/4168/files/preview.png http://www.nathael.org/Data/vi-vim-cheat-sheet.svg http://michael.peopleofhonoronly.com/vim/vim_cheat_sheet_for_programmers_screen.png http://www.rosipov.com/images/posts/vim-movement-commands-cheatsheet.png 或者在 Vim 中快速打开备忘录：vim-cheat40。\n返回主目录 ⤴️\n基础 缓冲区，窗口，标签 Vim 是一个文本编辑器。每次文本都是作为缓冲区的一部分显示的。每一份文件都是在他们自己独有的缓冲区打开的，插件显示的内容也在它们自己的缓冲区中。\n缓冲区有很多属性，比如这个缓冲区的内容是否可以修改，或者这个缓冲区是否和文件相关联，是否需要同步保存到磁盘上。\n窗口 是缓冲区上一层的视窗。如果你想同时查看几个文件或者查看同一文件的不同位置，那样你会需要窗口。\n请别把他们叫做 分屏 。你可以把一个窗口分割成两个，但是这并没有让这两个窗口完全 分离 。\n窗口可以水平或者竖直分割并且现有窗口的高度和宽度都是可以被调节设置的，因此，如果你需要多种窗口布局，请考虑使用标签。\n标签页 （标签）是窗口的集合。因此当你想使用多种窗口布局时候请使用标签。\n简单的说，如果你启动 Vim 的时候没有附带任何参数，你会得到一个包含着一个呈现一个缓冲区的窗口的标签。\n顺带提一下，缓冲区列表是全局可见的，你可以在任何标签中访问任何一个缓冲区。\n返回主目录 ⤴️\n已激活、已载入、已列出、已命名的缓冲区 用类似 vim file1 的命令启动 Vim 。这个文件的内容将会被加载到缓冲区中，你现在有一个已载入的缓冲区。如果你在 Vim 中保存这个文件，缓冲区内容将会被同步到磁盘上（写回文件中）。\n由于这个缓冲区也在一个窗口上显示，所以他也是一个已激活的缓冲区。如果你现在通过 :e file2 命令加载另一个文件，file1 将会变成一个隐藏的缓冲区，并且 file2 变成已激活缓冲区。\n使用 :ls 我们能够列出所有可以列出的缓冲区。插件缓冲区和帮助缓冲区通常被标记为不可以列出的缓冲区，因为那并不是你经常需要在编辑器中编辑的常规文件。通过 :ls! 命令可以显示被放入缓冲区列表的和未被放入列表的缓冲区。\n未命名的缓冲区是一种没有关联特定文件的缓冲区，这种缓冲区经常被插件使用。比如 :enew 将会创建一个无名临时缓冲区。添加一些文本然后使用 :w /tmp/foo 将他写入到磁盘，这样这个缓冲区就会变成一个已命名的缓冲区。\n返回主目录 ⤴️\n参数列表 全局缓冲区列表是 Vim 的特性。在这之前的 vi 中，仅仅只有参数列表，参数列表在 Vim 中依旧可以使用。\n每一个通过 shell 命令传递给 Vim 的文件名都被记录在一个参数列表中。可以有多个参数列表：默认情况下所有参数都被放在全局参数列表下，但是你可以使用 :arglocal 命令去创建一个新的本地窗口的参数列表。\n使用 :args 命令可以列出当前参数。使用 :next，:previous，:first，:last 命令可以在切换在参数列表中的文件。通过使用 :argadd，:argdelete 或者 :args 等命令加上一个文件列表可以改变参数列表。\n偏爱缓冲区列表还是参数列表完全是个人选择，我的印象中大多数人都是使用缓冲区列表的。\n然而参数列表在有些情况下被大量使用：批处理 使用 :argdo！ 一个简单的重构例子：\n:args **/*.[ch] :argdo %s/foo/bar/ge | update 这条命令将替换掉当前目录下以及当前目录的子目录中所有的 C 源文件和头文件中的“foo”，并用“bar”代替。\n相关帮助：:h argument-list\n返回主目录 ⤴️\n按键映射 使用 :map 命令家族你可以定义属于你自己的快捷键。该家族的每一个命令都限定在特定的模式下。从技术上来说 Vim 自带高达 12 中模式，其中 6 种可以被映射。另外一些命令作用于多种模式：\n递归 非递归 模式 :map :noremap normal, visual, operator-pending :nmap :nnoremap normal :xmap :xnoremap visual :cmap :cnoremap command-line :omap :onoremap operator-pending :imap :inoremap insert 例如：这个自定义的快捷键只在普通模式下工作。\n:nmap \u0026lt;space\u0026gt; :echo \u0026#34;foo\u0026#34;\u0026lt;cr\u0026gt; 使用 :nunmap \u0026lt;space\u0026gt; 可以取消这个映射。\n对于更少数，不常见的模式（或者他们的组合），查看 :h map-modes。\n到现在为止还好，对新手而言有一个问题会困扰他们：:nmap 是递归执行的！结果是，右边执行可能的映射。\n你自定义了一个简单的映射去输出“Foo”：\n:nmap b :echo \u0026#34;Foo\u0026#34;\u0026lt;cr\u0026gt; 但是如果你想要映射 b （回退一个单词）的默认功能到一个键上呢？\n:nmap a b 如果你敲击a，我们期望着光标回退到上一个单词，但是实际情况是“Foo”被输出到命令行里！因为在右边，b 已经被映射到别的行为上了，换句话说就是 :echo \u0026quot;Foo\u0026quot;\u0026lt;cr\u0026gt;。\n解决此问题的正确方法是使用一种 非递归 的映射代替：\n:nnoremap a b 经验法则：除递归映射是必须的，否则总是使用非递归映射。\n通过不给一个右值来检查你的映射。比如:nmap 显示所以普通模式下的映射，:nmap \u0026lt;leader\u0026gt; 显示所有以 \u0026lt;leader\u0026gt; 键开头的普通模式下的映射。\n如果你想禁止用标准映射，把他们映射到特殊字符 \u0026lt;nop\u0026gt; 上，例如：:noremap \u0026lt;left\u0026gt; \u0026lt;nop\u0026gt;。\n相关帮助：\n:h key-notation :h mapping :h 05.3 返回主目录 ⤴️\n映射前置键 映射前置键（Leader 键）本身就是一个按键映射，默认为 \\。我们可以通过在 map 中调用 \u0026lt;leader\u0026gt; 来为把它添加到其他按键映射中。\nnnoremap \u0026lt;leader\u0026gt;h :helpgrep\u0026lt;space\u0026gt; 这样，我们只需要先按 \\ 然后按 h 就可以激活这个映射 :helpgrep\u0026lt;space\u0026gt;。如果你想通过先按 空格 键来触发，只需要这样做：\nlet g:mapleader = \u0026#39; \u0026#39; nnoremap \u0026lt;leader\u0026gt;h :helpgrep\u0026lt;space\u0026gt; 此处建议使用 g:mapleader，因为在 Vim 脚本中，函数外的变量缺省的作用域是全局变量，但是在函数内缺省作用域是局部变量，而设置快捷键前缀需要修改全局变量 g:mapleader 的值。\n另外，还有一个叫 \u0026lt;localleader\u0026gt; 的，可以把它理解为局部环境中的 \u0026lt;leader\u0026gt;，默认值依然为 \\。当我们需要只对某一个条件下（比如，特定文件类型的插件）的缓冲区设置特别的 \u0026lt;leader\u0026gt; 键，那么我们就可以通过修改当前环境下的 \u0026lt;localleader\u0026gt; 来实现。\n注意：如果你打算设置 Leader 键，请确保在设置按键映射之前，先设置好 Leader 键。如果你先设置了含有 Leader 键的映射，然后又修改了 Leader 键，那么之前映射内的 Leader 键是不会因此而改变的。你可以通过执行 :nmap \u0026lt;leader\u0026gt; 来查看普通模式中已绑定给 Leader 键的所有映射。\n请参阅 :h mapleader 与 :h maploacalleader 来获取更多帮助。\n返回主目录 ⤴️\n寄存器 寄存器就是存储文本的地方。我们常用的「复制」操作就是把文本存储到寄存器，「 粘贴」 操作就是把文本从寄存器中读出来。顺便，在 Vim 中复制的快捷键是 y，粘贴的快捷键是 p。\nVim 为我们提供了如下的寄存器：\n类型 标识 读写者 是否为只读 包含的字符来源 Unnamed \u0026quot; vim 否 最近一次的复制或删除操作 (d, c, s, x, y) Numbered 0至9 vim 否 寄存器 0: 最近一次复制。寄存器 1: 最近一次删除。寄存器 2: 倒数第二次删除，以此类推。对于寄存器 1 至 9，他们其实是只读的最多包含 9 个元素的队列。这里的队列即为数据类型 queue Small delete - vim 否 最近一次行内删除 Named a至z, A至Z 用户 否 如果你通过复制操作存储文本至寄存器 a，那么 a 中的文本就会被完全覆盖。如果你存储至 A，那么会将文本添加给寄存器 a，不会覆盖之前已有的文本 Read-only :与.和% vim 是 :: 最近一次使用的命令，.: 最近一次添加的文本，%: 当前的文件名 Alternate buffer # vim 否 大部分情况下，这个寄存器是当前窗口中，上一次访问的缓冲区。请参阅 :h alternate-file 来获取更多帮助 Expression = 用户 否 复制 VimL 代码时，这个寄存器用于存储代码片段的执行结果。比如，在插入模式下复制 \u0026lt;c-r\u0026gt;=5+5\u0026lt;cr\u0026gt;，那么这个寄存器就会存入 10 Selection +和* vim 否 * 和 + 是 剪贴板 寄存器 Drop ~ vim 是 最后一次拖拽添加至 Vim 的文本（需要 \u0026ldquo;+dnd\u0026rdquo; 支持，暂时只支持 GTK GUI。请参阅 :help dnd 及 :help quote~） Black hole _ vim 否 一般称为黑洞寄存器。对于当前操作，如果你不希望在其他寄存器中保留文本，那就在命令前加上 _。比如，\u0026quot;_dd 命令不会将文本放到寄存器 \u0026quot;、1、+ 或 * 中 Last search pattern / vim 否 最近一次通过 /、? 或 :global 等命令调用的匹配条件 只要不是只读的寄存器，用户都有权限修改它的内容，比如：\n:let @/ = \u0026#39;register\u0026#39; 这样，我们按 n 的时候就会跳转到单词\u0026quot;register\u0026quot; 出现的地方。\n有些时候，你的操作可能已经修改了寄存器，而你没有察觉到。请参阅 :h registers 获取更多帮助。\n上面提到过，复制的命令是 y，粘贴的命令是 p 或者 P。但请注意，Vim 会区分「字符选取」与「行选取」。请参阅 :h linewise 获取更多帮助。\n行选取： 命令 yy 或 Y 都是复制当前行。这时移动光标至其他位置，按下 p 就可以在光标下方粘贴复制的行，按下 P 就可以在光标上方粘贴至复制的行。\n字符选取： 命令 0yw 可以复制第一个单词。这时移动光标至其他位置，按下 p 就可以在当前行、光标后的位置粘贴单词，按下 P 就可以在当前行、光标前的位置粘贴单词。\n将文本存到指定的寄存器中： 命令 \u0026quot;aY 可以将当前行复制，并存储到寄存器 a 中。这时移动光标至其他位置，通过命令 \u0026quot;AY 就可以把这一行的内容扩展到寄存器 a 中，而之前存储的内容也不会丢失。\n为了便于理解和记忆，建议大家现在就试一试上面提到的这些操作。操作过程中，你可以随时通过 :reg 来查看寄存器的变化。\n有趣的是： 在 Vim 中，y 是复制命令，源于单词 \u0026ldquo;yanking\u0026rdquo;。而在 Emacs 中，\u0026ldquo;yanking\u0026rdquo; 代表的是粘贴（或者说，重新插入刚才删掉的内容），而并不是复制。\n返回主目录 ⤴️\n范围 范围 (Ranges) 其实很好理解，但很多 Vim 用户的理解不到位。\n很多命令都可以加一个数字，用于指明操作范围 范围可以是一个行号，用于指定某一行 范围也可以是一对通过 , 或 ; 分割的行号 大部分命令，默认只作用于当前行 只有 :write 和 :global 是默认作用于所有行的 范围的使用是十分直观的。以下为一些例子（其中，:d 为 :delete 的缩写）：\n命令 操作的行 :d 当前行 :.d 当前行 :1d 第一行 :$d 最后一行 :1,$d 所有行 :%d 所有行（这是 1,$ 的语法糖） :.,5d 当前行至第 5 行 :,5d 同样是当前行至第 5 行 :,+3d 当前行及接下来的 3 行 :1,+3d 第一行至当前行再加 3 行 :,-3d 当前行及向上的 3 行（Vim 会弹出提示信息，因为这是一个保留的范围） :3,'xdelete 第三行至标注 为 x 的那一行 :/^foo/,$delete 当前行以下，以字符 \u0026ldquo;foo\u0026rdquo; 开头的那一行至结尾 :/^foo/+1,$delete 当前行以下，以字符 \u0026ldquo;foo\u0026rdquo; 开头的那一行的下一行至结尾 需要注意的是，; 也可以用于表示范围。区别在于，a,b 的 b 是以当前行作为参考的。而 a;b 的 b 是以 a 行作为参考的。举个例子，现在你的光标在第 5 行。这时 :1,+1d 会删除第 1 行至第 6 行，而 :1;+1d 会删除第 1 行和第 2 行。\n如果你想设置多个寻找条件，只需要在条件前加上 /，比如：\n:/foo//bar//quux/d 这就会删除当前行之后的某一行。定位方式是，先在当前行之后寻找第一个包含 \u0026ldquo;foo\u0026rdquo; 字符的那一行，然后在找到的这一行之后寻找第一个包含 \u0026ldquo;bar\u0026rdquo; 字符的那一行，然后再在找到的这一行之后寻找第一个包含 \u0026ldquo;quux\u0026rdquo; 的那一行。删除的就是最后找到的这一行。\n有时，Vim 会在命令前自动添加范围。举个例子，如果你先通过 V 命令进入行选取模式，选中一些行后按下 : 进入命令模式，这时候你会发现 Vim 自动添加了 '\u0026lt;,'\u0026gt; 范围。这表示，接下来的命令会使用之前选取的行号作为范围。但如果后续命令不支持范围，Vim 就会报错。为了避免这样的情况发生，有些人会设置这样的按键映射：:vnoremap foo :\u0026lt;c-u\u0026gt;command，组合键 Ctrl + u 可以清除当前命令行中的内容。\n另一个例子是在普通模式中按下 !!，命令行中会出现 :.!。如果这时你如果输入一个外部命令，那么当前行的内容就会被这个外部命令的输出替换。你也可以通过命令 :?^$?+1,/^$/-1!ls 把当前段落的内容替换成外部命令 ls 的输出，原理是向前和向后各搜索一个空白行，删除这两个空白行之间的内容，并将外部命令 ls 的输出放到这两个空白行之间。\n请参阅以下两个命令来获取更多帮助：\n:h cmdline-ranges :h 10.3 返回主目录 ⤴️\n标注 你可以使用标注功能来标记一个位置，也就是记录文件某行的某个位置。\n标注 设置者 使用 a-z 用户 仅对当前的一个文件生效，也就意味着只可以在当前文件中跳转 A-Z 用户 全局标注，可以作用于不同文件。大写标注也称为「文件标注」。跳转时有可能会切换到另一个缓冲区 0-9 viminfo 0 代表 viminfo 最后一次被写入的位置。实际使用中，就代表 Vim 进程最后一次结束的位置。1 代表 Vim 进程倒数第二次结束的位置，以此类推 如果想跳转到指定的标注，你可以先按下 ' / g' 或者 ` / g` 然后按下标注名。\n如果你想定义当前文件中的标注，可以先按下 m 再按下标注名。比如，按下 mm 就可以把当前位置标注为 m。在这之后，如果你的光标切换到了文件的其他位置，只需要通过 'm 或者 `m即可回到刚才标注的行。区别在于，'m会跳转回被标记行的第一个非空字符，而`m会跳转回被标记行的被标记列。根据 viminfo 的设置，你可以在退出 Vim 的时候保留小写字符标注。请参阅:h viminfo-' 来获取更多帮助。\n如果你想定义全局的标注，可以先按下 m 再按下大写英文字符。比如，按下 mM 就可以把当前文件的当前位置标注为 M。在这之后，就算你切换到其他的缓冲区，依然可以通过 'M 或 `M 跳转回来。\n关于跳转，还有以下的方式：\n按键 跳转至 '[ 与 `[ 上一次修改或复制的第一行或第一个字符 '] 与 `] 上一次修改或复制的最后一行或最后一个字符 '\u0026lt; 与 `\u0026lt; 上一次在可视模式下选取的第一行或第一个字符 '\u0026gt; 与 `\u0026gt; 上一次在可视模式下选取的最后一行或最后一个字符 '' 与 `' 上一次跳转之前的光标位置 '\u0026quot; 与 `\u0026quot; 上一次关闭当前缓冲区时的光标位置 '^ 与 `^ 上一次插入字符后的光标位置 '. 与 `. 上一次修改文本后的光标位置 '( 与 `( 当前句子的开头 ') 与 `) 当前句子的结尾 '{ 与 `{ 当前段落的开头 '} 与 `} 当前段落的结尾 标注也可以搭配 范围 一起使用。前面提到过，如果你在可视模式下选取一些文本，然后按下 :，这时候你会发现命令行已经被填充了 :'\u0026lt;,'\u0026gt;。对照上面的表格，现在你应该明白了，这段代表的就是可视模式下选取的范围。\n请使用 :marks 命令来显示所有的标注，参阅 :h mark-motions 来获取关于标注的更多帮助。\n返回主目录 ⤴️\n补全 Vim 在插入模式中为我们提供了多种补全方案。如果有多个补全结果，Vim 会弹出一个菜单供你选择。\n常见的补全有标签、项目中引入的模块或库中的方法名、文件名、字典及当前缓冲区的字段。\n针对不同的补全方案，Vim 为我们提供了不同的按键映射。这些映射都是在插入模式中通过 Ctrl + x 来触发：\n映射 类型 帮助文档 \u0026lt;c-x\u0026gt;\u0026lt;c-l\u0026gt; 整行 :h i^x^l \u0026lt;c-x\u0026gt;\u0026lt;c-n\u0026gt; 当前缓冲区中的关键字 :h i^x^n \u0026lt;c-x\u0026gt;\u0026lt;c-k\u0026gt; 字典（请参阅 :h 'dictionary'）中的关键字 :h i^x^k \u0026lt;c-x\u0026gt;\u0026lt;c-t\u0026gt; 同义词字典（请参阅 :h 'thesaurus'）中的关键字 :h i^x^t \u0026lt;c-x\u0026gt;\u0026lt;c-i\u0026gt; 当前文件以及包含的文件中的关键字 :h i^x^i \u0026lt;c-x\u0026gt;\u0026lt;c-]\u0026gt; 标签 :h i^x^] \u0026lt;c-x\u0026gt;\u0026lt;c-f\u0026gt; 文件名 :h i^x^f \u0026lt;c-x\u0026gt;\u0026lt;c-d\u0026gt; 定义或宏定义 :h i^x^d \u0026lt;c-x\u0026gt;\u0026lt;c-v\u0026gt; Vim 命令 :h i^x^v \u0026lt;c-x\u0026gt;\u0026lt;c-u\u0026gt; 用户自定义补全（通过 'completefunc' 定义） :h i^x^u \u0026lt;c-x\u0026gt;\u0026lt;c-o\u0026gt; Omni Completion（通过 'omnifunc' 定义） :h i^x^o \u0026lt;c-x\u0026gt;s 拼写建议 :h i^Xs 尽管用户自定义补全与 Omni Completion 是不同的，但他们做的事情基本一致。共同点在于，他们都是一个监听当前光标位置的函数，返回值为一系列的补全建议。用户自定义补全是由用户定义的，基于用户的个人用途，因此你可以根据自己的喜好和需求随意定制。而 Omni Completion 是针对文件类型的补全，比如在 C 语言中补全一个结构体（struct）的成员（members），或者补全一个类的方法，因而它通常都是由文件类型插件设置和调用的。\n如果你设置了 'complete' 选项，那么你就可以在一次操作中采用多种补全方案。这个选项默认包含了多种可能性，因此请按照自己的需求来配置。你可以通过 \u0026lt;c-n\u0026gt; 来调用下一个补全建议，或通过 \u0026lt;c-p\u0026gt; 来调用上一个补全建议。当然，这两个映射同样可以直接调用补全函数。请参阅 :h i^n 与 :h 'complete' 来获得更多帮助。\n如果你想配置弹出菜单的行为，请一定要看一看 :h 'completeopt' 这篇帮助文档。默认的配置已经不错了，但我个人（原作者）更倾向于把 \u0026ldquo;noselect\u0026rdquo; 加上。\n请参阅以下文档获取更多帮助：\n:h ins-completion :h popupmenu-keys :h new-omni-completion 返回主目录 ⤴️\n动作，操作符，文本对象 动作也就是指移动光标的操作，你肯定很熟悉 h、j、k 和 l，以及 w 和 b。但其实，/ 也是一个动作。他们都可以搭配数字使用，比如 2?the\u0026lt;cr\u0026gt; 可以将光标移动到倒数第二个 \u0026ldquo;the\u0026rdquo; 出现的位置。\n以下会列出一些常用的动作。你也可以通过 :h navigation 来获取更多的帮助。\n操作符是对某个区域文本执行的操作。比如，d、~、gU 和 \u0026gt; 都是操作符。这些操作符既可以在普通模式下使用，也可以在可视模式下使用。在普通模式中，顺序是先按操作符，再按动作指令，比如 \u0026gt;j。在可视模式中，选中区域后直接按操作符就可以，比如 Vjd。\n与动作一样，操作符也可以搭配数字使用，比如 2gUw 可以将当前单词以及下一个单词转成大写。由于动作和操作符都可以搭配数字使用，因此 2gU2w 与执行两次 gU2w 效果是相同的。\n请参阅 :h operator 来查看所有的操作符。你也可以通过 :set tildeop 命令把 ~ 也变成一个操作符\n值得注意的是，动作是单向的，而文本对象是双向的。文本对象不仅作用于符号（比如括号、中括号和大括号等）标记的范围内，也作用于整个单词、整个句子等其他情况。\n文本对象不能用于普通模式中移动光标的操作，因为光标还没有智能到可以向两个方向同时跳转。但这个功能可以在可视模式中实现，因为在对象的一端选中的情况下，光标只需要跳转到另一端就可以了。\n文本对象操作一般用 i 或 a 加上对象标识符操作，其中 i 表示在对象内（英文 inner）操作，a 表示对整个对象（英文 around）操作，这时开头和结尾的空格都会被考虑进来。举个例子，diw 可以删除当前单词，ci( 可以改变括号中的内容。\n文本对象同样可以与数字搭配使用。比如，像 ((( ))) 这样的文本，假如光标位于最内层的括号上或最内层的括号内，那么 d2a( 将会删除从最内层开始的两对括号，以及他们之间的所有内容。其实，d2a( 这个操作等同于 2da(。在 Vim 的命令中，如果有两处都可以接收数字作为参数，那么最终结果就等同于两个数字相乘。在这里，d 与 a( 都是可以接收参数的，一个参数是 1，另一个是 2，我们可以把它们相乘然后放到最前面。\n请参阅 :h text-objects 来获取更多关于文本对象的帮助。\n返回主目录 ⤴️\n自动命令 在特定的情况下，Vim 会传出事件。如果你想针对这些事件执行回调方法，那么就需要用到自动命令这个功能。\n如果没有了自动命令，那你基本上是用不了 Vim 的。自动命令一直都在执行，只是很多时候你没有注意到。不信的话，可以执行命令 :au ，不要被结果吓到，这些是当前有效的所有自动命令。\n请使用 :h {event} 来查看 Vim 中所有事件的列表，你也可以参考 :h autocmd-events-abc 来获取关于事件的更多帮助。\n一个很常用的例子，就是针对文件类型执行某些设置：\nautocmd FileType ruby setlocal shiftwidth=2 softtabstop=2 comments-=:# 但是缓冲区是如何知道当前的文件中包含 Ruby 代码呢？这其实是另一个自动命令检测的到的，然后把文件类型设置成为 Ruby，这样就触发了上面的 FileType 事件。\n在配置 vimrc 的时候，一般第一行加进去的就是 filetype on。这就意味着，Vim 启动时会读取 filetype.vim 文件，然后根据文件类型来触发相应的自动命令。\n如果你勇于尝试，可以查看下 :e $VIMRUNTIME/filetype.vim，然后在输出中搜索 \u0026ldquo;Ruby\u0026rdquo;。这样，你就会发现其实 Vim 只是通过文件扩展名 .rb 判断某个文件是不是 Ruby 的。\n注意：对于相同事件，如果有多个自动命令，那么自动命令会按照定义时的顺序执行。通过 :au 就可以查看它们的执行顺序。\nau BufNewFile,BufRead *.rb,*.rbw setf ruby BufNewFile 与 BufRead 事件是被写在 Vim 源文件中的。因此，每当你通过 :e 或者类似的命令打开文件，这两个事件都会触发。然后，就是读取 filetype.vim 文件来判断打开的文件类型。\n简单来说，事件和自动命令在 Vim 中的应用十分广泛。而且，Vim 为我们留出了一些易用的接口，方便用户配置适合自己的事件驱动回调。\n请参阅 :h autocommand 来获取更多帮助\n返回主目录 ⤴️\n变更历史，跳转历史 在 Vim 中，用户最近 100 次的文字改动都会被保存在变更历史中。如果在同一行有多个小改动，那么 Vim 会把它们合并成一个。尽管内容改动会合并，但作用的位置还是会只记录下最后一次改动的位置。\n在你移动光标或跳转的时候，每一次的移动或跳转前的位置会被记录到跳转历史中。类似地，跳转历史也可以最多保存 100 条记录。对于每个窗口，跳转记录是独立的。但当你分离窗口时（比如使用 :split 命令），跳转历史会被复制过去。\nVim 中的跳转命令，包括 '、`、G、/、?、n、N、%、(、)、[[、]]、{、}、:s、:tag、L、M、H 以及开始编辑一个新文件的命令。\n列表 显示所有条目 跳转到上一个位置 跳转到下一个位置 跳转历史 :jumps [count]\u0026lt;c-o\u0026gt; [count]\u0026lt;c-i\u0026gt; 变更历史 :changes [count]g; [count]g, 如果你执行第二列的命令显示所有条目，这时 Vim 会用 \u0026gt; 标记来为你指示当前位置。通常这个标记位于 1 的下方，也就代表最后一次的位置。\n如果你希望关闭 Vim 之后还保留这些条目，请参阅 :h viminfo-' 来获取更多帮助。\n注意：上面提到过，最后一次跳转前的位置也会记录在标注中，也可以通过连按 `` 或 \u0026rsquo;\u0026rsquo; 跳转到那个位置\n请参阅以下两个命令来获取更多帮助：\n:h changelist :h jumplist 返回主目录 ⤴️\n内容变更历史记录 Vim 会记录文本改变之前的状态。因此，你可以使用「撤销」操作 u 来取消更改，也可以通过「重做」操作 Ctrl + r 来恢复更改。\n值得注意的是，Vim 采用 tree 数据结构来存储内容变更的历史记录，而不是采用 queue。你的每次改动都会成为存储为树的节点。而且，除了第一次改动（根节点），之后的每次改动都可以找到一个对应的父节点。每一个节点都会记录改动的内容和时间。其中，「分支」代表从任一节点到根节点的路径。当你进行了撤销操作，然后又输入了新的内容，这时候就相当于创建了分支。这个原理和 git 中的 branch（分支）十分类似。\n考虑以下这一系列按键操作：\nifoo\u0026lt;esc\u0026gt; obar\u0026lt;esc\u0026gt; obaz\u0026lt;esc\u0026gt; u oquux\u0026lt;exc\u0026gt; 那么现在，Vim 中会显示三行文本，分别是 \u0026ldquo;foo\u0026rdquo;、\u0026ldquo;bar\u0026rdquo; 和 \u0026ldquo;quux\u0026rdquo;。这时候，存储的树形结构如下：\nfoo(1) / bar(2) / \\ baz(3) quux(4) 这个树形结构共包含四次改动，括号中的数字就代表时间顺序。\n现在，我们有两种方式遍历这个树结构。一种叫「按分支遍历」，一种叫「按时间遍历」。\n撤销 u 与重做 Ctrl + r 操作是按分支遍历。对于上面的例子，现在我们有三行字符。这时候按 u 会回退到 \u0026ldquo;bar\u0026rdquo; 节点，如果再按一次 u 则会回退到 \u0026ldquo;foo\u0026rdquo; 节点。这时，如果我们按下 Ctrl + r 就会前进至 \u0026ldquo;bar\u0026rdquo; 节点，再按一次就回前进至 \u0026ldquo;quux\u0026rdquo; 节点。在这种方式下，我们无法访问到兄弟节点（即 \u0026ldquo;baz\u0026rdquo; 节点）。\n与之对应的是按时间遍历，对应的按键是 g- 和 g+。对于上面的例子，按下 g- 会首先回退到 \u0026ldquo;baz\u0026rdquo; 节点。再次按下 g- 会回退到 \u0026ldquo;bar\u0026rdquo; 节点。\n命令/按键 执行效果 [count]u 或 :undo [count] 回退到 [count] 次改动之前 [count]\u0026lt;c-r\u0026gt; 或 :redo [count] 重做 [count] 次改动 U 回退至最新的改动 [count]g- 或 :earlier [count]? 根据时间回退到 [count] 次改动之前。\u0026quot;?\u0026quot; 为 \u0026ldquo;s\u0026rdquo;、\u0026ldquo;m\u0026rdquo;、\u0026ldquo;h\u0026rdquo;、\u0026ldquo;d\u0026rdquo; 或 \u0026ldquo;f\u0026quot;之一。例如，:earlier 2d 会回退到两天之前。:earlier 1f 则会回退到最近一次文件保存时的内容 [count]g+ 或 :later [count]? 类似 g-，但方向相反 内容变更记录会储存在内存中，当 Vim 退出时就会清空。如果需要持久化存储内容变更记录，请参阅备份文件，交换文件，撤销文件以及 viminfo 文件的处理章节的内容。\n如果你觉得这一部分的内容难以理解，请参阅 undotree，这是一个可视化管理内容变更历史记录的插件。类似的还有 vim-mundo。\n请参阅以下链接获取更多帮助：\n:h undo.txt :h usr_32 返回主目录 ⤴️\n全局位置信息表，局部位置信息表 在某一个动作返回一系列「位置」的时候，我们可以利用「全局位置信息表」和「局部位置信息表」来存储这些位置信息，方便以后跳转回对应的位置。每一个存储的位置包括文件名、行号和列号。\n比如，编译代码是出现错误，这时候我们就可以把错误的位置直接显示在全局位置信息表，或者通过外部抓取工具使位置显示在局部位置信息表中。\n尽管我们也可以把这些信息显示到一个空格缓冲区中，但用这两个信息表显示的好处在于接口调用很方便，而且也便于浏览输出。\nVim 中，全局位置信息表只能有一个，但每一个窗口都可以有自己的局部位置信息表。这两个信息表的外观看上去很类似，但在操作上会稍有不同。\n以下为两者的操作比较：\n动作 全局位置信息表 局部位置信息表 打开窗口 :copen :lopen 关闭窗口 :cclose :lclose 下一个条目 :cnext :lnext 上一个条目 :cprevious :lprevious 第一个条目 :cfirst :lfirst 最后一个条目 :clast :llast 请参阅 :h :cc 以及底下的内容，来获取更多命令的帮助。\n应用实例： 如果我们想用 grep 递归地在当前文件夹中寻找某个关键词，然后把输出结果放到全局位置信息表中，只需要这样：\n:let \u0026amp;grepprg = \u0026#39;grep -Rn $* .\u0026#39; :grep! foo \u0026lt;grep output - hit enter\u0026gt; :copen 执行了上面的代码，你就能看到所有包含字符串 \u0026ldquo;foo\u0026rdquo; 的文件名以及匹配到的相关字段都会显示在全局位置信息表中。\n返回主目录 ⤴️\n宏 你可以在 Vim 中录制一系列按键，并把他们存储到寄存器中。对于一些需要临时使用多次的一系列操作，把它们作为宏保存起来会显著地提升效率。对于一些复杂的操作，建议使用 Vim 脚本来实现。\n首先，按下 q，然后按下你想要保存的寄存器，任何小写字母都可以。比如我们来把它保存到 q 这个寄存器中。按下 qq，你会发现命令行里已经显示了 \u0026ldquo;recording @q\u0026rdquo;。 如果你已经录制完成，那么只需要再按一次 q 就可以结束录制。 如果你想调用刚才录制的宏，只需要 [count]@q 如果你想调用上一次使用的宏，只需要 [count]@@ 实例 1：\n一个插入字符串 \u0026ldquo;abc\u0026rdquo; 后换行的宏，重复调用十次：\nqq iabc\u0026lt;cr\u0026gt;\u0026lt;esc\u0026gt; q 10@q （对于上面这个功能，你同样可以通过如下的按键： oabc 然后 ESC 然后 10. 来实现）。\n实例 2：\n一个在每行前都加上行号的宏。从第一行开始，行号为 1，后面依次递增。我们可以通过 Ctrl + a 来实现递增的行号，在定义宏的时候，它会显示成 ^A。\nqq 0yf jP0^A q 1000 @q 这里能实现功能，是因为我们假定了文件最多只有 1000 行。但更好的方式是使用「递归」宏，它会一直执行，知道不能执行为止：\nqq 0yf jP0^A@q q @q （对于上面这个插入行号的功能，如果你不愿意使用宏，同样可以通过这段按键操作来实现：:%s/^/\\=line('.') . '. '）。\n这里向大家展示了如何不用宏来达到相应的效果，但要注意，这些不用宏的实现方式只适用于这些简单的示例。对于一些比较复杂的自动化操作，你确实应该考虑使用宏。\n请参阅以下文档获取更多帮助：\n:h recording :h \u0026#39;lazyredraw\u0026#39; 返回主目录 ⤴️\n颜色主题 颜色主题可以把你的 Vim 变得更漂亮。Vim 是由多个组件构成的，我们可以给每一个组件都设置不同的文字颜色、背景颜色以及文字加粗等等。比如，我们可以通过这个命令来设置背景颜色：\n:highlight Normal ctermbg=1 guibg=red 执行后你会发现，现在背景颜色变成红色了。请参阅 :h :highlight 来获取更多帮助。\n其实，颜色主题就是一系列的 :highlight 命令的集合。\n事实上，大部分颜色主题都包含两套配置。一套适用于例如 xterm 和 iTerm 这样的终端环境（使用前缀 cterm），另一套适用于例如 gvim 和 MacVim 的图形界面环境（使用前缀 gui）。对于上面的例子，ctermbg 就是针对终端环境的，而 guibg 就是针对图形界面环境的。\n如果你下载了一个颜色主题，并且在终端环境中打开了 Vim，然后发现显示的颜色与主题截图中差别很大，那很可能是配置文件只设置了图形界面环境的颜色。反之同理，如果你使用的是图形界面环境，发现显示颜色有问题，那就很可能是配置文件只设置了终端环境的颜色。\n第二种情况（图形界面环境的显示问题）其实不难解决。如果你使用的是 Neovim 或者 Vim 7.4.1830 的后续版本，可以通过打开真彩色设置来解决显示问题。这就可以让终端环境的 Vim 使用 GUI 的颜色定义，但首先，你要确认一下你的终端环境和环境内的组件（比如 tmux）是否都支持真彩色。可以看一下这篇文档，描述的十分详细。\n请参阅以下文档或链接来获取更多帮助：\n:h 'termguicolors' 主题列表 自定义主题中的颜色 返回主目录 ⤴️\n折叠 每一部分文字（或者代码）都会有特定的结构。对于存在结构的文字和代码，也就意味着它们可以按照一定的逻辑分割成不同区域。Vim 中的折叠功能，就是按照特定的逻辑把文字和代码折叠成一行，并显示一些简短的描述。折叠功能涉及到很多操作，而且折叠功能可以嵌套使用。\n在 Vim 中，有以下 6 中折叠类型：\n折叠方式 概述 diff 在「比较窗口」中折叠未改变的文本 expr 使用 'foldexpr' 来创建新的折叠逻辑 indent 基于缩进折叠 manual 使用 zf、zF 或 :fold 来自定义折叠 marker 根据特定的文本标记折叠（通常用于代码注释） syntax 根据语法折叠，比如折叠 if 代码块 注意：折叠功能可能会显著地影响性能。如果你在使用折叠功能的时候出现了打字卡顿之类的问题，请考虑使用 FastFold 插件。这个插件可以让 Vim 按需更新折叠内容，而不是一直调用。\n请参阅以下文档获取更多帮助：\n:h usr_28 :h folds 会话 如果你保存了当前的「视图」（请参阅 :h :mkview），那么当前窗口、配置和按键映射都会被保存下来（请参阅 :h :loadview）。\n「会话」就是存储所有窗口的相关设置，以及全局设置。简单来说，就是给当前的 Vim 运行实例拍个照，然后把相关信息存储到会话文件中。存储之后的改动就不会在会话文件中显示，你只需要在改动后更新一下会话文件就可以了。\n你可以把当前工作的「项目」存储起来，然后可以在不同的「项目」之间切换。\n现在就来试试吧。打开几个窗口和标签，然后执行 :mksession Foo.vim。如果你没有指定文件名，那就会默认保存为 Session.vim。这个文件会保存在当前的目录下，你可以通过 :pwd 来显示当前路径。重启 Vim 之后，你只需要执行 :source Foo.vim，就可以恢复刚才的会话了。所有的缓冲区、窗口布局、按键映射以及工作路径都会恢复到保存时的状态。\n其实 Vim 的会话文件就只是 Vim 命令的集合。你可以通过命令 :vs Foo.vim 来看看会话文件中究竟有什么。\n你可以决定 Vim 会话中究竟要保存哪些配置，只需要设置一下 'sessionoptions' 就可以了。\n为了方便开发，Vim 把最后一次调用或写入的会话赋值给了一个内部变量 v:this_session。\n请参阅以下文档来获取更多帮助：\n:h Session :h \u0026#39;sessionoptions\u0026#39; :h v:this_session 局部化 以上提到的很多概念，都有一个局部化（非全局）的版本：\n全局 局部 作用域 帮助文档 :set :setlocal 缓冲区或窗口 :h local-options :map :map \u0026lt;buffer\u0026gt; 缓冲区 :h :map-local :autocmd :autocmd * \u0026lt;buffer\u0026gt; 缓冲区 :h autocmd-buflocal :cd :lcd 窗口 :h :lcd :\u0026lt;leader\u0026gt; :\u0026lt;localleader\u0026gt; 缓冲区 :h maploacalleader 变量也有不同的作用域，详细内容请参考 Vim scripting 的文档。\n用法 获取离线帮助 Vim 自带了一套很完善的帮助文档，它们是一个个有固定排版格式的文本文件，通过标签可以访问这些文件的特定位置。\n在开始之前先读一下这个章节：:help :help。执行这个命令以后会在新窗口打开 $VIMRUNTIME/doc/helphelp.txt 文件并跳转到这个文件中 :help 标签的位置。\n一些关于帮助主题的简单规则：\n用单引号把文本包起来表示选项，如：:h 'textwidth' 以小括号结尾表示 VimL 函数，如：:h reverse() 以英文冒号开头表示命令，如：:h :echo 使用快捷键 \u0026lt;c-d\u0026gt; （这是 ctrl+d）来列出所有包含你当前输入的内容的帮助主题。如：:h tab\u0026lt;c-d\u0026gt; 会列出所有包含 tab 主题，从 softtabstop 到 setting-guitablabel （译者注：根据安装的插件不同列出的选项也会不同）。\n你想查看所有的 VimL 方法吗？很简单，只要输入：:h ()\u0026lt;c-d\u0026gt; 就可以了。你想查看所有与窗口相关的函数吗？输入 :h win*()\u0026lt;c-d\u0026gt;。\n相信你很快就能掌握这些技巧，但是在刚开始的时候，你可能对于该通过什么进行查找一点线索都没有。这时你可以想象一些与要查找的内容相关的关键字，再让 :helpgrep 来帮忙。\n:helpgrep backwards 上面的命令会在所有的帮助文件中搜索“backwards”，然后跳转到第一个匹配的位置。所有的匹配位置都会被添加到全局位置信息表，用 :cp / :cn 可以在匹配位置之间进行切换。或者用 :copen 命令来打开全局位置信息表，将光标定位到你想要的位置，再按 回车就可以跳转到该匹配项。详细说明请参考 :h quickfix。\n获取离线帮助（补充） 这个列表最初发表在 vim_dev，由 @chrisbra 编辑的，他是 Vim 开发人员中最活跃的一个。\n经过一些微小的改动后，重新发布到了这里。\n如果你知道你想要找什么，使用帮助系统的搜索会更简单一些，因为搜索出的主题都带有固定的格式。\n而且帮助系统中的主题包含了你当前使用的 Vim 版本的所特有特性，而网上那些已经过时或者是早期发布的话题是不会包含这些的。\n因此学习使用帮助系统以及它所用的语言是很有必要的。这里是一些例子（不一定全，我有可能忘了一些什么）。\n（译者注：下面列表中提及的都是如何指定搜索主题以便快速准确的找到你想要的帮助）\n选项要用单引号引起来。用 :h 'list' 来查看列表选项帮助。只有你明确的知道你要找这么一个选项的时候才可以这么做，不然的话你可以用 :h options.txt 来打开所有选项的帮助页面，再用正则表达式进行搜索，如：/width。某些选项有它们自己的命名空间，如：:h cpo-a，:h cpo-A， :h cpo-b 等等。\n普通模式的命令不能用冒号作为前缀。使用 :h gt 来转到“gt”命令的帮助页面。\n正则表达式以“/”开头，所以 :h /\\+ 会带你到正则表达式中量词“+”的帮助页面。\n组合键经常以一个字母开头表示它们可以在哪些模式中使用。如：:h i_CTRL-X 会带你到插入模式下的 CTRL-X 命令的用法帮助页面，这是一个自动完成类的组合键。需要注意的是某些键是有固定写法的，如 Control 键写成 CTRL。还有，查找普通模式下的组合键帮助时，可以省略开头的字母“n”，如：:h CTRL-A。而 :h c_CTRL-A（译者注：原文为 :h c_CRTL-R，感觉改为 A 更符合上下文语境）会解释 CTRL-A 在命令模式下输入命令时的作用；:h v_CTRL-A 说的是在可见模式下把光标所在处的数字加 1；:h g_CTRL-A 则说的是 g 命令（你需要先按 \u0026ldquo;g\u0026rdquo; 的命令）。这里的 \u0026ldquo;g\u0026rdquo; 代表一个普通的命令，这个命令总是与其它的按键组合使用才生效，与 \u0026ldquo;z\u0026rdquo; 开始的命令相似。\n寄存器是以 \u0026ldquo;quote\u0026rdquo; 开头的。如：:h quote: （译者注：原文为:h quote，感觉作者想以\u0026rdquo;:\u0026ldquo;来举例）来查看关于\u0026rdquo;:\u0026ldquo;寄存器的说明。\n关于 Vim 脚本（VimL）的帮助都在 :h eval.txt 里。而某些方面的语言可以使用 :h expr-X 获取帮助，其中的 \u0026lsquo;X\u0026rsquo; 是一个特定的字符，如：:h expr-! 会跳转到描述 VimL 中\u0026rsquo;!\u0026rsquo;（非）的章节。另外一个重要提示，可以使用 :h function-list 来查看所有函数的简要描述，列表中包括函数名和一句话描述。\n关于映射都可以在 :h map.txt 中找到。通过 :h mapmode-i 来查找 :imap 命令的相关信息；通过 :h map-topic 来查找专门针对映射的帮助（译者注：topic 为一个占位符，正如上面的字符 \u0026lsquo;X\u0026rsquo; 一样，在实际使用中需要替换成相应的单词）（如：:h :map-local 查询本地 buffer 的映射，:h map-bar 查询如何在映射中处理\u0026rsquo;|\u0026rsquo;)。\n命令定义用 \u0026ldquo;command-\u0026rdquo; 开头，如用 :h command-bar 来查看自定义命令中\u0026rsquo;!\u0026lsquo;的作用。\n窗口管理类的命令是以 \u0026ldquo;CTRL-W\u0026rdquo; 开头的，所以你可以用 :h CTRL-W_* 来查找相应的帮助（译者注：\u0026rsquo;*\u0026lsquo;同样为占位符）（如：:h CTRL-W_p 查看切换到之前访问的窗口命令的解释）。如果你想找窗口处理的命令，还可以通过访问 :h windows.txt 并逐行向下浏览，所有窗口管理的命令都在这里了。\n执行类的命令以\u0026rdquo;:\u0026ldquo;开头，即：:h :s 讲的是 \u0026ldquo;:s\u0026rdquo; 命令。\n在输入某个话题时按 CTRL-D，让 Vim 列出所有的近似项辅助你输入。\n用 :helpgrep 在所有的帮助页面（通常还包括了已安装的插件的帮助页面）中进行搜索。参考 :h :helpgrep 来了解如何使用。当你搜索了一个话题之后，所有的匹配结果都被保存到了全局位置信息表（或局部位置信息表）当中，可以通过 :copen 或 :lopen 打开。在打开的窗口中可能通过 / 对搜索结果进行进一步的过滤。\n:h helphelp 里介绍了如何使用帮助系统。\n用户手册。它采用了一种对初学者更加友好的方式来展示帮助话题。用 :h usr_toc.txt 打开目录（你可能已经猜到这个命令的用处了）。浏览用户手册能帮助你找出某些你想了解的话题，如你可以在第 24 章看到关于“复合字符”以及“输入特殊字符”的讲解（用 :h usr_24.txt 可以快速打开相关章节）。\n高亮分组的帮助以 hl- 开头。如：:h hl-WarningMsg 说的是警告信息分组的高亮。\n语法高亮以:syc- 开头，如：:h :syn-conceal 讲的是 :syn 命令的对于隐藏字符是如何显示的。\n快速修复命令以 :c 开头，而位置列表命令以 :l 开头。\n:h BufWinLeave 讲的是 BufWinLeave 自动命令。还有，:h autocommand-events （译者注：原文是 :h autocommands-events，但是没有该帮助）讲的是所有可用的事件。\n启动参数都以“-”开头，如：:h -f 会告诉你 Vim 中 “-f” 参数的作用。\n额外的特性都以“+”开头，如：:h +conceal 讲的是关于隐藏字符的支持。\n错误代码可以在帮助系统中直接查到。:h E297 会带你到关于这一错误的详细解释。但是有时并没有转到错误描述，而是列出了经常导出这一错误的 Vim 命令，如 :h E128 （译者注：原文为:h hE128，但是并没有该帮助）会直接跳转到 :function 命令。\n关于包含的语法文件的文档的帮助话题格式是 :h ft-*-syntax。如：:h ft-c-syntax 说的就是 C 语言语法文件以及它所提供的选项。有的语法文件还会带有自动完成（:h ft-php-omni）或文件类型插件（:h ft-tex-plugin）相关的章节可以查看。\n另外在每个帮助页的顶端通常会包含一个用户文档链接（更多的从从用户的角度出发来主角命令的功能和用法，不涉及那么多细节）。如：:h pattern.txt 里包含了 :h 03.9 和 :h usr_27 两个章节的链接。\n获取在线帮助 如果你遇到了无法解决的问题，或者需要指引的话，可以参考 Vim 使用邮件列表。 IRC 也是一个很不错的资源。 Freenode 上的 #vim 频道很庞大，并且里面有许多乐于助人的人。\n如果你想给 Vim 提交 Bug 的话，可以使用 vim_dev 邮件列表。\n执行自动命令 你可以触发任何事件，如：:doautocmd BufRead。\n用户自定义事件 对于插件而言，创建你自己的自定义事件有时非常有用。\nfunction! Chibby() \u0026#34; A lot of stuff is happening here. \u0026#34; And at last.. doautocmd User ChibbyExit endfunction 现在你插件的用户可以在 Chibby 执行完成之后做任何他想做的事情：\nautocmd User ChibbyExit call ChibbyCleanup() 顺便提一句，如果在使用 :autocmd 或 :doautocmd 时没有捕捉异常，那么会输出 \u0026ldquo;No matching autocommands\u0026rdquo; 信息。这也是为什么许多插件用 silent doautocmd ... 的原因。但是这也会有不足，那就是你不能再在 :autocmd 中使用 echo \u0026quot;foo\u0026quot; 了，取而代之的是你要使用 unsilent echo \u0026quot;foo\u0026quot; 来输出。\n这就是为什么要在触发事件之前先判断事件是否存在的原因，\nif exists(\u0026#39;#User#ChibbyExit\u0026#39;) doautocmd User ChibbyExit endif 帮助文档：:h User\n事件嵌套 默认情况下，自动命令不能嵌套！如果某个自动命令执行了一个命令，这个命令再依次触发其它的事件，这是不可能的。\n例如你想在每次启动 Vim 的时候自动打开你的 vimrc 文件：\nautocmd VimEnter * edit $MYVIMRC 当你启动 Vim 的时候，它会帮你打开你的 vimrc 文件，但是你很快会注意到这个文件没有任何的高亮，尽管平时它是正常可以高亮的。\n问题在于你的非嵌套自动命令 :edit 不会触发“BufRead”事件，所以并不会把文件类型设置成“vim”，进而 $VIMRUNTIME/syntax/vim.vim 永远不会被引入。详细信息请参考：:au BufRead *.vim。要想完成上面所说的需求，使用下面这个命令：\nautocmd VimEnter * nested edit $MYVIMRC 帮助文档：:h autocmd-nested\n剪切板 如果你想在没有 GUI 支持的 Unix 系统中使用 Vim 的 'clipboard' 选项，则需要 +clipboard 以及可选的 +xterm_clipboard 两个特性支持。\n帮助文档：\n:h \u0026#39;clipboard\u0026#39; :h gui-clipboard :h gui-selections 另外请参考：持续粘贴（为什么我每次都要设置 \u0026lsquo;paste\u0026rsquo; 模式\n剪贴板的使用（Windows, OSX） Windows 自带了剪贴板，OSX 则带了一个粘贴板\n在这两个系统中都可以用大家习惯用的 ctrl+c / cmd+c 复制选择的文本，然后在另外一个应用中用 ctrl+v / cmd+v 进行粘贴。\n需要注意的是复制的文本已经被发送到了剪贴板，所以你在粘贴复制的内容之前关闭这个应用是没有任何问题的。\n每次复制的时候，都会向剪贴板寄存器 * 中写入数据。 而在 Vim 中分别使用 \u0026quot;*y 和 \u0026quot;*p 来进行复制（yank) 和 粘贴（paste)。\n如果你不想每次操作都要指定 * 寄存器，可以在你的 vimrc 中添加如下配置：\nset clipboard=unnamed 通常情况下复制/删除/放入操作会往 \u0026quot; 寄存器中写入数据，而加上了上面的配置之后 * 寄存器也会被写入同样数据，因此简单的使用 y 和 p 就可以复制粘贴了。\n我再说一遍：使用上面的选项意味着每一次的复制/粘贴，即使在同一个 Vim 窗口里，都会修改剪贴板的内容。你自己决定上面的选项是否适合。\n如果你觉得输入 y 还是太麻烦的话，可以使用下面的设置把在可视模式下选择的内容发送到剪贴板：\nset clipboard=unnamed,autoselect set guioptions+=a 帮助文档：\n:h clipboard-unnamed :h autoselect :h \u0026#39;go_a\u0026#39; 剪贴板的使用（Linux, BSD, \u0026hellip;） 如果你的系统使用了 X 图形界面，事情会变得有一点不同。X 图形界面实现了 X 窗口系统协议, 这个协议在 1987 年发布的主版本 11，因此 X 也通常被称为 X11。\n在 X10 版本中，剪贴缓冲区被用来实现像 clipboard 一样由 X 来复制文本，并且可以被所有的程序访问。现在这个机制在 X 中还存在，但是已经过时了，很多程序都不再使用这一机制。\n近年来数据在程序之间是通过选择进行传递的。一共有三种选择，经常用到的有两种：PRIMARY 和 CLIPBOARD。\n选择的工作工模大致是这样的：\nProgram A：\u0026lt;ctrl+c\u0026gt; Program A：声称对 CLIPBOARD 的所有权 Program B：\u0026lt;ctrl+v\u0026gt; Program B：发现CLIPBOARD的所有权被Program A持有 Program B：从Program A请求数据 Program A：响应这个请求并发送数据给Program B Program B：从Program A接收数据并插入到窗口中 选择 何时使用 如何粘贴 如何在 Vim 中访问 PRIMARY 选择文本 鼠标中键, shift+insert * 寄存器 CLIPBOARD 选择文本并按 ctrl+c ctrl+v +寄存器 注意：X 服务器并不会保存选择（不仅仅是 CLIPBOARD 选择）！因此在关闭了相应的程序后，你用 ctrl+c 复制的内容将丢失。\n使用 \u0026quot;*p 来贴粘 PRIMARY 选择中的内容，或者使用 \u0026quot;+y1G 来将整个文件的内容复制到 CLIPBOARD 选择。\n如果你需要经常访问这两个寄存器，可以考虑使用如下配置：\nset clipboard^=unnamed \u0026#34; * 寄存器 \u0026#34; 或者 set clipboard^=unnamedplus \u0026#34; + 寄存器 （^= 用来将设置的值加到默认值之前，详见：:h :set^=）\n这会使得所有复制/删除/放入操作使用 * 或 + 寄存器代替默认的未命令寄存器 \u0026quot;。之后你就可以直接使用 y 或 p 访问你的 X 选择了。\n帮助文档：\n:h clipboard-unnamed :h clipboard-unnamedplus 打开文件时恢复光标位置 如果没有这个设置，每次打开文件时光标都将定位在第一行。而加入了这个设置以后，你就可以恢复到上次关闭文件时光标所在的位置了。\n将下面的配置添加到你的 vimrc 文件：\nautocmd BufReadPost * \\ if line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026gt; 1 \u0026amp;\u0026amp; line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026lt;= line(\u0026#34;$\u0026#34;) | \\ exe \u0026#34;normal! g`\\\u0026#34;\u0026#34; | \\ endif 这是通过判断之前的光标位置是否存在（文件可能被其它程序修改而导致所记录的位置已经不存在了），如果存在的话就执行 g`\u0026quot; （转到你离开时的光标位置但是不更改跳转列表）。\n这需要使用 viminfo 文件：:h viminfo-。\n临时文件 根据选项的不同， Vim 最多会创建 4 种工作文件。\n备份文件 你可以让 Vim 在将修改写入到文件之前先备份原文件。默认情况下， Vim 会保存一个备份文件但是当修改成功写入后会立即删除它（:set writebackup）。如果你想一直保留这个备份文件的话，可以使用 :set backup。而如果你想禁用备份功能的话，可以使用 :set nobackup nowritebackup。\n咱们来看一下上次我在 vimrc 中改了什么：\n$ diff ~/.vim/vimrc ~/.vim/files/backup/vimrc-vimbackup 390d389 \u0026lt; command! -bar -nargs=* -complete=help H helpgrep \u0026lt;args\u0026gt; 帮助文档：:h backup\n交换文件 假设你有一个非常棒的科幻小说的构思。在按照故事情节已经写了好几个小时几十万字的时候..忽然停电了！而那时你才想起来你上次保存 ~/来自外太空的邪恶入侵者.txt 是在.. 好吧，你从来没有保存过。\n但是并非没有希望了！在编辑某个文件的时候， Vim 会创建一个交换文件，里面保存的是对当前文件所有未保存的修改。自己试一下，打开任意的文件，并使用 :swapname 获得当前的交换文件的保存路径。你也可以将 :set noswapfile 加入到 vimrc 中来禁用交换文件。\n默认情况下，交换文件会自动保存在被编辑文件所在的目录下，文件名以 .file.swp 后缀结尾，每当你修改了超过 200 个字符或是在之前 4 秒内没有任何动作时更新它的内容，在你不再编辑这个文件的时候会被删除。你可以自己修改这些数字，详见：:h 'updatecount' 和 :h 'updatetime'。\n而在断电时，交换文件并不会被删除。当你再次打开 vim ~/来自外太空的邪恶入侵者.txt 时， Vim 会提示你恢复这个文件。\n帮助文档：:h swap-file 和 :h usr_11\n撤销文件 内容变更历史记录是保存在内存中的，并且会在 Vim 退出时清空。如果你想让它持久化到磁盘中，可以设置 :set undofile。这会把文件 ~/foo.c 的撤销文件保存在 ~/foo.c.un~。\n帮助文档：:h 'undofile' 和 :h undo-persistence\nviminfo 文件 备份文件、交换文件和撤销文件都是与文本状态相关的，而 viminfo 文件是用来保存在 Vim 退出时可能会丢失的其它的信息的。包括历史记录（命令历史、搜索历史、输入历史）、寄存器内容、标注、缓冲区列表、全局变量等等。\n默认情况下，viminfo 被保存在 ~/.viminfo。\n帮助文档：:h viminfo 和 :h 'viminfo'\n临时文件管理设置示例 如果你跟我一样，也喜欢把这些文件放到一个位置（如：~/.vim/files）的话，可以使用下面的配置：\n\u0026#34; 如果文件夹不存在，则新建文件夹 if !isdirectory($HOME.\u0026#39;/.vim/files\u0026#39;) \u0026amp;\u0026amp; exists(\u0026#39;*mkdir\u0026#39;) call mkdir($HOME.\u0026#39;/.vim/files\u0026#39;) endif \u0026#34; 备份文件 set backup set backupdir =$HOME/.vim/files/backup/ set backupext =-vimbackup set backupskip = \u0026#34; 交换文件 set directory =$HOME/.vim/files/swap// set updatecount =100 \u0026#34; 撤销文件 set undofile set undodir =$HOME/.vim/files/undo/ \u0026#34; viminfo 文件 set viminfo =\u0026#39;100,n$HOME/.vim/files/info/viminfo 注意：如果你在一个多用户系统中编辑某个文件时， Vim 提示你交换文件已经存在的话，可能是因为有其他的用户此时正在编辑这个文件。而如果将交换文件放到自己的 home 目录的话，这个功能就失效了。因此服务器非常不建议将这些文件修改到 HOME 目录，避免多人同时编辑一个文件，却没有任何警告。\n编辑远程文件 Vim 自带的 netrw 插件支持对远程文件的编辑。实际上它将远程的文件通过 scp 复制到本地的临时文件中，再用那个文件打开一个缓冲区，然后在保存时把文件再复制回远程位置。\n下面的命令在你本地的 VIM 配置与 SSH 远程服务器上管理员想让你使用的配置有冲突时尤其有用：\n:e scp://bram@awesome.site.com/.vimrc 如果你已经设置了 ~/.ssh/config，SSH 会自动读取这里的配置：\nHost awesome HostName awesome.site.com Port 1234 User bram 如果你的 ~/.ssh/config 中有以上的内容，那么下面的命令就可以正常执行了：\n:e scp://awesome/.vimrc 可以用同样的方法编辑 ~/.netrc, 详见：:h netrc-netrc。\n确保你已经看过了 :h netrw-ssh-hack 和 :h g:netrw_ssh_cmd。\n另外一种编辑远程文件的方法是使用 sshfs，它会用 FUSE 来挂载远程的文件系统到你本地的系统当中。\n插件管理 Pathogen是第一个比较流行的插件管理工具。实际上它只是修改了 runtimepath （:h 'rtp'） 来引入所有放到该目录下的文件。你需要自己克隆插件的代码仓库到那个目录。\n真正的插件管理工具会在 Vim 中提供帮助你安装或更新插件的命令。以下是一些常用的插件管理工具：\ndein plug vim-addon-manager vundle 多行编辑 这是一种可以同时输入多行连续文本的技术。参考这个示例。\n用 \u0026lt;c-v\u0026gt; 切换到可视块模式。然后向下选中几行，按 I 或 A （译者注：大写字母，即 shift+i 或 shift+a）然后开始输入你想要输入的文本。\n在刚开始的时候可能会有些迷惑，因为文本只出现在了当前编辑的行，只有在当前的插入动作结束后，之前选中的其它行才会出现插入的文本。\n举一个简单的例子：\u0026lt;c-v\u0026gt;3jItext\u0026lt;esc\u0026gt;。\n如果你要编辑的行长度不同，但是你想在他们后面追加相同的内容的话，可以试一下这个：\u0026lt;c-v\u0026gt;3j$Atext\u0026lt;esc\u0026gt;。\n有时你可能需要把光标放到当前行末尾之后，默认情况下你是不可能做到的，但是可能通过设置 virtualedit 选项达到目的：\nset virtualedit=all 设置之后 $10l 或 90| 都会生效，即使超过了行尾的长度。\n详见 :h blockwise-examples。在开始的时候可能会觉得有些复杂，但是它很快就会成为你的第二天性的。\n如果你想探索更有趣的事情，可以看看多光标\n使用外部程序和过滤器 免责声明：Vim 是单线程的，因此在 Vim 中以前端进程执行其它的程序时会阻止其它的一切。当然你可以使用 Vim 程序接口，如 Lua，并且使用它的多线程支持，但是在那期间， Vim 的处理还是被阻止了。Neovim 添加了任务 API 解决了此问题。\n（据说 Bram 正在考虑在 Vim 中也添加任务控制。如果你使用了较新版本的的 Vim ，可以看一下 :helpgrep startjob。）\n使用 :! 启动一个新任务。如果你想列出当前工作目录下的所有文件，可以使用 :!ls。 用 | 来将结果通过管道重定向，如：:!ls -l | sort | tail -n5。\n没有使用范围时（译者注：范围就是 : 和 ! 之间的内容，. 表示当前行，+4 表示向下偏移 4 行，$ 表示最末行等，多行时用 , 将它们分开，如 .,$ 表示从当前行到末行），:! 会显示在一个可滚动的窗口中（译者注：在 GVim 和在终端里运行的结果稍有不同）。相反的，如果指定了范围，这些行会被过滤。这意味着它们会通过管道被重定向到过滤程序的 stdin，在处理后再通过过滤程序的 stdout 输出，用输出结果替换范围内的文本。例如：为接下来的 5 行文本添加行号，可以使用：\n:.,+4!nl -ba -w1 -s\u0026#39; \u0026#39; 由于手动添加范围很麻烦， Vim 提供了一些辅助方法以方便的添加范围。如果需要经常带着范围的话，你可以在可见模式中先选择，然后再按 : （译者注：选中后再按 ! 更方便）。还可以使用 ! 来取用一个 motion 的范围，如 !ipsort （译者注：原文为 !ip!sort ，但经过实验发现该命令执行报错，可能是因为 Vim 版本的原因造成的，新版本使用 ip 选择当前段落后自动在命令后添加了 ! ，按照作者的写法来看，可能之前的版本没有自动添加 ! ）可以将当前段落的所有行按字母表顺序进行排序。\n一个使用过滤器比较好的案例是Go 语言。它的缩进语法非常个性，甚至还专门提供了一个名为 gofmt 的过滤器来对 Go 语言的源文件进行正确的缩进。Go 语言的插件通常会提供一个名为 :Fmt 的函数，这个函数就是执行了 :%!gofmt 来对整个文件进行缩进。\n人们常用 :r !prog 将 prog 程序的插入放到当前行的下面，这对于脚本来说是很不错的选择，但是在使用的过程中我发现 !!ls 更加方便，它会用输出结果替换当前行的内容。（译者注：前面命令中的 prog 只是个占位符，在实际使用中需要替换成其它的程序，如 :r !ls，这就与后面的 !!ls 相对应了，两者唯一的不同是第一个命令不会覆盖当前行内容，但是第二个命令会）\n帮助文档：\n:h filter :h :read! Cscope Cscope 的功能比 ctags 要完善，但是只支持 C（通过设置 cscope.files 后同样支持 C++以及 Java）。\n鉴于 Tag 文件只是知道某个符号是在哪里定义的，cscope 的数据库里的数据信息就多的多了：\n符号是在哪里定义的？ 符号是在哪里被使用的？ 这个全局符号定义了什么？ 这个变量是在哪里被赋值的？ 这个函数在源文件的哪个位置？ 哪些函数调用了这个函数？ 这个函数调用了哪些函数？ \u0026ldquo;out of space\u0026quot;消息是从哪来的？ 在目录结构中当前的源文件在哪个位置？ 哪些文件引用了这个头文件？ 1. 构建数据库 在你项目的根目录执行下面的命令：\n$ cscope -bqR 这条命令会在当前目录下创建三个文件：cscope{,.in,.po}.out 。把它们想象成你的数据库。\n不幸的时 cscope 默认只分析 *.[c|h|y|l] 文件。如果你想在 Java 项目中使用 cscope ，需要这样做：\n$ find . -name \u0026#34;*.java\u0026#34; \u0026gt; cscope.files $ cscope -bq 2. 添加数据库 打开你新创建的数据库连接：\n:cs add cscope.out 检查连接已经创建成功：\n:cs show （当然你可以添加多个连接。）\n3. 查询数据库 :cs find \u0026lt;kind\u0026gt; \u0026lt;query\u0026gt; 如：:cs find d foo 会列出 foo(...) 调用的所有函数。\nKind 说明 s symbol：查找使用该符号的引用 g global：查找该全局符号的定义 c calls：查找调用当前方法的位置 t text：查找出现该文本的位置 e egrep：使用 egrep 搜索当前单词 f file：打开文件名 i includes：查询引入了当前文件的文件 d depends：查找当前方法调用的方法 推荐一些比较方便的映射，如：\nnnoremap \u0026lt;buffer\u0026gt; \u0026lt;leader\u0026gt;cs :cscope find s \u0026lt;c-r\u0026gt;=expand(\u0026#39;\u0026lt;cword\u0026gt;\u0026#39;)\u0026lt;cr\u0026gt;\u0026lt;cr\u0026gt; nnoremap \u0026lt;buffer\u0026gt; \u0026lt;leader\u0026gt;cg :cscope find g \u0026lt;c-r\u0026gt;=expand(\u0026#39;\u0026lt;cword\u0026gt;\u0026#39;)\u0026lt;cr\u0026gt;\u0026lt;cr\u0026gt; nnoremap \u0026lt;buffer\u0026gt; \u0026lt;leader\u0026gt;cc :cscope find c \u0026lt;c-r\u0026gt;=expand(\u0026#39;\u0026lt;cword\u0026gt;\u0026#39;)\u0026lt;cr\u0026gt;\u0026lt;cr\u0026gt; nnoremap \u0026lt;buffer\u0026gt; \u0026lt;leader\u0026gt;ct :cscope find t \u0026lt;c-r\u0026gt;=expand(\u0026#39;\u0026lt;cword\u0026gt;\u0026#39;)\u0026lt;cr\u0026gt;\u0026lt;cr\u0026gt; nnoremap \u0026lt;buffer\u0026gt; \u0026lt;leader\u0026gt;ce :cscope find e \u0026lt;c-r\u0026gt;=expand(\u0026#39;\u0026lt;cword\u0026gt;\u0026#39;)\u0026lt;cr\u0026gt;\u0026lt;cr\u0026gt; nnoremap \u0026lt;buffer\u0026gt; \u0026lt;leader\u0026gt;cf :cscope find f \u0026lt;c-r\u0026gt;=expand(\u0026#39;\u0026lt;cfile\u0026gt;\u0026#39;)\u0026lt;cr\u0026gt;\u0026lt;cr\u0026gt; nnoremap \u0026lt;buffer\u0026gt; \u0026lt;leader\u0026gt;ci :cscope find i ^\u0026lt;c-r\u0026gt;=expand(\u0026#39;\u0026lt;cfile\u0026gt;\u0026#39;)\u0026lt;cr\u0026gt;$\u0026lt;cr\u0026gt; nnoremap \u0026lt;buffer\u0026gt; \u0026lt;leader\u0026gt;cd :cscope find d \u0026lt;c-r\u0026gt;=expand(\u0026#39;\u0026lt;cword\u0026gt;\u0026#39;)\u0026lt;cr\u0026gt;\u0026lt;cr\u0026gt; 所以 :tag （或 \u0026lt;c-]\u0026gt;）跳转到标签定义的文件，而 :cstag 可以达到同样的目的，同时还会打开 cscope 的数据库连接。'cscopetag' 选项使得 :tag 命令自动的像 :cstag 一样工作。这在你已经使用了基于标签的映射时会非常方便。\n帮助文档：:h cscope\nMatchIt 由于 Vim 是用 C 语言编写的，因此许多功能都假设使用类似 C 语言的语法。默认情况下，如果你的光标在 { 或 #endif , 就可以使用 % 跳转到与之匹配的 } 或 #ifdef。\nVim 自带了一个名为 matchit.vim 的插件，但是默认没有启用。启用后可以用 % 在 HTML 相匹配的标签或 VimL 的 if/else/endif 块之间进行跳转，它还带来了一些新的命令。\n在 Vim 8 中安装 \u0026#34; vimrc packadd! matchit 在 Vim 7 或者更早的版本中安装 \u0026#34;vimrc runtime macros/matchit.vim 由于 matchit 的文档很全面，我建议安装以后执行一次下面的命令：\n:!mkdir -p ~/.vim/doc :!cp $VIMRUNTIME/macros/matchit.vim ~/.vim/doc :helptags ~/.vim/doc 简短的介绍 至此这个插件已经可以使用了。 参考 :h matchit-intro 来获得支持的命令以及 :h matchit-languages 来获得支持的语言。\n你可以很方便的定义自己的匹配对，如：\nautocmd FileType python let b:match_words = \u0026#39;\\\u0026lt;if\\\u0026gt;:\\\u0026lt;elif\\\u0026gt;:\\\u0026lt;else\\\u0026gt;\u0026#39; 之后你就可以在任何的 Python 文件中使用 % （向前）或 g% （向后）在这三个片断之间跳转了。\n帮助文档：\n:h matchit-install :h matchit :h b:match_words 技巧 跳至选择的区域另一端 在使用 v 或者 V 选择某段文字后，可以用 o 或者 O 按键跳至选择区域的开头或者结尾。\n:h v_o :h v_O 聪明地使用 n 和 N n 与 N 的实际跳转方向取决于使用 / 还是 ? 来执行搜索，其中 / 是向后搜索，? 是向前搜索。一开始我（原作者）觉得这里很难理解。\n如果你希望 n 始终为向后搜索，N 始终为向前搜索，那么只需要这样设置：\nnnoremap \u0026lt;expr\u0026gt; n \u0026#39;Nn\u0026#39;[v:searchforward] nnoremap \u0026lt;expr\u0026gt; N \u0026#39;nN\u0026#39;[v:searchforward] 聪明地使用命令行历史 我（原作者）习惯用 Ctrl + p 和 Ctrl + n 来跳转到上一个/下一个条目。其实这个操作也可以用在命令行中，快速调出之前执行过的命令。\n不仅如此，你会发现 上 和 下 其实更智能。如果命令行中已经存在了一些文字，我们可以通过按方向键来匹配已经存在的内容。比如，命令行中现在是 :echo，这时候我们按 上，就会帮我们补全成 :echo \u0026quot;Vim rocks!\u0026quot;（前提是，之前输入过这段命令）。\n当然，Vim 用户都不愿意去按方向键，事实上我们也不需要去按，只需要设置这样的映射：\ncnoremap \u0026lt;c-n\u0026gt; \u0026lt;down\u0026gt; cnoremap \u0026lt;c-p\u0026gt; \u0026lt;up\u0026gt; 这个功能，我（原作者）每天都要用很多次。\n智能 Ctrl-l Ctrl + l 的默认功能是清空并「重新绘制」当前的屏幕，就和 :redraw! 的功能一样。下面的这个映射就是执行重新绘制，并且取消通过 / 和 ? 匹配字符的高亮，而且还可以修复代码高亮问题（有时候，由于多个代码高亮的脚本重叠，或者规则过于复杂，Vim 的代码高亮显示会出现问题）。不仅如此，还可以刷新「比较模式」（请参阅 :help diff-mode）的代码高亮：\nnnoremap \u0026lt;leader\u0026gt;l :nohlsearch\u0026lt;cr\u0026gt;:diffupdate\u0026lt;cr\u0026gt;:syntax sync fromstart\u0026lt;cr\u0026gt;\u0026lt;c-l\u0026gt; 禁用错误报警声音和图标 set noerrorbells set novisualbell set t_vb= 请参阅 Vim Wiki: Disable beeping。\n快速移动当前行 有时，我（原作者）想要快速把当前行上移或下移一行，只需要这样设置映射：\nnnoremap [e :\u0026lt;c-u\u0026gt;execute \u0026#39;move -1-\u0026#39;. v:count1\u0026lt;cr\u0026gt; nnoremap ]e :\u0026lt;c-u\u0026gt;execute \u0026#39;move +\u0026#39;. v:count1\u0026lt;cr\u0026gt; 这个映射，同样可以搭配数字使用，比如连续按下 2 ] e 就可以把当前行向下移动两行。\n快速添加空行 nnoremap [\u0026lt;space\u0026gt; :\u0026lt;c-u\u0026gt;put! =repeat(nr2char(10), v:count1)\u0026lt;cr\u0026gt;\u0026#39;[ nnoremap ]\u0026lt;space\u0026gt; :\u0026lt;c-u\u0026gt;put =repeat(nr2char(10), v:count1)\u0026lt;cr\u0026gt; 设置之后，连续按下 5 [ 空格 在当前行上方插入 5 个空行。\n运行时检测 需要的特性：+profile\nVim 提供了一个内置的运行时检查功能，能够找出运行慢的代码。\n:profile 命令后面跟着子命令来确定要查看什么。\n如果你想查看所有的：\n:profile start /tmp/profile.log :profile file * :profile func * \u0026lt;do something in Vim\u0026gt; \u0026lt;quit Vim\u0026gt; Vim 不断地在内存中检查信息，只在退出的时候输出出来。（Neovim 已经解决了这个问题用 :profile dump 命令）\n看一下 /tmp/profile.log 文件，检查时运行的所有代码都会被显示出来，包括每一行代码运行的频率和时间。\n大多数代码都是用户不熟悉的插件代码，如果你是在解决一个确切的问题， 直接跳到这个日志文件的末尾，那里有 FUNCTIONS SORTED ON TOTAL TIME 和 FUNCTIONS SORTED ON SELF TIME 两个部分，如果某个 function 运行时间过长一眼就可以看到。\n查看启动时间 感觉 Vim 启动的慢？到了研究几个数字的时候了：\nvim --startuptime /tmp/startup.log +q \u0026amp;\u0026amp; vim /tmp/startup.log 第一栏是最重要的因为它显示了绝对运行时间，如果在前后两行之间时间差有很大的跳跃，那么是第二个文件太大或者含有需要检查的错误的 VimL 代码。\nNUL 符用新行表示 文件中的 NUL 符 （\\0），在内存中被以新行（\\n）保存，在缓存空间中显示为 ^@。\n更多信息请参看 man 7 ascii 和 :h NL-used-for-Nul 。\n快速编辑自定义宏 这个功能真的很实用！下面的映射，就是在一个新的命令行窗口中读取某一个寄存器（默认为 *）。当你设置完成后，只需要按下 回车 即可让它生效。\n在录制宏的时候，我经常用这个来更改拼写错误。\nnnoremap \u0026lt;leader\u0026gt;m :\u0026lt;c-u\u0026gt;\u0026lt;c-r\u0026gt;\u0026lt;c-r\u0026gt;=\u0026#39;let @\u0026#39;. v:register .\u0026#39; = \u0026#39;. string(getreg(v:register))\u0026lt;cr\u0026gt;\u0026lt;c-f\u0026gt;\u0026lt;left\u0026gt; 只需要连续按下 leader m 或者 \u0026quot; leader m 就可以调用了。\n请注意，这里之所以要写成 \u0026lt;c-r\u0026gt;\u0026lt;c-r\u0026gt; 是为了确保 \u0026lt;c-r\u0026gt; 执行了。请参阅 :h c_^R^R\n快速跳转到源(头)文件 这个技巧可以用在多种文件类型中。当你从源文件或者头文件中切换到其他文件的时候，这个技巧可以设置「文件标记」（请参阅 :h marks），然后你就可以通过连续按下 \u0026rsquo; C 或者 \u0026rsquo; H 快速跳转回去（请参阅 :h 'A）。\nautocmd BufLeave *.{c,cpp} mark C autocmd BufLeave *.h mark H 注意：由于这个标记是设置在 viminfo 文件中，因此请先确认 :set viminfo? 中包含了 :h viminfo-'。\n在 GUI 中快速改变字体大小 印象中，我（原作者）记得一下代码是来自 tpope\u0026rsquo;s 的配置文件：\ncommand! Bigger :let \u0026amp;guifont = substitute(\u0026amp;guifont, \u0026#39;\\d\\+$\u0026#39;, \u0026#39;\\=submatch(0)+1\u0026#39;, \u0026#39;\u0026#39;) command! Smaller :let \u0026amp;guifont = substitute(\u0026amp;guifont, \u0026#39;\\d\\+$\u0026#39;, \u0026#39;\\=submatch(0)-1\u0026#39;, \u0026#39;\u0026#39;) 根据模式改变光标类型 我（原作者）习惯在普通模式下用块状光标，在插入模式下用条状光标（形状类似英文 \u0026ldquo;I\u0026rdquo; 的样子），然后在替换模式中使用下划线形状的光标。\nif empty($TMUX) let \u0026amp;t_SI = \u0026#34;\\\u0026lt;Esc\u0026gt;]50;CursorShape=1\\x7\u0026#34; let \u0026amp;t_EI = \u0026#34;\\\u0026lt;Esc\u0026gt;]50;CursorShape=0\\x7\u0026#34; let \u0026amp;t_SR = \u0026#34;\\\u0026lt;Esc\u0026gt;]50;CursorShape=2\\x7\u0026#34; else let \u0026amp;t_SI = \u0026#34;\\\u0026lt;Esc\u0026gt;Ptmux;\\\u0026lt;Esc\u0026gt;\\\u0026lt;Esc\u0026gt;]50;CursorShape=1\\x7\\\u0026lt;Esc\u0026gt;\\\\\u0026#34; let \u0026amp;t_EI = \u0026#34;\\\u0026lt;Esc\u0026gt;Ptmux;\\\u0026lt;Esc\u0026gt;\\\u0026lt;Esc\u0026gt;]50;CursorShape=0\\x7\\\u0026lt;Esc\u0026gt;\\\\\u0026#34; let \u0026amp;t_SR = \u0026#34;\\\u0026lt;Esc\u0026gt;Ptmux;\\\u0026lt;Esc\u0026gt;\\\u0026lt;Esc\u0026gt;]50;CursorShape=2\\x7\\\u0026lt;Esc\u0026gt;\\\\\u0026#34; endif 原理很简单，就是让 Vim 在进入和离开插入模式的时候，输出一些序列，请参考 escape sequence。Vim 与终端之间的中间层，比如 tmux 会处理并执行上面的代码。\n但上面这个还是有一个缺点的。终端环境的内部原理不尽相同，对于序列的处理方式也稍有不同。因此，上面的代码可能无法在你的环境中运行。甚至，你的运行环境也有可能不支持其他光标形状，请参阅你的 Vim 运行环境的文档。\n好消息是，上面这个代码，可以在 iTerm2 中完美运行。\n防止水平滑动的时候失去选择 如果你选中了一行或多行，那么你可以用 \u0026lt; 或 \u0026gt; 来调整他们的缩进。但在调整之后就不会保持选中状态了。\n你可以连续按下 g v 来重新选中他们，请参考 :h gv。因此，你可以这样来配置映射：\nxnoremap \u0026lt; \u0026lt;gv xnoremap \u0026gt; \u0026gt;gv 设置好之后，在可视模式中使用 \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 就不会再出现上面提到的问题了。\n选择当前行至结尾，排除换行符 在 Vim 里，我们可以同过 v$ 选择当前行至结尾，但此时会把最后一个换行符也选中，通常需要按额外的 h 来取消最后选中最后一个换行符号。 Vim 提供了一个 g_ 快捷键，可以移动光标至最后一个非空字符。因此，为达到次效果，可以使用 vg_。当然，如果觉得按三个键比较麻烦， 可以添加一个映射：\nnnoremap L g_ 这样就可以通过 vL 达到一样的效果了。\n重新载入保存文件 通过自动命令，你可以在保存文件的同时触发一些其他功能。比如，如果这个文件是一个配置文件，那么就重新载入；或者你还可以对这个文件进行代码风格检查。\nautocmd BufWritePost $MYVIMRC source $MYVIMRC autocmd BufWritePost ~/.Xdefaults call system(\u0026#39;xrdb ~/.Xdefaults\u0026#39;) 更加智能的当前行高亮 我（原作者）很喜欢「当前行高亮」（请参阅 :h cursorline）这个功能，但我只想让这个效果出现在当前窗口，而且在插入模式中关闭这个效果：\nautocmd InsertLeave,WinEnter * set cursorline autocmd InsertEnter,WinLeave * set nocursorline 更快的关键字补全 关键字补全（\u0026lt;c-n\u0026gt; 或 \u0026lt;c-p\u0026gt;）功能的工作方式是，无论 'complete' 设置中有什么，它都会尝试着去补全。这样，一些我们用不到的标签也会出现在补全列表中。而且，它会扫描很多文件，有时候运行起来非常慢。如果你不需要这些，那么完全可以像这样把它们禁用掉：\nset complete-=i \u0026#34; disable scanning included files set complete-=t \u0026#34; disable searching tags 改变颜色主题的默认外观 如果你想让状态栏在颜色主题更改后依然保持灰色，那么只需要这样设置：\nautocmd ColorScheme * highlight StatusLine ctermbg=darkgray cterm=NONE guibg=darkgray gui=NONE 同理，如果你想让某一个颜色主题（比如 \u0026ldquo;lucius\u0026rdquo;）的状态栏为灰色（请使用 :echo color_name 来查看当前可用的所有颜色主题）：\nautocmd ColorScheme lucius highlight StatusLine ctermbg=darkgray cterm=NONE guibg=darkgray gui=NONE 命令 下面的命令都比较有用，最好了解一下。用 :h :\u0026lt;command name\u0026gt; 来了解更多关于它们的信息，如：:h :global。\n:global 和 :vglobal - 在所有匹配行执行命令 在所有符合条件的行上执行某个命令。如： :global /regexp/ print 会在所有包含 \u0026ldquo;regexp\u0026rdquo; 的行上执行 print 命令（译者注：regexp 有正则表达式的意思，该命令同样支持正则表达式，在所有符合正则表达式的行上执行指定的命令）。\n趣闻：你们可能都知道老牌的 grep 命令，一个由 Ken Thompson 编写的过滤程序。它是干什么用的呢？它会输出所有匹配指定正则表达式的行！现在猜一下 :global /regexp/ print 的简写形式是什么？没错！就是 :g/re/p 。 Ken Thompsom 在编写 grep 程序的时候是受了 vi :global 的启发。（译者注： https://robots.thoughtbot.com/how-grep-got-its-name）\n既然它的名字是 :global，理应仅作用在所有行上，但是它也是可以带范围限制的。假设你想使用 :delete 命令删除从当前行到下一个空行（由正则表达式 ^$ 匹配）范围内所有包含 \u0026ldquo;foo\u0026rdquo; 的行：\n:,/^$/g/foo/d 如果要在所有 不 匹配的行上执行命令的话，可以使用 :global! 或是它的别名 :vglobal （ V 代表的是 inVerse ）。\n:normal 和 :execute - 脚本梦之队 这两个命令经常在 Vim 的脚本里使用。\n借助于 :normal 可以在命令行里进行普通模式的映射。如：:normal! 4j 会令光标下移 4 行（由于加了\u0026rdquo;!\u0026quot;，所以不会使用自定义的映射 \u0026ldquo;j\u0026rdquo;）。\n需要注意的是 :normal 同样可以使用范围数（译者注：参考 :h range 和 :h :normal-range 了解更多），故 :%norm! Iabc 会在所有行前加上 \u0026ldquo;abc\u0026rdquo;。\n借助于 :execute 可以将命令和表达式混合在一起使用。假设你正在编辑一个 C 语言的文件，想切换到它的头文件：\n:execute \u0026#39;edit\u0026#39; fnamemodify(expand(\u0026#39;%\u0026#39;), \u0026#39;:r\u0026#39;) . \u0026#39;.h\u0026#39; （译者注：头文件为与与源文件同名但是扩展名为 .h 的文件。上面的命令中 expand 获得当前文件的名称，fnamemodify 获取不带扩展名的文件名，再连上 \u0026lsquo;.h\u0026rsquo; 就是头文件的文件名了，最后在使用 edit 命令打开这个头文件。）\n这两个命令经常一起使用。假设你想让光标下移 n 行：\n:let n = 4 :execute \u0026#39;normal!\u0026#39; n . \u0026#39;j\u0026#39; 重定向消息 许多命令都会输出消息，:redir 用来重定向这些消息。它可以将消息输出到文件、寄存器或是某个变量中。\n\u0026#34; 将消息重定向到变量 `neatvar` 中 :redir =\u0026gt; neatvar \u0026#34; 打印所有寄存器的内容 :reg \u0026#34; 结束重定向 :redir END \u0026#34; 输出变量 :echo neatvar \u0026#34; 恶搞一下，我们把它输出到当前缓冲区 :put =neatvar 再 Vim 8 中，可以更简单的方式即位：\n:put =execute('reg') （译者注：原文最后一条命令是 :put =nicevar 但是实际会报变量未定义的错误） （实测 neovim/vim8 下没问题）\n帮助文档：:h :redir\n调试 常规建议 如果你遇到了奇怪的行为，尝试用这个命令重现它：\nvim -u NONE -N 这样会在不引用 vimrc（默认设置）的情况下重启 vim，并且在 nocompatible 模式下（使用 vim 默认设置而不是 vi 的）。（搜索 :h --noplugin 命令了解更多启动加载方式）\n如果仍旧能够出现该错误，那么这极有可能是 vim 本身的 bug，请给 vim_dev 发送邮件反馈错误，多数情况下问题不会立刻解决，你还需要进一步研究\n许多插件经常会提供新的（默认的/自动的）操作。如果在保存的时候发生了，那么请用 :verb au BufWritePost 命令检查潜在的问题\n如果你在使用一个插件管理工具，将插件行注释调，再进行调试。\n问题还没有解决？如果不是插件的问题，那么肯定是你的自定义的设置的问题，可能是你的 options 或 autocmd 等等。\n到了一行行代码检查的时候了，不断地排除缩小检查范围知道你找出错误，根据二分法的原理你不会花费太多时间的。\n在实践过程中，可能就是这样，把 :finish 放在你的 vimrc 文件中间，Vim 会跳过它之后的设置。如果问题还在，那么问题就出在:finish之前的设置中，再把:finish放到前一部分设置的中间位置。否则问题就出现在它后面的半部分设置，那么就把:finish放到后半部分的中间位置。不断的重复即可找到。\n调整日志等级 Vim 现在正在使用的另一个比较有用的方法是增加 debug 信息输出详细等级。现在 Vim 支持 9 个等级，可以用:h 'verbose'命令查看。\n:e /tmp/foo :set verbose=2 :w :set verbose=0 这可以显示出所有引用的文件、没有变化的文件或者各种各样的作用于保存的插件。\n如果你只是想用简单的命令来提高等级，也是用 :verbose ，放在其他命令之前，通过计数来指明等级，默认是 1.\n:verb set verbose \u0026#34; verbose=1 :10verb set verbose \u0026#34; verbose=10 通常用等级 1 来显示上次从哪里设置的选项\n:verb set ai? \u0026#34; Last set from ~/.vim/vimrc 一般等级越高输出信息月详细。但是不要害怕，亦可以把输出导入到文件中：\n:set verbosefile=/tmp/foo | 15verbose echo \u0026#34;foo\u0026#34; | vsplit /tmp/foo 你可以一开始的时候就打开 verbosity，用 -V 选项，它默认设置调试等级为 10。 例如：vim -V5\n查看启动日志 查看运行时日志 Vim 脚本调试 如果你以前使用过命令行调试器的话，对于:debug命令你很快就会感到熟悉。\n只需要在任何其他命令之前加上:debug就会让你进入调试模式。也就是，被调试的 Vim 脚本会在第一行停止运行，同时该行会被显示出来。\n想了解可用的 6 个调试命令，可以查阅:h \u0026gt;cont和阅读下面内容。需要指出的是，类似 gdb 和其他相似调试器，调试命令可以使用它们的简短形式：c、 q、n、s、 i和 f。\n除了上面的之外，你还可以自由地使用任何 Vim 的命令。比如，:echo myvar，该命令会在当前的脚本代码位置和上下文上被执行。\n只需要简单使用:debug 1，你就获得了REPL调试特性。\n当然，调试模式下是可以定义断点的，不然的话每一行都去单步调试就会十分痛苦。（断点之所以被叫做断点，是因为运行到它们的时候，运行就会停止下来。因此，你可以利用断点跳过自己不感兴趣的代码区域）。请查阅:h :breakadd、 :h :breakdel和 :h :breaklist获取更多细节。\n假设你需要知道你每次在保存一个文件的时候有哪些代码在运行：\n:au BufWritePost \u0026#34; signify BufWritePost \u0026#34; * call sy#start() :breakadd func *start :w \u0026#34; Breakpoint in \u0026#34;sy#start\u0026#34; line 1 \u0026#34; Entering Debug mode. Type \u0026#34;cont\u0026#34; to continue. \u0026#34; function sy#start \u0026#34; line 1: if g:signify_locked \u0026gt;s \u0026#34; function sy#start \u0026#34; line 3: endif \u0026gt; \u0026#34; function sy#start \u0026#34; line 5: let sy_path = resolve(expand(\u0026#39;%:p\u0026#39;)) \u0026gt;q :breakdel * 正如你所见，使用\u0026lt;cr\u0026gt;命令会重复之前的调试命令，也就是在该例子中的s命令。\n:debug命令可以和verbose选项一起使用。\n语法文件调试 语法文件由于包含错误的或者复制的正则表达式，常常会使得 Vim 的运行较慢。如果 Vim 在编译的时候包含了+profile feature特性，就可以给用户提供一个超级好用的:syntime命令。\n:syntime on \u0026#34; 多次敲击\u0026lt;c-l\u0026gt;来重绘窗口，这样的话就会使得相应的语法规则被重新应用一次 :syntime off :syntime report 输出结果包含了很多的度量维度。比如，你可以通过结果知道哪些正则表达式耗时太久需要被优化；哪些正则表达式一直在别使用但重来没有一次成功匹配。\n请查阅:h :syntime。\n杂项 附加资源 资源名称 简介 七个高效的文本编辑习惯 作者：Bram Moolenaar（即 Vim 的作者） 七个高效的文本编辑习惯 2.0（PDF 版） 同上 IBM DeveloperWorks: 使用脚本编写 Vim 编辑器 Vim 脚本编写五辑 《漫漫 Vim 路》 使用魔抓定制 Vim 插件 《 Vim 实践 (第 2 版)》 轻取 Vim 最佳书籍 Vimcasts.org Vim 录屏演示 为什么是个脚本都用 vi？ 常见误区释疑 你不爱 vi，所以你不懂 Vim 简明,扼要,准确的干货 Vim 配置集合 目前，网上有很多流行 Vim 配置集合，对于 Vim 配置集合，个人认为有利有弊。 对于维护的比较好的配置，比如 SpaceVim 还是值得尝试的，可以节省很多自行配置的时间。 当然，网上还有很多其他很流行的配置，比如：\nk-vim amix\u0026rsquo;s vimrc janus 常见问题 编辑小文件时很慢 有两个因素对性能影响非常大：\n过于复杂的 正则表达式 。尤其是 Ruby 的语法文件，以前会造成性能下降。（见调试语法文件）\n屏幕重绘 。有一些功能会强制重绘所有行。\n典型肇事者 原因 解决方案 :set cursorline 会导致所有行重绘 :set nocursorline :set cursorcolumn 会导致所有行重绘 :set nocursorcolumn :set relativenumber 会导致所有行重绘 :set norelativenumber :set foldmethod=syntax 如果语法文件已经很慢了，这只会变得更慢 :set foldmethod=manual，:set foldmethod=marker 或者使用快速折叠插件 :set synmaxcol=3000 由于内部表示法，Vim 处理比较长的行时会有问题。让它高亮到 3000 列…… :set synmaxcol=200 matchparen.vim Vim 默认加载的插件，用正则表达式查找配对的括号 禁用插件：:h matchparen 注意：只有在你真正遇到性能问题的时候才需要做上面的调整。在大多数情况下使用上面提到的选项是完全没有问题的。\n编辑大文件的时候很慢 Vim 处理大文件最大的问题就是它会一次性读取整个文件。这么做是由于缓冲区的内部机理导致的（在 vim_dev 中讨论）。\n如果只是想查看的话，tail hugefile | vim - 是一个不错的选择。\n如果你能接受没有语法高亮，并且禁用所有插件和设置的话，使用：\n$ vim -u NONE -N 这将会使得跳转变快很多，尤其是省去了基于很耗费资源的正则表达式的语法高亮。你还可以告诉 Vim 不要使用交换文件和 viminfo 文件，以避免由于写这些文件而造成的延时：\n$ vim -n -u NONE -i NONE -N 简而言之，尽量避免使用 Vim 写过大的文件。\n持续粘贴（为什么我每次都要设置 \u0026lsquo;paste\u0026rsquo; 模式） 持续粘贴模式让终端模拟器可以区分输入内容与粘贴内容。\n你有没有遇到过往 Vim 里粘贴代码之后被搞的一团糟？\n这在你使用 cmd+v、shirt-insert、middle-click 等进行粘贴的时候才会发生。 因为那样的话你只是向终端模拟器扔了一大堆的文本。 Vim 并不知道你刚刚是粘贴的文本，它以为你在飞速的输入。 于是它想缩进这些行但是失败了。\n这明显不是个问题，如果你用 Vim 的寄存器粘贴，如：\u0026quot;+p ，这时 Vim 就知道了你在粘贴，就不会导致格式错乱了。\n使用 :set paste 就可以解决这个问题正常进行粘贴。见 :h 'paste' 和 :h 'pastetoggle' 获取更多信息。\n如果你受够了每次都要设置 'paste' 的话，看看这个能帮你自动设置的插件：bracketed-paste。\n点此查看该作者对于这个插件的更多描述。\nNeovim 尝试把这些变得更顺畅，如果终端支持的话，它会自动开启持续粘贴模式，无须再手动进行切换。\n在终端中按 ESC 后有延时 如果你经常使用命令行，那么肯定要接触 终端模拟器 ，如 xterm、gnome-terminal、iTerm2 等等（与实际的终端不同）。\n终端模拟器与他们的祖辈一样，使用 转义序列 （也叫 控制序列 ）来控制光标移动、改变文本颜色等。转义序列就是以转义字符开头的 ASCII 字符串（用脱字符表示法表示成 ^[ ）。当遇到这样的字符串后，终端模拟器会从终端信息数据库中查找对应的动作。\n为了使用问题更加清晰，我会先来解释一下什么是映射超时。在映射存在歧义的时候就会产生映射超时：\n:nnoremap ,a :echo \u0026#39;foo\u0026#39;\u0026lt;cr\u0026gt; :nnoremap ,ab :echo \u0026#39;bar\u0026#39;\u0026lt;cr\u0026gt; 上面的例子中两个映射都能正常工作，但是当输入 ,a 之后，Vim 会延时 1 秒，因为它要确认用户是否还要输入那个 b。\n转义序列会产生同样的问题：\n\u0026lt;esc\u0026gt; 作为返回普通模式或取消某个动作的按键而被大量使用 光标键使用转义序列进行的编码 Vim 期望 Alt （也叫作 Mate Key ）会发送一个正确的 8-bit 编码的高位，但是许多终端模拟器并不支持这个（也可能默认没有启用），而只是发送一个转义序列作为代替。 你可以这样测试上面所提到的事情： vim -u NONE -N 然后输入 i\u0026lt;c-v\u0026gt;\u0026lt;left\u0026gt; ，你会看到一个以 ^[ 开头的字符串，表明这是一个转义序列，^[ 就是转义字符。\n简而言之，Vim 在区分录入的 \u0026lt;esc\u0026gt; 和转义序列的时候需要一定的时间。\n默认情况下，Vim 用 :set timeout timeoutlen=1000，就是说它会用 1 秒的时间来区分有歧义的映射 以及 按键编码。这对于映射来说是一个比较合理的值，但是你可以自行定义按键延时的长短，这是解决该问题最根本的办法：\nset timeout \u0026#34; for mappings set timeoutlen=1000 \u0026#34; default value set ttimeout \u0026#34; for key codes set ttimeoutlen=10 \u0026#34; unnoticeable small value 在 :h ttimeout 里你可以找到一个关于这些选项之间关系的小表格。\n而如果你在 tmux 中使用 Vim 的话，别忘了把下面的配置加入到你的 ~/.tmux.conf文件中：\nset -sg escape-time 0 无法重复函数中执行的搜索 在命令中的搜索（/、:substitute 等）内容会改变“上次使用的搜索内容”。（它保存在/寄存器中，用 :echo @/ 可以输出它里面的内容） 简单的文本变化可以通过 . 重做。（它保存在 . 寄存器，用 :echo @. 可以输出它的内容） 而在你在函数中进行这些操作的时候，一切就会变得不同。因此你不能用 N/n 查找某个函数刚刚查找的内容，也不能重做函数中对文本的修改。\n帮助文档：:h function-search-undo。\n进阶阅读 Vim 插件开发指南 参考资料 Nifty Little Nvim Techniques to Make My Life Easier \u0026ndash; Series 1 整理自：https://github.com/wsdjeg/vim-galore-zh_cn\n","permalink":"http://121.199.2.5:6080/fhRygg/","summary":"什么是 Vim？ Vim 是一个历史悠久的文本编辑器，可以追溯到 qed。 Bram Moolenaar 于 1991 年发布初始版本。\nLinux、Mac 用户，可以使用包管理器安装 Vim，对于 Windows 用户，可以从 我的网盘 下载。 该版本可轻易添加 python 、python3 、lua 等支持，只需要安装 python、lua 即可。\n项目在 Github 上开发，项目讨论请订阅 vim_dev 邮件列表。\n通过阅读 Why, oh WHY, do those #?@! nutheads use vi? 来对 Vim 进行大致的了解。\nVim 哲学 Vim 采用模式编辑的理念，即它提供了多种模式，按键在不同的模式下作用不同。 你可以在普通模式 下浏览文件，在插入模式下插入文本， 在可视模式下选择行，在命令模式下执行命令等等。起初这听起来可能很复杂， 但是这有一个很大的优点：不需要通过同时按住多个键来完成操作， 大多数时候你只需要依次按下这些按键即可。越常用的操作，所需要的按键数量越少。\n和模式编辑紧密相连的概念是 操作符 和 动作。操作符 指的是开始某个行为， 例如：修改、删除或者选择文本，之后你要用一个 动作 来指定需要操作的文本区域。 比如，要改变括号内的文本，需要执行 ci( （读做 change inner parentheses）； 删除整个段落的内容，需要执行 dap （读做：delete around paragraph）。\n如果你能看见 Vim 老司机操作，你会发现他们使用 Vim 脚本语言就如同钢琴师弹钢琴一样。复杂的操作只需要几个按键就能完成。他们甚至不用刻意去想，因为这已经成为肌肉记忆了。这减少认识负荷并帮助人们专注于实际任务。\n入门 Vim 自带一个交互式的教程，内含你需要了解的最基础的信息，你可以通过终端运行以下命令打开教程：\n$ vimtutor 不要因为这个看上去很无聊而跳过，按照此教程多练习。你以前用的 IDE 或者其他编辑器很少是有“模式”概念的，因此一开始你会很难适应模式切换。但是你 Vim 使用的越多，肌肉记忆 将越容易形成。\nVim 基于一个 vi 克隆，叫做 Stevie，支持两种运行模式：\u0026ldquo;compatible\u0026rdquo; 和 \u0026ldquo;nocompatible\u0026rdquo;。在兼容模式下运行 Vim 意味着使用 vi 的默认设置，而不是 Vim 的默认设置。除非你新建一个用户的 vimrc 或者使用 vim -N 命令启动 Vim，否则就是在兼容模式下运行 Vim！请大家不要在兼容模式下运行 Vim。","title":"Vim 从入门到精通"},{"content":"数字货币的交易所有很多。\n比特儿 邀请注册：https://www.gate.io/signup/633017 ，点击这个链接注册的用户可以获取90$的交易点卡哦。\ngate.io 8年长期稳定运营，行业口碑品牌，安全可靠，注册即可获得手续费优惠!\nGate.io 作为前十的交易所它最吸引人的地方是秒充秒提，在速度上要比其他交易所好很多。同时它还独有地址共享技术，充错币的情况也不会发生。除此之外他们平台的活动也很多，不是只有交易才能获得奖励，写一句话写篇文章都是可以获得奖励的，类似的活动非常多。所以总的来说，Gate.io交易所还是很不错的。\nGate.io 上可以购买的币种也非常多，可以购买比特币、购买eth，购买dot，所有主流的数字货币都可以购买。\n最后一句提醒各位，数字货币价格波动很大，投资须谨慎！\n","permalink":"http://121.199.2.5:6080/0PyB8g/","summary":"数字货币的交易所有很多。\n比特儿 邀请注册：https://www.gate.io/signup/633017 ，点击这个链接注册的用户可以获取90$的交易点卡哦。\ngate.io 8年长期稳定运营，行业口碑品牌，安全可靠，注册即可获得手续费优惠!\nGate.io 作为前十的交易所它最吸引人的地方是秒充秒提，在速度上要比其他交易所好很多。同时它还独有地址共享技术，充错币的情况也不会发生。除此之外他们平台的活动也很多，不是只有交易才能获得奖励，写一句话写篇文章都是可以获得奖励的，类似的活动非常多。所以总的来说，Gate.io交易所还是很不错的。\nGate.io 上可以购买的币种也非常多，可以购买比特币、购买eth，购买dot，所有主流的数字货币都可以购买。\n最后一句提醒各位，数字货币价格波动很大，投资须谨慎！","title":"购买比特币"},{"content":"为了节省空间，docker 容器里面有很多命令是没有安装的，提示： Reading package lists\u0026hellip; Done Building dependency tree Reading state information\u0026hellip; Done E: Unable to locate package vim\n更新apt-get源 这时候需要敲：apt-get update\n这个命令的作用是：同步 /etc/apt/sources.list 和 /etc/apt/sources.list.d 中列出的源的索引，这样才能获取到最新的软件包。\n安装vim apt-get install vim\n安装telnet apt-get install telnet\n安装ifconfig apt-get install net-tools\n","permalink":"http://121.199.2.5:6080/d5nhvK/","summary":"为了节省空间，docker 容器里面有很多命令是没有安装的，提示： Reading package lists\u0026hellip; Done Building dependency tree Reading state information\u0026hellip; Done E: Unable to locate package vim\n更新apt-get源 这时候需要敲：apt-get update\n这个命令的作用是：同步 /etc/apt/sources.list 和 /etc/apt/sources.list.d 中列出的源的索引，这样才能获取到最新的软件包。\n安装vim apt-get install vim\n安装telnet apt-get install telnet\n安装ifconfig apt-get install net-tools","title":"docker apt-get"},{"content":"有时需要查看本地开发的git项目的远程仓库地址\ngit remote -v 可以方便的查看远程仓库的地址\ngit remote -v origin https://gitee.com/leonardodacn/pixiublog.git (fetch) origin https://gitee.com/leonardodacn/pixiublog.git (push) -v : verbose，冗余的意思\n","permalink":"http://121.199.2.5:6080/ss9sfI/","summary":"有时需要查看本地开发的git项目的远程仓库地址\ngit remote -v 可以方便的查看远程仓库的地址\ngit remote -v origin https://gitee.com/leonardodacn/pixiublog.git (fetch) origin https://gitee.com/leonardodacn/pixiublog.git (push) -v : verbose，冗余的意思","title":"git 查看远程地址"},{"content":"OLAP 在数据仓库一章中我们讨论了数据仓库及其模型的概念，而数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发的支持用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题历史数据进行分析，支持管理决策。\nOLAP 与 OLTP 在互联网浪潮出现之前，企业的数据量普遍不大，特别是核心的业务数据，通常一个单机的数据库就可以保存。那时候的存储并不需要复杂的架构，所有的线上请求(OLTP, Online Transactional Processing) 和后台分析 (OLAP, Online Analytical Processing) 都跑在同一个数据库实例上。后来渐渐的业务越来越复杂，数据量越来越大，DBA 们再也优化不动 SQL 了。其中一个显著问题是：单机数据库支持线上的 TP 请求已经非常吃力，没办法再跑比较重的 AP 分析型任务。跑起来要么 OOM，要么影响线上业务，要么做了主从分离、分库分表之后很难实现业务需求。\n在这样的背景下，以 Hadoop 为代表的大数据技术开始蓬勃发展，它用许多相对廉价的 x86 机器构建了一个数据分析平台，用并行的能力破解大数据集的计算问题。所以从某种程度上说，大数据技术可以算是传统关系型数据库技术发展过程的一个分支。当然在过程中大数据领域也发展出了属于自己的全新场景，诞生了许多新的技术，这个不深入提了。\n由此，架构师把存储划分成线上业务和数据分析两个模块。如下图所示，业务库的数据通过 ETL 工具抽取出来，导入专用的分析平台。业务数据库专注提供 TP 能力，分析平台提供 AP 能力，各施其职，看起来已经很完美了。但其实这个架构也有自己的不足。\nHTAP 首先是复杂性问题。本身 ETL 过程就是一个很繁琐的过程，一个例证是 ETL 做的好，可以成为一个商业模式。因为是两个系统，必然带来更高的学习成本、维护成本和整合成本。如果你使用的是开源的大数据工具搭建的分析平台，那么肯定会遇到各种工具之间的磨合的问题，还有由于各种工具良莠不齐所导致的质量问题。\n其次是实时性问题。通常我们认为越接近实时的数据，它的价值越大。很多业务场景，对实时性有很高的要求，比如风控系统，它需要对数据不停的分析，并且在险情出现之后尽快响应。而通常的 ETL 是一个周期性的操作，比如一天或者一个小时导一次数据，数据实时性是没有办法保证的。最后是一致性问题。一致性在数据库里面是很重要的概念，数据库的事务就是用来保证一致性的。如果把数据分表存储在两个不同的系统内，那么很难保证一致性，即 AP 系统的查询结果没有办法与线上业务正确对应。那么这两个系统的联动效应就会受到限制，比如用户没办法在一个事务里面，同时访问两个系统的数据。\n由于现有的数据平台存在的以上局限性，我们认为开发一个HTAP（Hybrid Transactional/Analytical Processing）融合型数据库产品可以缓解大家在 TP or AP 抉择上的焦虑，或者说，让数据库的使用者不用考虑过度复杂的架构，在一套数据库中既能满足 OLTP 类需求，也能满足 OLAP 类需求。\n","permalink":"http://121.199.2.5:6080/OLAP/","summary":"OLAP 在数据仓库一章中我们讨论了数据仓库及其模型的概念，而数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发的支持用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题历史数据进行分析，支持管理决策。\nOLAP 与 OLTP 在互联网浪潮出现之前，企业的数据量普遍不大，特别是核心的业务数据，通常一个单机的数据库就可以保存。那时候的存储并不需要复杂的架构，所有的线上请求(OLTP, Online Transactional Processing) 和后台分析 (OLAP, Online Analytical Processing) 都跑在同一个数据库实例上。后来渐渐的业务越来越复杂，数据量越来越大，DBA 们再也优化不动 SQL 了。其中一个显著问题是：单机数据库支持线上的 TP 请求已经非常吃力，没办法再跑比较重的 AP 分析型任务。跑起来要么 OOM，要么影响线上业务，要么做了主从分离、分库分表之后很难实现业务需求。\n在这样的背景下，以 Hadoop 为代表的大数据技术开始蓬勃发展，它用许多相对廉价的 x86 机器构建了一个数据分析平台，用并行的能力破解大数据集的计算问题。所以从某种程度上说，大数据技术可以算是传统关系型数据库技术发展过程的一个分支。当然在过程中大数据领域也发展出了属于自己的全新场景，诞生了许多新的技术，这个不深入提了。\n由此，架构师把存储划分成线上业务和数据分析两个模块。如下图所示，业务库的数据通过 ETL 工具抽取出来，导入专用的分析平台。业务数据库专注提供 TP 能力，分析平台提供 AP 能力，各施其职，看起来已经很完美了。但其实这个架构也有自己的不足。\nHTAP 首先是复杂性问题。本身 ETL 过程就是一个很繁琐的过程，一个例证是 ETL 做的好，可以成为一个商业模式。因为是两个系统，必然带来更高的学习成本、维护成本和整合成本。如果你使用的是开源的大数据工具搭建的分析平台，那么肯定会遇到各种工具之间的磨合的问题，还有由于各种工具良莠不齐所导致的质量问题。\n其次是实时性问题。通常我们认为越接近实时的数据，它的价值越大。很多业务场景，对实时性有很高的要求，比如风控系统，它需要对数据不停的分析，并且在险情出现之后尽快响应。而通常的 ETL 是一个周期性的操作，比如一天或者一个小时导一次数据，数据实时性是没有办法保证的。最后是一致性问题。一致性在数据库里面是很重要的概念，数据库的事务就是用来保证一致性的。如果把数据分表存储在两个不同的系统内，那么很难保证一致性，即 AP 系统的查询结果没有办法与线上业务正确对应。那么这两个系统的联动效应就会受到限制，比如用户没办法在一个事务里面，同时访问两个系统的数据。\n由于现有的数据平台存在的以上局限性，我们认为开发一个HTAP（Hybrid Transactional/Analytical Processing）融合型数据库产品可以缓解大家在 TP or AP 抉择上的焦虑，或者说，让数据库的使用者不用考虑过度复杂的架构，在一套数据库中既能满足 OLTP 类需求，也能满足 OLAP 类需求。","title":"OLAP"},{"content":"数据中台 阿里在 2018 年提出了所谓“数据中台”的概念：即数据被统一采集，规范数据语义和业务口径形成企业基础数据模型，提供统一的分析查询和新业务的数据对接能力。数据中台并不是新的颠覆式技术，而是一种企业数据资产管理和应用方法学，涵盖了数据集成、数据质量管理、元数据与主数据管理、数仓建模、支持高并发访问的数据服务接口层开发等内容。\n在数据中台建设中，结合企业自身的业务需求特点，架构和功能可能各不相同，但其中一个最基本的需求是数据采集的实时性和完整性。数据从源端产生，到被采集到数据汇集层的时间要尽可能短，至少应做到秒级延迟，这样中台的数据模型更新才可能做到近实时，构建在中台之上依赖实时数据流驱动的应用（例如商品推荐、欺诈检测等）才能够满足业务的需求。\n以阿里双十一为例，在极高的并发情况下，订单产生到大屏统计数据更新延迟不能超过 5s，一般在 2s 内。中台对外提供的数据应该是完整的，源端数据的 Create、Update 和 Delete 都要能够被捕获，不能少也不能多，即数据需要有端到端一致性的能力（Exactly Once Semantic，EOS）。当然，EOS 并非在任何业务场景下都需要，但从平台角度必须具备这种能力，并且允许用户根据业务需求灵活开启和关闭。\n数据中台的产生背景 起初，企业只有一个主营业务，比如电商，但随着公司战略和发展需要，会新增多支业务线，由于存在负责业务线开发的团队不一致，随之而来的就是风格迥异的代码风格和数据烟囱问题。\n数据中台的产生就是为了解决数据烟囱的问题，打通数据孤岛，让数据活起来，让数据产生价值，结合前台能力，达到快速响应用户的目标。\n中台只会同步能服务于超过两个业务线的数据，如果仅仅带有自身业务属性(不存在共性)的数据，不在中台的考虑范围内。例如:电商的产品产地信息，对于金融业务来说，其实是没有价值的，但电商的用户收货地址对金融业务来说是有价值的。所以不要简单的认为数据中台会汇集企业的所有数据，还是有侧重点的。导致这个结果的原因还包括数据中台建设本身是一个长周期的事，如果数据仅仅作用于一方，由业务方(前台)自行开发，更符合敏捷开发的特性。\n关于何时应该建立数据中台这个问题，我的思考是这样的。复杂的业务线、丰富的数据维度和公司上层领导主推。三者缺一，都没有实行的必要。 一只手都能数的过来的业务线量，跨多个项目的需求相对还是比较少的，取数也比较方便，直接走接口方式基本就能满足。反而，通过数据中台流转，将问题复杂化了。\n数据的维度越丰富，数据的价值越大。只知道性别数据，与知道性别和年龄，所得到的用户画像，肯定是维度丰富的准确性高。维度不丰富的情况下，没有计算的价值。\n可能会很奇怪为什么一定需要公司上层的同意。这里就可能涉及到动了谁的奶酪的问题，数据是每个业务线最重要的资源，在推行中台过程中，势必会遇到阻力，只有成为全公司的战略任务，才有可能把事情做好。\n中台如果没有考虑通用的业务能力，也会导致无法更专注于对中台技术的深入研究。中台如果不从抽象度、共性等角度出发，很有可能局限于某单一业务，导致中台无法很好地适应其他相关业务的要求，从而不能很好地应对业务的变化。如果中台的抽象程度低、扩展性差，则会导致中台无法满足前台业务需求。这时前台应用又因为业务本身的发展目标和压力不得不自行组织团队完成这部分功能，由此可能发生本应由中台提供的能力却最终实现在业务应用中，失去了中台存在的价值。\n","permalink":"http://121.199.2.5:6080/3OKmkr/","summary":"数据中台 阿里在 2018 年提出了所谓“数据中台”的概念：即数据被统一采集，规范数据语义和业务口径形成企业基础数据模型，提供统一的分析查询和新业务的数据对接能力。数据中台并不是新的颠覆式技术，而是一种企业数据资产管理和应用方法学，涵盖了数据集成、数据质量管理、元数据与主数据管理、数仓建模、支持高并发访问的数据服务接口层开发等内容。\n在数据中台建设中，结合企业自身的业务需求特点，架构和功能可能各不相同，但其中一个最基本的需求是数据采集的实时性和完整性。数据从源端产生，到被采集到数据汇集层的时间要尽可能短，至少应做到秒级延迟，这样中台的数据模型更新才可能做到近实时，构建在中台之上依赖实时数据流驱动的应用（例如商品推荐、欺诈检测等）才能够满足业务的需求。\n以阿里双十一为例，在极高的并发情况下，订单产生到大屏统计数据更新延迟不能超过 5s，一般在 2s 内。中台对外提供的数据应该是完整的，源端数据的 Create、Update 和 Delete 都要能够被捕获，不能少也不能多，即数据需要有端到端一致性的能力（Exactly Once Semantic，EOS）。当然，EOS 并非在任何业务场景下都需要，但从平台角度必须具备这种能力，并且允许用户根据业务需求灵活开启和关闭。\n数据中台的产生背景 起初，企业只有一个主营业务，比如电商，但随着公司战略和发展需要，会新增多支业务线，由于存在负责业务线开发的团队不一致，随之而来的就是风格迥异的代码风格和数据烟囱问题。\n数据中台的产生就是为了解决数据烟囱的问题，打通数据孤岛，让数据活起来，让数据产生价值，结合前台能力，达到快速响应用户的目标。\n中台只会同步能服务于超过两个业务线的数据，如果仅仅带有自身业务属性(不存在共性)的数据，不在中台的考虑范围内。例如:电商的产品产地信息，对于金融业务来说，其实是没有价值的，但电商的用户收货地址对金融业务来说是有价值的。所以不要简单的认为数据中台会汇集企业的所有数据，还是有侧重点的。导致这个结果的原因还包括数据中台建设本身是一个长周期的事，如果数据仅仅作用于一方，由业务方(前台)自行开发，更符合敏捷开发的特性。\n关于何时应该建立数据中台这个问题，我的思考是这样的。复杂的业务线、丰富的数据维度和公司上层领导主推。三者缺一，都没有实行的必要。 一只手都能数的过来的业务线量，跨多个项目的需求相对还是比较少的，取数也比较方便，直接走接口方式基本就能满足。反而，通过数据中台流转，将问题复杂化了。\n数据的维度越丰富，数据的价值越大。只知道性别数据，与知道性别和年龄，所得到的用户画像，肯定是维度丰富的准确性高。维度不丰富的情况下，没有计算的价值。\n可能会很奇怪为什么一定需要公司上层的同意。这里就可能涉及到动了谁的奶酪的问题，数据是每个业务线最重要的资源，在推行中台过程中，势必会遇到阻力，只有成为全公司的战略任务，才有可能把事情做好。\n中台如果没有考虑通用的业务能力，也会导致无法更专注于对中台技术的深入研究。中台如果不从抽象度、共性等角度出发，很有可能局限于某单一业务，导致中台无法很好地适应其他相关业务的要求，从而不能很好地应对业务的变化。如果中台的抽象程度低、扩展性差，则会导致中台无法满足前台业务需求。这时前台应用又因为业务本身的发展目标和压力不得不自行组织团队完成这部分功能，由此可能发生本应由中台提供的能力却最终实现在业务应用中，失去了中台存在的价值。","title":"数据中台"},{"content":"服务经常会重启失败，写了个简单的shell叫脚本，可以发现服务端口是否起来，没有发现监听端口会重启服务\n1.监控脚本 if [ -z \u0026#34;`lsof -i:443 | grep LISTEN`\u0026#34; ];then echo -e \u0026#34;restart $(date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)\u0026#34; systemctl restart pixiublog fi 2.系统定时任务 */3 * * * * /usr/local/pixiublog/monitor.sh \u0026gt;\u0026gt; /usr/local/pixiublog/monitor.log 2\u0026gt;\u0026amp;1 ","permalink":"http://121.199.2.5:6080/5XKHkb/","summary":"服务经常会重启失败，写了个简单的shell叫脚本，可以发现服务端口是否起来，没有发现监听端口会重启服务\n1.监控脚本 if [ -z \u0026#34;`lsof -i:443 | grep LISTEN`\u0026#34; ];then echo -e \u0026#34;restart $(date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)\u0026#34; systemctl restart pixiublog fi 2.系统定时任务 */3 * * * * /usr/local/pixiublog/monitor.sh \u0026gt;\u0026gt; /usr/local/pixiublog/monitor.log 2\u0026gt;\u0026amp;1 ","title":"简单脚本实现服务监控"},{"content":"haokiu 的启动脚本，配置 https 可以参考beego 通过acms.sh 使用 https\n#!/bin/sh siteDir=\u0026#39;/usr/local/pixiublog\u0026#39; appName=\u0026#39;pixiublogMain\u0026#39; echo \u0026#34;kill the running program\u0026#34; ps -ef | grep $appName | grep -v grep | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9 echo \u0026#34;sleep 3 secons for app to shutdown\u0026#34; sleep 3 echo \u0026#34;start program\u0026#34; cd $siteDir nohup $siteDir/$appName \u0026gt;\u0026gt; $siteDir/console.log 2\u0026gt;\u0026amp;1 \u0026amp; ","permalink":"http://121.199.2.5:6080/v3yn45/","summary":"haokiu 的启动脚本，配置 https 可以参考beego 通过acms.sh 使用 https\n#!/bin/sh siteDir=\u0026#39;/usr/local/pixiublog\u0026#39; appName=\u0026#39;pixiublogMain\u0026#39; echo \u0026#34;kill the running program\u0026#34; ps -ef | grep $appName | grep -v grep | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9 echo \u0026#34;sleep 3 secons for app to shutdown\u0026#34; sleep 3 echo \u0026#34;start program\u0026#34; cd $siteDir nohup $siteDir/$appName \u0026gt;\u0026gt; $siteDir/console.log 2\u0026gt;\u0026amp;1 \u0026amp; ","title":"haokiu网站启动脚本"},{"content":"现在 presto 有两个官网：\n旧presto官网 新presto官网 ","permalink":"http://121.199.2.5:6080/FhpG3z/","summary":"现在 presto 有两个官网：\n旧presto官网 新presto官网 ","title":"presto 官网"},{"content":"presto 有很多连接器以支持不同的数据源：\nAccumulo BigQuery Black Hole Cassandra Druid Elasticsearch Google Sheets Iceberg Hive JMX Kafka Kinesis Kudu Local File Memory MemSQL MongoDB MySQL Oracle Phoenix Pinot PostgreSQL Prometheus Redis Redshift SQL Server System Thrift TPCDS TPCH ","permalink":"http://121.199.2.5:6080/7mlwkW/","summary":"presto 有很多连接器以支持不同的数据源：\nAccumulo BigQuery Black Hole Cassandra Druid Elasticsearch Google Sheets Iceberg Hive JMX Kafka Kinesis Kudu Local File Memory MemSQL MongoDB MySQL Oracle Phoenix Pinot PostgreSQL Prometheus Redis Redshift SQL Server System Thrift TPCDS TPCH ","title":"presto 支持的数据源"},{"content":"idea 付费版和免费版的区别 IntelliJ IDEA Ultimate IntelliJ IDEA Community Edition Java, Kotlin, Groovy, Scala + + Android + + Maven, Gradle, sbt + + Git, SVN, Mercurial + + Debugger + + Profiling tools + - Spring, Java EE, Micronaut, Quarkus, Helidon, and more + - Swagger, Open API Specifications + - JavaScript, TypeScript + - Database Tools, SQL + - ","permalink":"http://121.199.2.5:6080/oqQib8/","summary":"idea 付费版和免费版的区别 IntelliJ IDEA Ultimate IntelliJ IDEA Community Edition Java, Kotlin, Groovy, Scala + + Android + + Maven, Gradle, sbt + + Git, SVN, Mercurial + + Debugger + + Profiling tools + - Spring, Java EE, Micronaut, Quarkus, Helidon, and more + - Swagger, Open API Specifications + - JavaScript, TypeScript + - Database Tools, SQL + - ","title":"idea 付费版和免费版的区别"},{"content":"okex 是一个老牌的数字货币交易所，我一直在用OKEx安全简便地交易数字货币。\n用我的邀请链接注册OKEx可以免费获得比特币奖励！点击这里 或者 使用这个地址： https://www.ouyi.fit/join/1841513\nokex 是全球著名的数字资产交易平台之一，主要面向全球用户提供比特币、莱特币、以太币等数字资产的币币和衍生品交易服务。\n","permalink":"http://121.199.2.5:6080/4Li9s1/","summary":"okex 是一个老牌的数字货币交易所，我一直在用OKEx安全简便地交易数字货币。\n用我的邀请链接注册OKEx可以免费获得比特币奖励！点击这里 或者 使用这个地址： https://www.ouyi.fit/join/1841513\nokex 是全球著名的数字资产交易平台之一，主要面向全球用户提供比特币、莱特币、以太币等数字资产的币币和衍生品交易服务。","title":"okex"},{"content":"SQL 优化已经成为衡量程序猿优秀与否的硬性指标，甚至在各大厂招聘岗位职能上都有明码标注。\n有朋友疑问到，SQL 优化真的有这么重要么？如下图所示，SQL 优化在提升系统性能中是：成本最低和优化效果最明显的途径。\n**优化成本：**硬件\u0026gt;系统配置\u0026gt;数据库表结构\u0026gt;SQL 及索引。\n**优化效果：**硬件\u0026lt;系统配置\u0026lt;数据库表结构\u0026lt;SQL 及索引。\n对于MySQL层优化我一般遵从五个原则：\n**减少数据访问：**设置合理的字段类型，启用压缩，通过索引访问等减少磁盘 IO。 **返回更少的数据：**只返回需要的字段和数据分页处理，减少磁盘 IO 及网络 IO。 **减少交互次数：**批量 DML 操作，函数存储等减少数据连接次数。 **减少服务器 CPU 开销：**尽量减少数据库排序操作以及全表查询，减少 CPU 内存占用。 **利用更多资源：**使用表分区，可以增加并行操作，更大限度利用 CPU 资源。 总结到 SQL 优化中，就如下三点：\n最大化利用索引。 尽可能避免全表扫描。 减少无效数据的查询。 理解 SQL 优化原理 ，首先要搞清楚 SQL 执行顺序。\nSELECT 语句，语法顺序如下：\n1. SELECT 2. DISTINCT \u0026lt;select_list\u0026gt; 3. FROM \u0026lt;left_table\u0026gt; 4. \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; 5. ON \u0026lt;join_condition\u0026gt; 6. WHERE \u0026lt;where_condition\u0026gt; 7. GROUP BY \u0026lt;group_by_list\u0026gt; 8. HAVING \u0026lt;having_condition\u0026gt; 9. ORDER BY \u0026lt;order_by_condition\u0026gt; 10.LIMIT \u0026lt;limit_number\u0026gt; SELECT 语句，执行顺序如下：\nFROM \u0026lt;表名\u0026gt; # 选取表，将多个表数据通过笛卡尔积变成一个表。 ON \u0026lt;筛选条件\u0026gt; # 对笛卡尔积的虚表进行筛选 JOIN \u0026lt;join, left join, right join...\u0026gt; \u0026lt;join表\u0026gt; # 指定join，用于添加数据到on之后的虚表中，例如left join会将左表的剩余数据添加到虚表中 WHERE \u0026lt;where条件\u0026gt; # 对上述虚表进行筛选 GROUP BY \u0026lt;分组条件\u0026gt; # 分组 \u0026lt;SUM()等聚合函数\u0026gt; # 用于having子句进行判断，在书写上这类聚合函数是写在having判断里面的 HAVING \u0026lt;分组筛选\u0026gt; # 对分组后的结果进行聚合筛选 SELECT \u0026lt;返回数据列表\u0026gt; # 返回的单列必须在group by子句中，聚合函数除外 DISTINCT # 数据除重 ORDER BY \u0026lt;排序条件\u0026gt; # 排序 LIMIT \u0026lt;行数限制\u0026gt; 以下 SQL 优化策略适用于数据量较大的场景下，如果数据量较小，没必要以此为准，以免画蛇添足。\n避免不走索引的场景\n①尽量避免在字段开头模糊查询，会导致数据库引擎放弃索引进行全表扫描\n如下：\nSELECT * FROM t WHERE username LIKE \u0026#39;%陈%\u0026#39; **优化方式：**尽量在字段后面使用模糊查询。\n如下：\nSELECT * FROM t WHERE username LIKE \u0026#39;陈%\u0026#39; 如果需求是要在前面使用模糊查询：\n使用 MySQL 内置函数 INSTR（str，substr）来匹配，作用类似于 Java 中的 indexOf()，查询字符串出现的角标位置。\n使用 FullText 全文索引，用 match against 检索。\n数据量较大的情况，建议引用 ElasticSearch、Solr，亿级数据量检索速度秒级。 当表数据量较少（几千条儿那种），别整花里胡哨的，直接用 like \u0026lsquo;%xx%\u0026rsquo;。\n②尽量避免使用 in 和 not in，会导致引擎走全表扫描\n如下：\nSELECT * FROM t WHERE id IN (2,3) **优化方式：**如果是连续数值，可以用 between 代替。\n如下：\nSELECT * FROM t WHERE id BETWEEN 2 AND 3 如果是子查询，可以用 exists 代替。\n如下：\n-- 不走索引 select * from A where A.id in (select id from B); -- 走索引 select * from A where exists (select * from B where B.id = A.id); ③尽量避免使用 or，会导致数据库引擎放弃索引进行全表扫描\n如下：\nSELECT * FROM t WHERE id = 1 OR id = 3 **优化方式：**可以用 union 代替 or。\n如下：\nSELECT * FROM t WHERE id = 1 UNION SELECT * FROM t WHERE id = 3 ④尽量避免进行 null 值的判断，会导致数据库引擎放弃索引进行全表扫描\n如下：\nSELECT * FROM t WHERE score IS NULL **优化方式：**可以给字段添加默认值 0，对 0 值进行判断。\n如下：\nSELECT * FROM t WHERE score = 0 ⑤尽量避免在 where 条件中等号的左侧进行表达式、函数操作，会导致数据库引擎放弃索引进行全表扫描\n可以将表达式、函数操作移动到等号右侧，如下：\n-- 全表扫描 SELECT * FROM T WHERE score/10 = 9 -- 走索引 SELECT * FROM T WHERE score = 10*9 ⑥当数据量大时，避免使用 where 1=1 的条件\n通常为了方便拼装查询条件，我们会默认使用该条件，数据库引擎会放弃索引进行全表扫描。\n如下：\nSELECT username, age, sex FROM T WHERE 1=1 **优化方式：**用代码拼装 SQL 时进行判断，没 where 条件就去掉 where，有 where 条件就加 and。\n⑦查询条件不能用 \u0026lt;\u0026gt; 或者 !=\n使用索引列作为条件进行查询时，需要避免使用\u0026lt;\u0026gt;或者!=等判断条件。\n如确实业务需要，使用到不等于符号，需要在重新评估索引建立，避免在此字段上建立索引，改由查询条件中其他索引字段代替。\n⑧where 条件仅包含复合索引非前置列\n如下：复合（联合）索引包含 key_part1，key_part2，key_part3 三列，但 SQL 语句没有包含索引前置列\u0026quot;key_part1\u0026quot;，按照 MySQL 联合索引的最左匹配原则，不会走联合索引。\nselect col1 from table where key_part2=1 and key_part3=2 ⑨隐式类型转换造成不使用索引\n如下 SQL 语句由于索引对列类型为 varchar，但给定的值为数值，涉及隐式类型转换，造成不能正确走索引。\nselect col1 from table where col_varchar=123; ⑩order by 条件要与 where 中条件一致，否则 order by 不会利用索引进行排序\n如下：\n-- 不走age索引 SELECT * FROM t order by age; -- 走age索引 SELECT * FROM t where age \u0026gt; 0 order by age; 对于上面的语句，数据库的处理顺序是：\n**第一步：**根据 where 条件和统计信息生成执行计划，得到数据。 **第二步：**将得到的数据排序。当执行处理数据（order by）时，数据库会先查看第一步的执行计划，看 order by 的字段是否在执行计划中利用了索引。如果是，则可以利用索引顺序而直接取得已经排好序的数据。如果不是，则重新进行排序操作。 **第三步：**返回排序后的数据。 当 order by 中的字段出现在 where 条件中时，才会利用索引而不再二次排序，更准确的说，order by 中的字段在执行计划中利用了索引时，不用排序操作。\n这个结论不仅对 order by 有效，对其他需要排序的操作也有效。比如 group by 、union 、distinct 等。\n⑪正确使用 hint 优化语句\nMySQL 中可以使用 hint 指定优化器在执行时选择或忽略特定的索引。\n一般而言，处于版本变更带来的表结构索引变化，更建议避免使用 hint，而是通过 Analyze table 多收集统计信息。\n但在特定场合下，指定 hint 可以排除其他索引干扰而指定更优的执行计划：\nUSE INDEX 在你查询语句中表名的后面，添加 USE INDEX 来提供希望 MySQL 去参考的索引列表，就可以让 MySQL 不再考虑其他可用的索引。\n例子: SELECT col1 FROM table USE INDEX (mod_time, name)\u0026hellip;\nIGNORE INDEX 如果只是单纯的想让 MySQL 忽略一个或者多个索引，可以使用 IGNORE INDEX 作为 Hint。\n例子: SELECT col1 FROM table IGNORE INDEX (priority) \u0026hellip;\nFORCE INDEX 为强制 MySQL 使用一个特定的索引，可在查询中使用FORCE INDEX 作为 Hint。\n例子: SELECT col1 FROM table FORCE INDEX (mod_time) \u0026hellip;\n在查询的时候，数据库系统会自动分析查询语句，并选择一个最合适的索引。但是很多时候，数据库系统的查询优化器并不一定总是能使用最优索引。\n如果我们知道如何选择索引，可以使用 FORCE INDEX 强制查询使用指定的索引。\n例如：\nSELECT * FROM students FORCE INDEX (idx_class_id) WHERE class_id = 1 ORDER BY id DESC; SELECT 语句其他优化\n①避免出现 select *\n首先，select * 操作在任何类型数据库中都不是一个好的 SQL 编写习惯。\n使用 select * 取出全部列，会让优化器无法完成索引覆盖扫描这类优化，会影响优化器对执行计划的选择，也会增加网络带宽消耗，更会带来额外的 I/O，内存和 CPU 消耗。\n建议提出业务实际需要的列数，将指定列名以取代 select *。具体详情见《为什么大家都说SELECT * 效率低》\n②避免出现不确定结果的函数\n特定针对主从复制这类业务场景。由于原理上从库复制的是主库执行的语句，使用如 now()、rand()、sysdate()、current_user() 等不确定结果的函数很容易导致主库与从库相应的数据不一致。\n另外不确定值的函数，产生的 SQL 语句无法利用 query cache。\n③多表关联查询时，小表在前，大表在后\n在 MySQL 中，执行 from 后的表关联查询是从左往右执行的（Oracle 相反），第一张表会涉及到全表扫描。\n所以将小表放在前面，先扫小表，扫描快效率较高，在扫描后面的大表，或许只扫描大表的前 100 行就符合返回条件并 return 了。\n例如：表 1 有 50 条数据，表 2 有 30 亿条数据；如果全表扫描表 2，你品，那就先去吃个饭再说吧是吧。\n④使用表的别名\n当在 SQL 语句中连接多个表时，请使用表的别名并把别名前缀于每个列名上。这样就可以减少解析的时间并减少哪些友列名歧义引起的语法错误。\n⑤用 where 字句替换 HAVING 字句\n避免使用 HAVING 字句，因为 HAVING 只会在检索出所有记录之后才对结果集进行过滤，而 where 则是在聚合前刷选记录，如果能通过 where 字句限制记录的数目，那就能减少这方面的开销。\nHAVING 中的条件一般用于聚合函数的过滤，除此之外，应该将条件写在 where 字句中。\nwhere 和 having 的区别：where 后面不能使用组函数。\n⑥调整 Where 字句中的连接顺序\nMySQL 采用从左往右，自上而下的顺序解析 where 子句。根据这个原理，应将过滤数据多的条件往前放，最快速度缩小结果集。\n增删改 DML 语句优化\n①大批量插入数据\n如果同时执行大量的插入，建议使用多个值的 INSERT 语句（方法二）。这比使用分开 INSERT 语句快（方法一），一般情况下批量插入效率有几倍的差别。\n方法一：\ninsert into T values(1,2); insert into T values(1,3); insert into T values(1,4); 方法二：\nInsert into T values(1,2),(1,3),(1,4); 选择后一种方法的原因有三：\n减少 SQL 语句解析的操作，MySQL 没有类似 Oracle 的 share pool，采用方法二，只需要解析一次就能进行数据的插入操作。 在特定场景可以减少对 DB 连接次数。 SQL 语句较短，可以减少网络传输的 IO。 ②适当使用 commit\n适当使用 commit 可以释放事务占用的资源而减少消耗，commit 后能释放的资源如下：\n事务占用的 undo 数据块。\n事务在 redo log 中记录的数据块。\n释放事务施加的，减少锁争用影响性能。特别是在需要使用 delete 删除大量数据的时候，必须分解删除量并定期 commit。 ③避免重复查询更新的数据\n针对业务中经常出现的更新行同时又希望获得改行信息的需求，MySQL 并不支持 PostgreSQL 那样的 UPDATE RETURNING 语法，在 MySQL 中可以通过变量实现。\n例如，更新一行记录的时间戳，同时希望查询当前记录中存放的时间戳是什么？\n简单方法实现：\nUpdate t1 set time=now() where col1=1; Select time from t1 where id =1; 使用变量，可以重写为以下方式：\nUpdate t1 set time=now () where col1=1 and @now: = now (); Select @now; 前后二者都需要两次网络来回，但使用变量避免了再次访问数据表，特别是当 t1 表数据量较大时，后者比前者快很多。\n④查询优先还是更新（insert、update、delete）优先\nMySQL 还允许改变语句调度的优先级，它可以使来自多个客户端的查询更好地协作，这样单个客户端就不会由于锁定而等待很长时间。改变优先级还可以确保特定类型的查询被处理得更快。\n我们首先应该确定应用的类型，判断应用是以查询为主还是以更新为主的，是确保查询效率还是确保更新的效率，决定是查询优先还是更新优先。\n下面我们提到的改变调度策略的方法主要是针对只存在表锁的存储引擎，比如 MyISAM 、MEMROY、MERGE，对于 Innodb 存储引擎，语句的执行是由获得行锁的顺序决定的。\nMySQL 的默认的调度策略可用总结如下：\n写入操作优先于读取操作。 对某张数据表的写入操作某一时刻只能发生一次，写入请求按照它们到达的次序来处理。 对某张数据表的多个读取操作可以同时地进行。 MySQL 提供了几个语句调节符，允许你修改它的调度策略：\nLOW_PRIORITY 关键字应用于 DELETE、INSERT、LOAD DATA、REPLACE 和 UPDATE。\nHIGH_PRIORITY 关键字应用于 SELECT 和 INSERT 语句。\nDELAYED 关键字应用于 INSERT 和 REPLACE 语句。 如果写入操作是一个 LOW_PRIORITY（低优先级）请求，那么系统就不会认为它的优先级高于读取操作。\n在这种情况下，如果写入者在等待的时候，第二个读取者到达了，那么就允许第二个读取者插到写入者之前。\n只有在没有其它的读取者的时候，才允许写入者开始操作。这种调度修改可能存在 LOW_PRIORITY 写入操作永远被阻塞的情况。\nSELECT 查询的 HIGH_PRIORITY（高优先级）关键字也类似。它允许 SELECT 插入正在等待的写入操作之前，即使在正常情况下写入操作的优先级更高。\n另外一种影响是，高优先级的 SELECT 在正常的 SELECT 语句之前执行，因为这些语句会被写入操作阻塞。\n如果希望所有支持 LOW_PRIORITY 选项的语句都默认地按照低优先级来处理，那么请使用\u0026ndash;low-priority-updates 选项来启动服务器。\n通过使用 INSERTHIGH_PRIORITY 来把 INSERT 语句提高到正常的写入优先级，可以消除该选项对单个 INSERT 语句的影响。\n查询条件优化\n①对于复杂的查询，可以使用中间临时表暂存数据\n②优化 group by 语句\n默认情况下，MySQL 会对 GROUP BY 分组的所有值进行排序，如 “GROUP BY col1，col2，\u0026hellip;.;” 查询的方法如同在查询中指定 “ORDER BY col1，col2，\u0026hellip;;” 。\n如果显式包括一个包含相同的列的 ORDER BY 子句，MySQL 可以毫不减速地对它进行优化，尽管仍然进行排序。\n因此，如果查询包括 GROUP BY 但你并不想对分组的值进行排序，你可以指定 ORDER BY NULL 禁止排序。\n例如：\nSELECT col1, col2, COUNT(*) FROM table GROUP BY col1, col2 ORDER BY NULL ; ③优化 join 语句\nMySQL 中可以通过子查询来使用 SELECT 语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。\n使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的 SQL 操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询可以被更有效率的连接（JOIN）..替代。\n例子：假设要将所有没有订单记录的用户取出来，可以用下面这个查询完成：\nSELECT col1 FROM customerinfo WHERE CustomerID NOT in (SELECT CustomerID FROM salesinfo ) 如果使用连接（JOIN）..来完成这个查询工作，速度将会有所提升。\n尤其是当 salesinfo 表中对 CustomerID 建有索引的话，性能将会更好，查询如下：\nSELECT col1 FROM customerinfo LEFT JOIN salesinfoON customerinfo.CustomerID=salesinfo.CustomerID WHERE salesinfo.CustomerID IS NULL 连接（JOIN）..之所以更有效率一些，是因为 MySQL 不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作。\n④优化 union 查询\nMySQL 通过创建并填充临时表的方式来执行 union 查询。除非确实要消除重复的行，否则建议使用 union all。\n原因在于如果没有 all 这个关键词，MySQL 会给临时表加上 distinct 选项，这会导致对整个临时表的数据做唯一性校验，这样做的消耗相当高。\n高效：\nSELECT COL1, COL2, COL3 FROM TABLE WHERE COL1 = 10 UNION ALL SELECT COL1, COL2, COL3 FROM TABLE WHERE COL3= \u0026#39;TEST\u0026#39;; 低效：\nSELECT COL1, COL2, COL3 FROM TABLE WHERE COL1 = 10 UNION SELECT COL1, COL2, COL3 FROM TABLE WHERE COL3= \u0026#39;TEST\u0026#39;; ⑤拆分复杂 SQL 为多个小 SQL，避免大事务\n如下：\n简单的 SQL 容易使用到 MySQL 的 QUERY CACHE。 减少锁表时间特别是使用 MyISAM 存储引擎的表。 可以使用多核 CPU。 ⑥使用 truncate 代替 delete\n当删除全表中记录时，使用 delete 语句的操作会被记录到 undo 块中，删除记录也记录 binlog。\n当确认需要删除全表时，会产生很大量的 binlog 并占用大量的 undo 数据块，此时既没有很好的效率也占用了大量的资源。\n使用 truncate 替代，不会记录可恢复的信息，数据不能被恢复。也因此使用 truncate 操作有其极少的资源占用与极快的时间。另外，使用 truncate 可以回收表的水位，使自增字段值归零。\n⑦使用合理的分页方式以提高分页效率\n使用合理的分页方式以提高分页效率 针对展现等分页需求，合适的分页方式能够提高分页的效率。\n案例 1：\nselect * from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15; 上述例子通过一次性根据过滤条件取出所有字段进行排序返回。数据访问开销=索引 IO+索引全部记录结果对应的表数据 IO。\n因此，该种写法越翻到后面执行效率越差，时间越长，尤其表数据量很大的时候。\n适用场景：当中间结果集很小（10000 行以下）或者查询条件复杂（指涉及多个不同查询字段或者多表连接）时适用。\n案例 2：\nselect t.* from (select id from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15) a, t where a.id = t.id; 上述例子必须满足 t 表主键是 id 列，且有覆盖索引 secondary key：（thread_id, deleted, gmt_create）。\n通过先根据过滤条件利用覆盖索引取出主键 id 进行排序，再进行 join 操作取出其他字段。\n数据访问开销=索引 IO+索引分页后结果（例子中是 15 行）对应的表数据 IO。因此，该写法每次翻页消耗的资源和时间都基本相同，就像翻第一页一样。\n**适用场景：**当查询和排序字段（即 where 子句和 order by 子句涉及的字段）有对应覆盖索引时，且中间结果集很大的情况时适用。\n建表优化\n①在表中建立索引，优先考虑 where、order by 使用到的字段。\n②尽量使用数字型字段（如性别，男：1 女：2），若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。\n这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。\n③查询数据量大的表 会造成查询缓慢。主要的原因是扫描行数过多。这个时候可以通过程序，分段分页进行查询，循环遍历，将结果合并处理进行展示。\n要查询 100000 到 100050 的数据，如下：\nSELECT * FROM (SELECT ROW_NUMBER() OVER(ORDER BY ID ASC) AS rowid,* FROM infoTab)t WHERE t.rowid \u0026gt; 100000 AND t.rowid \u0026lt;= 100050 ④用 varchar/nvarchar 代替 char/nchar。\n尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。\n不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL 也包含在内），都是占用 100 个字符的空间的，如果是 varchar 这样的变长字段， null 不占用空间\n","permalink":"http://121.199.2.5:6080/XEhpYv/","summary":"SQL 优化已经成为衡量程序猿优秀与否的硬性指标，甚至在各大厂招聘岗位职能上都有明码标注。\n有朋友疑问到，SQL 优化真的有这么重要么？如下图所示，SQL 优化在提升系统性能中是：成本最低和优化效果最明显的途径。\n**优化成本：**硬件\u0026gt;系统配置\u0026gt;数据库表结构\u0026gt;SQL 及索引。\n**优化效果：**硬件\u0026lt;系统配置\u0026lt;数据库表结构\u0026lt;SQL 及索引。\n对于MySQL层优化我一般遵从五个原则：\n**减少数据访问：**设置合理的字段类型，启用压缩，通过索引访问等减少磁盘 IO。 **返回更少的数据：**只返回需要的字段和数据分页处理，减少磁盘 IO 及网络 IO。 **减少交互次数：**批量 DML 操作，函数存储等减少数据连接次数。 **减少服务器 CPU 开销：**尽量减少数据库排序操作以及全表查询，减少 CPU 内存占用。 **利用更多资源：**使用表分区，可以增加并行操作，更大限度利用 CPU 资源。 总结到 SQL 优化中，就如下三点：\n最大化利用索引。 尽可能避免全表扫描。 减少无效数据的查询。 理解 SQL 优化原理 ，首先要搞清楚 SQL 执行顺序。\nSELECT 语句，语法顺序如下：\n1. SELECT 2. DISTINCT \u0026lt;select_list\u0026gt; 3. FROM \u0026lt;left_table\u0026gt; 4. \u0026lt;join_type\u0026gt; JOIN \u0026lt;right_table\u0026gt; 5. ON \u0026lt;join_condition\u0026gt; 6. WHERE \u0026lt;where_condition\u0026gt; 7. GROUP BY \u0026lt;group_by_list\u0026gt; 8. HAVING \u0026lt;having_condition\u0026gt; 9. ORDER BY \u0026lt;order_by_condition\u0026gt; 10.LIMIT \u0026lt;limit_number\u0026gt; SELECT 语句，执行顺序如下：\nFROM \u0026lt;表名\u0026gt; # 选取表，将多个表数据通过笛卡尔积变成一个表。 ON \u0026lt;筛选条件\u0026gt; # 对笛卡尔积的虚表进行筛选 JOIN \u0026lt;join, left join, right join...\u0026gt; \u0026lt;join表\u0026gt; # 指定join，用于添加数据到on之后的虚表中，例如left join会将左表的剩余数据添加到虚表中 WHERE \u0026lt;where条件\u0026gt; # 对上述虚表进行筛选 GROUP BY \u0026lt;分组条件\u0026gt; # 分组 \u0026lt;SUM()等聚合函数\u0026gt; # 用于having子句进行判断，在书写上这类聚合函数是写在having判断里面的 HAVING \u0026lt;分组筛选\u0026gt; # 对分组后的结果进行聚合筛选 SELECT \u0026lt;返回数据列表\u0026gt; # 返回的单列必须在group by子句中，聚合函数除外 DISTINCT # 数据除重 ORDER BY \u0026lt;排序条件\u0026gt; # 排序 LIMIT \u0026lt;行数限制\u0026gt; 以下 SQL 优化策略适用于数据量较大的场景下，如果数据量较小，没必要以此为准，以免画蛇添足。","title":"sql 优化"},{"content":"基本概念 Node 与 Cluster Elastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。\n单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。\nIndex Elastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。\n所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。\nDocument Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。Document 使用 JSON 格式表示。同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。\nType Document 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。\n不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。\n在6.0之前的版本，一个ElasticSearch索引中，可以有多个类型；从6.0版本开始，，一个ElasticSearch索引中，只有1个类型。一个类型是索引的一个逻辑上的分类，通常具有一组相同字段的文档组成。ElasticSearch的类型概念相当于关系数据库的数据表。\nshard 当数据量较大时，索引的存储空间需求超出单个节点磁盘容量的限制，或者出现单个节点处理速度较慢。为了解决这些问题，ElasticSearch将索引中的数据进行切分成多个分片（shard），每个分片存储这个索引的一部分数据，分布在不同节点上。当需要查询索引时，ElasticSearch将查询发送到每个相关分片，之后将查询结果合并，这个过程对ElasticSearch应用来说是透明的，用户感知不到分片的存在。 一个索引的分片一旦指定，不再修改。\n副本 其实，分片全称是主分片，简称为分片。主分片是相对于副本来说的，副本是对主分片的一个或多个复制版本（或称拷贝），这些复制版本（拷贝）可以称为复制分片，可以直接称之为副本。当主分片丢失时，集群可以将一个副本升级为新的主分片。\n对比 ElasticSearch RDBMS 索引（index） 数据库（database） 类型（type） 表（table） 文档（document） 行（row） 字段（field） 列（column） 映射（mapping） 表结构（schema） 全文索引 索引 查询DSL SQL GET select PUT/POST update DELETE delete 节点信息 查看节点信息 curl localhost:9200 查看节点健康度 curl localhost:9200/_cat/health?v\u0026amp;pretty=true 查看集群状况 curl localhost:9200/_cat/nodes?v\u0026amp;pretty=true index(索引)操作 查看当前节点的所有 Index。 curl -X GET \u0026#39;http://localhost:9200/_cat/indices?v\u0026#39; 可以列出每个 Index 所包含的 Type curl \u0026#39;localhost:9200/_mapping?pretty=true\u0026#39; 新建一个名叫weather的 Index。 curl -X PUT \u0026#39;localhost:9200/weather\u0026#39; 删除这个 Index。 curl -X DELETE \u0026#39;localhost:9200/weather\u0026#39; 文档操作 获取 GET /website/blog/123?pretty { \u0026#34;_index\u0026#34; : \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;blog\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;123\u0026#34;, \u0026#34;_version\u0026#34; : 1, \u0026#34;found\u0026#34; : true, \u0026#34;_source\u0026#34; : { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Just trying this out...\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2014/01/01\u0026#34; } } 检索文档的一部分\nGET /website/blog/123?_source=title,text { \u0026#34;_index\u0026#34; : \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34; : \u0026#34;blog\u0026#34;, \u0026#34;_id\u0026#34; : \u0026#34;123\u0026#34;, \u0026#34;_version\u0026#34; : 1, \u0026#34;exists\u0026#34; : true, \u0026#34;_source\u0026#34; : { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34; , \u0026#34;text\u0026#34;: \u0026#34;Just trying this out...\u0026#34; } } 只想得到_source字段而不要其他的元数据\nGET /website/blog/123/_source { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Just trying this out...\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2014/01/01\u0026#34; } 存在 如果你想做的只是检查文档是否存在——你对内容完全不感兴趣——使用HEAD方法来代替GET。HEAD请求不会返回响应体，只有HTTP头：\ncurl -i -XHEAD http://localhost:9200/website/blog/123 Elasticsearch将会返回200 OK状态如果你的文档存在：\nHTTP/1.1 200 OK Content-Type: text/plain; charset=UTF-8 Content-Length: 0 如果不存在返回404 Not Found：\nHTTP/1.1 404 Not Found Content-Type: text/plain; charset=UTF-8 Content-Length: 0 更新整个文档 PUT /website/blog/123 { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;I am starting to get the hang of this...\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2014/01/02\u0026#34; } 新建文档 PUT /website/blog/123 { ... } 删除文档 DELETE /website/blog/123 配置 elasticsearch的config文件夹里面有两个配置文件：elasticsearch.yml和log4j2.properties，第一个是es的基本配置文件，第二个是日志配置文件，es也是使用log4j2来记录日志的，所以log4j2.properties里的设置按普通log4j2配置文件来设置就行了。\ncluster.name Elasticsearch 默认启动的集群名字叫 elasticsearch 。 你最好给你的生产环境的集群改个名字，改名字的目的很简单， 就是防止某人的笔记本电脑加入了集群这种意外。简单修改成 elasticsearch_production 会很省心。\n你可以在你的 elasticsearch.yml 文件中修改：\ncluster.name: elasticsearch_production\nnode.name 建议给每个节点设置一个有意义的、清楚的、描述性的名字，同样你可以在 elasticsearch.yml 中配置：\nnode.name: elasticsearch_005_data\nMaster节点 （主节点） node.master: true node.data: false 这样配置的节点为master节点。主节点的主要职责是和集群操作相关的内容，如创建或删除索引，跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点。稳定的主节点对集群的健康是非常重要的。\ndiscovery.zen.minimum_master_nodes 为了防止数据丢失，配置discovery.zen.minimum_master_nodes设置是至关重要的（默认为1），每个主节点应该知道形成一个集群的最小数量的主资格节点的数量。\n假设我们有一个集群。有3个主资格节点，当网络发生故障的时候，有可能其中一个节点不能和其他节点进行通信了。这个时候，当discovery.zen.minimum_master_nodes设置为1的时候，就会分成两个小的独立集群，当网络好的时候，就会出现数据错误或者丢失数据的情况。当discovery.zen.minimum_master_nodes设置为2的时候，一个网络中有两个主资格节点，可以继续工作，另一部分，由于只有一个主资格节点，则不会形成一个独立的集群，这个时候当网络回复的时候，节点又会从新加入集群。\n设置这个值的原则是：（master_eligible_nodes / 2）+ 1\ndiscovery.zen.ping.unicast.hosts Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。\n虽然组播仍然作为插件提供， 但它应该永远不被使用在生产环境了，否则你得到的结果就是一个节点意外的加入到了你的生产环境，仅仅是因为他们收到了一个错误的组播信号。 对于组播本身并没有错，组播会导致一些愚蠢的问题，并且导致集群变的脆弱（比如，一个网络工程师正在捣鼓网络，而没有告诉你，你会发现所有的节点突然发现不了对方了）。\n使用单播，你可以为 Elasticsearch 提供一些它应该去尝试连接的节点列表。 当一个节点联系到单播列表中的成员时，它就会得到整个集群所有节点的状态，然后它会联系 master 节点，并加入集群。\n这意味着你的单播列表不需要包含你的集群中的所有节点， 它只是需要足够的节点，当一个新节点联系上其中一个并且说上话就可以了。如果你使用 master 候选节点作为单播列表，你只要列出三个就可以了。 这个配置在 elasticsearch.yml 文件中：\ndiscovery.zen.ping.unicast.hosts: [\u0026quot;host1\u0026quot;, \u0026quot;host2:port\u0026quot;]\n注：端口非9200的节点， ip后需加端口号， 因 es 默认识别端口是9200\nData节点（数据节点） node.master: false node.data: true 数据节点主要是存储索引数据的节点，主要对文档进行增删改查操作，聚合操作等。数据节点对cpu，内存，io要求较高，在优化的时候需要监控数据节点的状态，当资源不够的时候，需要在集群中添加新的节点。\nClient节点 (客户端节点) node.master: false node.data: false 当主节点和数据节点配置都设置为false的时候，该节点只能处理路由请求，处理搜索，分发索引操作等，从本质上来说该客户节点表现为智能负载平衡器。独立的客户端节点在一个比较大的集群中是非常有用的，他协调主节点和数据节点，客户端节点加入集群可以得到集群的状态，根据集群的状态可以直接路由请求。 警告：添加太多的客户端节点对集群是一种负担，因为主节点必须等待每一个节点集群状态的更新确认！客户节点的作用不应被夸大，数据节点也可以起到类似的作用。\n安全 search-guard是elastcisearch的一款插件，提供加密，身份验证和授权，基于search guard SSL，另外提供可插入的身份验证/授权模块，search-guard是shield的替代品，可免费提供所有的基本安全功能。\n参考 阮一蜂的博客\n","permalink":"http://121.199.2.5:6080/3XTrh1/","summary":"基本概念 Node 与 Cluster Elastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。\n单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。\nIndex Elastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。\n所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。\nDocument Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。Document 使用 JSON 格式表示。同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。\nType Document 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。\n不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。\n在6.0之前的版本，一个ElasticSearch索引中，可以有多个类型；从6.0版本开始，，一个ElasticSearch索引中，只有1个类型。一个类型是索引的一个逻辑上的分类，通常具有一组相同字段的文档组成。ElasticSearch的类型概念相当于关系数据库的数据表。\nshard 当数据量较大时，索引的存储空间需求超出单个节点磁盘容量的限制，或者出现单个节点处理速度较慢。为了解决这些问题，ElasticSearch将索引中的数据进行切分成多个分片（shard），每个分片存储这个索引的一部分数据，分布在不同节点上。当需要查询索引时，ElasticSearch将查询发送到每个相关分片，之后将查询结果合并，这个过程对ElasticSearch应用来说是透明的，用户感知不到分片的存在。 一个索引的分片一旦指定，不再修改。\n副本 其实，分片全称是主分片，简称为分片。主分片是相对于副本来说的，副本是对主分片的一个或多个复制版本（或称拷贝），这些复制版本（拷贝）可以称为复制分片，可以直接称之为副本。当主分片丢失时，集群可以将一个副本升级为新的主分片。\n对比 ElasticSearch RDBMS 索引（index） 数据库（database） 类型（type） 表（table） 文档（document） 行（row） 字段（field） 列（column） 映射（mapping） 表结构（schema） 全文索引 索引 查询DSL SQL GET select PUT/POST update DELETE delete 节点信息 查看节点信息 curl localhost:9200 查看节点健康度 curl localhost:9200/_cat/health?v\u0026amp;pretty=true 查看集群状况 curl localhost:9200/_cat/nodes?v\u0026amp;pretty=true index(索引)操作 查看当前节点的所有 Index。 curl -X GET \u0026#39;http://localhost:9200/_cat/indices?v\u0026#39; 可以列出每个 Index 所包含的 Type curl \u0026#39;localhost:9200/_mapping?pretty=true\u0026#39; 新建一个名叫weather的 Index。 curl -X PUT \u0026#39;localhost:9200/weather\u0026#39; 删除这个 Index。 curl -X DELETE \u0026#39;localhost:9200/weather\u0026#39; 文档操作 获取 GET /website/blog/123?","title":"elasticsearch"},{"content":"从今年 10 月 1 日起，GitHub 在该平台上创建的所有新的源代码仓库将默认被命名为 \u0026ldquo;main\u0026rdquo;，而不是原先的\u0026quot;master\u0026quot;。值得注意的是，现有的存储库不会受到此更改影响。\n早在今年 6 月份，受美国大规模的 “Black Lives Matter”运动影响，为了安抚愈演愈烈的民众情绪，GitHub 就宣布将替换掉 master 等术语，以避免联想奴隶制。现如今，在外界一些声音的催促下，这一举措则终于要正式落地了。\n除 GitHub 外，为了避免带有所谓的“种族歧视色彩”，许多科技巨头或知名软件也都调整了自己的业务和产品，以平息社会舆论。包括有：MySQL 宣布删除 master、黑名单白名单等术语；Linus Torvalds 通过了 Linux 中避免 master/slave 等术语的提案；还有 Twitter 、GitHub、微软、LinkedIn、Ansible、Splunk、OpenZFS、OpenSSL、JP Morgan、 Android 移动操作系统、Go 编程语言、PHPUnit 和 Curl 等宣布要对此类术语进行删除或更改。同时，IBM、亚马逊、微软也都接连调整面部识别平台业务，以防加深歧视或遭受指责。\n且最初在 Git 中写下“master”一词的开发者 Petr Baudis 也于 6 月份在社交网站上表明立场称，自己当年不该使用“master”这个可能给别人造成伤害的词语。并表示，他曾多次希望可以将“master”改成“main”(和“upstream”）。不过直到现在，才由 GitHub 开始主导替换工作。\n而对于为何选择“main”而不是其他替换词汇，Github 方面给出的解释为，main 是他们在平台上看到的最受欢迎的 master 替代品。并且 main 这个词汇很短，可以帮助用户形成良好的肌肉记忆；在很多种语言中翻译起来也都很容易。\n此外，Github 还透露，截至今年年底，他们将使现有存储库无缝重命名其默认分支。当用户重命名分支机构时，他们将重新定位打开的 PR 和草稿版本、移动分支机构保护策略等，且所有的这些都将自动完成。\n事实上，计算机术语政治正确性早已不是新鲜话题。2004 年，“master/slave”曾被全球语言检测机构评为年度最不政治正确的十大词汇之一，时任主席称这是政治渗透到计算机技术控制中的表现。2008 年，开源软件 Drupal 在社区发布消息，高调站队，将“master/slave”重命名为“client/server”。2018年，IETF 也在草案当中指出，要求开源软件更改“master/slave”和“blacklist/whitelist”两项表述。\n但是值得思考的是，在计算机源码领域中，“master/slave”和“blacklist/whitelist”之类的技术用语有错吗？一味的“一刀切”的话，会不会导致所谓的矫枉过正呢\n","permalink":"http://121.199.2.5:6080/LcW9jL/","summary":"从今年 10 月 1 日起，GitHub 在该平台上创建的所有新的源代码仓库将默认被命名为 \u0026ldquo;main\u0026rdquo;，而不是原先的\u0026quot;master\u0026quot;。值得注意的是，现有的存储库不会受到此更改影响。\n早在今年 6 月份，受美国大规模的 “Black Lives Matter”运动影响，为了安抚愈演愈烈的民众情绪，GitHub 就宣布将替换掉 master 等术语，以避免联想奴隶制。现如今，在外界一些声音的催促下，这一举措则终于要正式落地了。\n除 GitHub 外，为了避免带有所谓的“种族歧视色彩”，许多科技巨头或知名软件也都调整了自己的业务和产品，以平息社会舆论。包括有：MySQL 宣布删除 master、黑名单白名单等术语；Linus Torvalds 通过了 Linux 中避免 master/slave 等术语的提案；还有 Twitter 、GitHub、微软、LinkedIn、Ansible、Splunk、OpenZFS、OpenSSL、JP Morgan、 Android 移动操作系统、Go 编程语言、PHPUnit 和 Curl 等宣布要对此类术语进行删除或更改。同时，IBM、亚马逊、微软也都接连调整面部识别平台业务，以防加深歧视或遭受指责。\n且最初在 Git 中写下“master”一词的开发者 Petr Baudis 也于 6 月份在社交网站上表明立场称，自己当年不该使用“master”这个可能给别人造成伤害的词语。并表示，他曾多次希望可以将“master”改成“main”(和“upstream”）。不过直到现在，才由 GitHub 开始主导替换工作。\n而对于为何选择“main”而不是其他替换词汇，Github 方面给出的解释为，main 是他们在平台上看到的最受欢迎的 master 替代品。并且 main 这个词汇很短，可以帮助用户形成良好的肌肉记忆；在很多种语言中翻译起来也都很容易。\n此外，Github 还透露，截至今年年底，他们将使现有存储库无缝重命名其默认分支。当用户重命名分支机构时，他们将重新定位打开的 PR 和草稿版本、移动分支机构保护策略等，且所有的这些都将自动完成。\n事实上，计算机术语政治正确性早已不是新鲜话题。2004 年，“master/slave”曾被全球语言检测机构评为年度最不政治正确的十大词汇之一，时任主席称这是政治渗透到计算机技术控制中的表现。2008 年，开源软件 Drupal 在社区发布消息，高调站队，将“master/slave”重命名为“client/server”。2018年，IETF 也在草案当中指出，要求开源软件更改“master/slave”和“blacklist/whitelist”两项表述。\n但是值得思考的是，在计算机源码领域中，“master/slave”和“blacklist/whitelist”之类的技术用语有错吗？一味的“一刀切”的话，会不会导致所谓的矫枉过正呢","title":"github 的 master 分支将更改为 main"},{"content":"ECMAScript 6.0（以下简称 ES6）是 JavaScript 语言的下一代标准，已经在 2015 年 6 月正式发布了。它的目标，是使得 JavaScript 语言可以用来编写复杂的大型应用程序，成为企业级开发语言。\nECMAScript 和 JavaScript 的关系 一个常见的问题是，ECMAScript 和 JavaScript 到底是什么关系？\n要讲清楚这个问题，需要回顾历史。1996 年 11 月，JavaScript 的创造者 Netscape 公司，决定将 JavaScript 提交给标准化组织 ECMA，希望这种语言能够成为国际标准。次年，ECMA 发布 262 号标准文件（ECMA-262）的第一版，规定了浏览器脚本语言的标准，并将这种语言称为 ECMAScript，这个版本就是 1.0 版。\n该标准从一开始就是针对 JavaScript 语言制定的，但是之所以不叫 JavaScript，有两个原因。一是商标，Java 是 Sun 公司的商标，根据授权协议，只有 Netscape 公司可以合法地使用 JavaScript 这个名字，且 JavaScript 本身也已经被 Netscape 公司注册为商标。二是想体现这门语言的制定者是 ECMA，不是 Netscape，这样有利于保证这门语言的开放性和中立性。\n因此，ECMAScript 和 JavaScript 的关系是，前者是后者的规格，后者是前者的一种实现（另外的 ECMAScript 方言还有 JScript 和 ActionScript）。日常场合，这两个词是可以互换的。\nBabel 转码器 Babel 是一个广泛使用的 ES6 转码器，可以将 ES6 代码转为 ES5 代码，从而在现有环境执行。这意味着，你可以用 ES6 的方式编写程序，又不用担心现有环境是否支持。下面是一个例子。\n// 转码前 input.map(item =\u0026gt; item + 1); // 转码后 input.map(function (item) { return item + 1; }); 上面的原始代码用了箭头函数，Babel 将其转为普通函数，就能在不支持箭头函数的 JavaScript 环境执行了。\n","permalink":"http://121.199.2.5:6080/Cqhoxc/","summary":"ECMAScript 6.0（以下简称 ES6）是 JavaScript 语言的下一代标准，已经在 2015 年 6 月正式发布了。它的目标，是使得 JavaScript 语言可以用来编写复杂的大型应用程序，成为企业级开发语言。\nECMAScript 和 JavaScript 的关系 一个常见的问题是，ECMAScript 和 JavaScript 到底是什么关系？\n要讲清楚这个问题，需要回顾历史。1996 年 11 月，JavaScript 的创造者 Netscape 公司，决定将 JavaScript 提交给标准化组织 ECMA，希望这种语言能够成为国际标准。次年，ECMA 发布 262 号标准文件（ECMA-262）的第一版，规定了浏览器脚本语言的标准，并将这种语言称为 ECMAScript，这个版本就是 1.0 版。\n该标准从一开始就是针对 JavaScript 语言制定的，但是之所以不叫 JavaScript，有两个原因。一是商标，Java 是 Sun 公司的商标，根据授权协议，只有 Netscape 公司可以合法地使用 JavaScript 这个名字，且 JavaScript 本身也已经被 Netscape 公司注册为商标。二是想体现这门语言的制定者是 ECMA，不是 Netscape，这样有利于保证这门语言的开放性和中立性。\n因此，ECMAScript 和 JavaScript 的关系是，前者是后者的规格，后者是前者的一种实现（另外的 ECMAScript 方言还有 JScript 和 ActionScript）。日常场合，这两个词是可以互换的。\nBabel 转码器 Babel 是一个广泛使用的 ES6 转码器，可以将 ES6 代码转为 ES5 代码，从而在现有环境执行。这意味着，你可以用 ES6 的方式编写程序，又不用担心现有环境是否支持。下面是一个例子。\n// 转码前 input.map(item =\u0026gt; item + 1); // 转码后 input.map(function (item) { return item + 1; }); 上面的原始代码用了箭头函数，Babel 将其转为普通函数，就能在不支持箭头函数的 JavaScript 环境执行了。","title":"es6 简介"},{"content":"七牛云存储 提供了每月10G的免费存储，一般的网站已经够用，如果超额了，费用也很低，可以考虑将网站的图片存在七牛，安全可靠。\n","permalink":"http://121.199.2.5:6080/0K1gzu/","summary":"七牛云存储 提供了每月10G的免费存储，一般的网站已经够用，如果超额了，费用也很低，可以考虑将网站的图片存在七牛，安全可靠。","title":"免费的云存储"},{"content":"网站的部署脚本\n#!/bin/sh #部署目录 siteDir=\u0026#39;/usr/local/pixiublog\u0026#39; # 源代码目录 cd /root/pixiublog echo \u0026#34;update code\u0026#34; git pull echo \u0026#34;build pixiublog\u0026#34; go build main.go echo \u0026#34;remove old\u0026#34; rm -rf $siteDir/views rm -rf $siteDir/static rm -rf $siteDir/pixiublog echo \u0026#34;mv new program to $siteDir\u0026#34; cp main $siteDir/pixiublog cp -rf views $siteDir/ cp -rf static $siteDir/ echo \u0026#34;kill the running program\u0026#34; ps -ef | grep \u0026#39;pixiublog\u0026#39; | grep -v grep | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9 echo \u0026#34;start program\u0026#34; cd $siteDir nohup $siteDir/pixiublog \u0026gt;\u0026gt; $siteDir/console.log 2\u0026gt;\u0026amp;1 \u0026amp; ","permalink":"http://121.199.2.5:6080/1gTd1h/","summary":"网站的部署脚本\n#!/bin/sh #部署目录 siteDir=\u0026#39;/usr/local/pixiublog\u0026#39; # 源代码目录 cd /root/pixiublog echo \u0026#34;update code\u0026#34; git pull echo \u0026#34;build pixiublog\u0026#34; go build main.go echo \u0026#34;remove old\u0026#34; rm -rf $siteDir/views rm -rf $siteDir/static rm -rf $siteDir/pixiublog echo \u0026#34;mv new program to $siteDir\u0026#34; cp main $siteDir/pixiublog cp -rf views $siteDir/ cp -rf static $siteDir/ echo \u0026#34;kill the running program\u0026#34; ps -ef | grep \u0026#39;pixiublog\u0026#39; | grep -v grep | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9 echo \u0026#34;start program\u0026#34; cd $siteDir nohup $siteDir/pixiublog \u0026gt;\u0026gt; $siteDir/console.log 2\u0026gt;\u0026amp;1 \u0026amp; ","title":"网站部署脚本"},{"content":"katacoda is Interactive Learning and Training Platform for Software Engineers Helping Developers Learn and Companies Increase Adoption\n可以交互式的学习各种前沿技术：k8s linux docker 机器学习等\n","permalink":"http://121.199.2.5:6080/baIB8x/","summary":"katacoda is Interactive Learning and Training Platform for Software Engineers Helping Developers Learn and Companies Increase Adoption\n可以交互式的学习各种前沿技术：k8s linux docker 机器学习等","title":"在线交互式学习k8s/docker/linux"},{"content":"vscode 列选择 快捷键: shift + alt + 鼠标左键\n","permalink":"http://121.199.2.5:6080/QCpYff/","summary":"vscode 列选择 快捷键: shift + alt + 鼠标左键","title":"vscode 列选择 快捷键"},{"content":"frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。本文简单的介绍frp的配置使用。\n实现内网穿需要有一台公网服务器。本文将公网服务器称为服务端，内网服务器称为客户端。需要开启相关的端口。相关端口没开通，访问就会失败。\nfrp git地址:https://github.com/fatedier/frp 中文文档：https://github.com/fatedier/frp/blob/master/README_zh.md frp下载地址：https://github.com/fatedier/frp/releases 本文使用软件：frp_0.21.0_linux_amd64.tar.gz 本文使用系统：centos7（公网一台，内网一台）\n本文使用软件：frp_0.21.0_linux_amd64.tar.gz，frp的客户端和服务端都在同一个包里。\n文件说明 frps.ini: 服务端配置文件 frps: 服务端软件 frpc.ini: 客户端配置文件 frpc: 客户端软件\nfrps.ini配置 [common] bind_port = 7000 # auth token token = Qwert123 dashboard_port = 7500 # dashboard 用户名密码，默认都为 admin dashboard_user = admin dashboard_pwd = Qwert123 vhost_http_port = 7083 开启服务端服务 ./frps -c ./frps.ini\n可以用脚本来启动：\n#!/bin/sh nohup /usr/local/frp/frps -c /usr/local/frp/frps.ini \u0026amp; frpc.ini配置 [common] #服务器ip地址 server_addr = 121.199.2.XXX server_port = 7000 #开放api，提供reload服务 admin_addr = 127.0.0.1 admin_port = 7400 # auth token token = Qwert123 [ssh] type = tcp local_ip = 127.0.0.1 #ssh端口 local_port = 22 remote_port = 1022 可以使用脚本来启动(start.sh)\n#!/bin/sh sudo nohup /usr/local/frp/frpc -c /usr/local/frp/frpc.ini \u0026amp; 如果更改配置文件，可以使用重新加载来更新配置(reload.sh)\n#!/bin/sh /usr/local/frp/frpc reload -c /usr/local/frp/frpc.ini \u0026amp; 配置完后就可以通过 ssh 121.199.2.XXX 1022 来连接到内网服务器。\ncentos7开机启动 /usr/lib/systemd/system/frp.service\n[Unit] Description=the frp service After=syslog.target network.target [Service] Type=forking ExecStart=/usr/local/frp/start.sh ExecReload=/bin/kill -USR2 $MAINPID ExecStop=/bin/kill -SIGINT $MAINPID [Install] WantedBy=multi-user.target 执行：\nsystemctl enable frp systemctl start frp 转自：http://www.seaxiang.com/blog/frp\n","permalink":"http://121.199.2.5:6080/HYawYY/","summary":"frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。本文简单的介绍frp的配置使用。\n实现内网穿需要有一台公网服务器。本文将公网服务器称为服务端，内网服务器称为客户端。需要开启相关的端口。相关端口没开通，访问就会失败。\nfrp git地址:https://github.com/fatedier/frp 中文文档：https://github.com/fatedier/frp/blob/master/README_zh.md frp下载地址：https://github.com/fatedier/frp/releases 本文使用软件：frp_0.21.0_linux_amd64.tar.gz 本文使用系统：centos7（公网一台，内网一台）\n本文使用软件：frp_0.21.0_linux_amd64.tar.gz，frp的客户端和服务端都在同一个包里。\n文件说明 frps.ini: 服务端配置文件 frps: 服务端软件 frpc.ini: 客户端配置文件 frpc: 客户端软件\nfrps.ini配置 [common] bind_port = 7000 # auth token token = Qwert123 dashboard_port = 7500 # dashboard 用户名密码，默认都为 admin dashboard_user = admin dashboard_pwd = Qwert123 vhost_http_port = 7083 开启服务端服务 ./frps -c ./frps.ini\n可以用脚本来启动：\n#!/bin/sh nohup /usr/local/frp/frps -c /usr/local/frp/frps.ini \u0026amp; frpc.ini配置 [common] #服务器ip地址 server_addr = 121.199.2.XXX server_port = 7000 #开放api，提供reload服务 admin_addr = 127.0.0.1 admin_port = 7400 # auth token token = Qwert123 [ssh] type = tcp local_ip = 127.0.0.1 #ssh端口 local_port = 22 remote_port = 1022 可以使用脚本来启动(start.sh)\n#!/bin/sh sudo nohup /usr/local/frp/frpc -c /usr/local/frp/frpc.","title":"frp 使用"}]