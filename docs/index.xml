<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>haokiu</title>
    <link>http://121.199.2.5:6080/</link>
    <description>Recent content on haokiu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>haokiu.com</copyright>
    <lastBuildDate>Sat, 24 Jun 2023 15:03:53 +0000</lastBuildDate><atom:link href="http://121.199.2.5:6080/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1</title>
      <link>http://121.199.2.5:6080/1/</link>
      <pubDate>Thu, 07 Dec 2023 16:00:47 +0800</pubDate>
      
      <guid>http://121.199.2.5:6080/1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>数字货币交易所-gateio</title>
      <link>http://121.199.2.5:6080/gateio/</link>
      <pubDate>Sat, 24 Jun 2023 15:03:53 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/gateio/</guid>
      <description>Gate.io芝麻开门创立于2013年，是全球真实交易量TOP10的数字资产交易平台。
作为全球首家提供100%保证金审计证明的交易所，Gate.io在全球多个国家进行合法注册，向全球数千万用户提供安全可靠、真实透明的数字资产交易服务。
使用下面的链接注册，可以获得平台的奖励
https://www.gate.io/ref/XgBMB1A/haokiu?ref_type?=102</description>
    </item>
    
    <item>
      <title>enkrypt invite code FC433A</title>
      <link>http://121.199.2.5:6080/wgJnpu/</link>
      <pubDate>Sun, 21 May 2023 09:05:24 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/wgJnpu/</guid>
      <description>The enkrypt invite code is FC433A
Mint tickets with Enkrypt every day, invite friends to earn more tickets.
More tickets — more chances to win!
Share your code with friends and get +1 ticket for every friend’s mint.
Code FC433A
address：https://raffle.enkrypt.com</description>
    </item>
    
    <item>
      <title>googo 邀请码 lbVOQjeS</title>
      <link>http://121.199.2.5:6080/googo/</link>
      <pubDate>Sat, 18 Mar 2023 14:42:36 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/googo/</guid>
      <description>googo 可以让你更好的浏览外文网站，比如meta、youtube，google等。 它之前的网址是googo.in，现在已经不用访问。是一款非常好用的科学上网工具，使用简单。
使用这个链接可以获取优惠：https://cn.googo.us/#/register?code=lbVOQjeS
或者使用这个邀请码：lbVOQjeS</description>
    </item>
    
    <item>
      <title>Hobert读书笔记</title>
      <link>http://121.199.2.5:6080/bk-5/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/bk-5/</guid>
      <description>第一章 并发编程的挑战第2章 InnoDB存储引擎第1章 Java的I/O演进之路第1章 Spring框架的由来第2章 Tomcat总体架构三、Paxos的工程实践序第1章 概述第6章 深入分析ClassLoader工作机制第8章 虚拟机字节码执行系统</description>
    </item>
    
    <item>
      <title>三、Paxos的工程实践</title>
      <link>http://121.199.2.5:6080/3426588a7834499f8a9a1b7e46817c0c/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/3426588a7834499f8a9a1b7e46817c0c/</guid>
      <description>2.2.3 Paxos算法详解 假设有一组可以提出提案的进程集合，那么对于一个一致性算法来说需要保证以下几点：
在这些被提出的提案中，只有一个会被选定。
如果没有填被提出，那么就不会有被选定的提案。
当一个填被选定后，进程应该可以获取被选定的提案信息。
对于一致性来说，安全性（Safety）需求如下：
只有被提出的提案才能被选定。 只有一个值被选定。 如果某么进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个。 提案的选定
大多数。
推导过程
P1：一个Accptor必须批准它收到的第一个提案
P1a：一个Acceptor只要尚未响应过任何编号大于Mn的Prepare请求，那么它就可以接收这个编号为Mn的提案。（优化：可以忽略已批准过的提案的Prepare请求）
P2：如果编号为M0，Value值为V0的提案（即[M0,V0]）被选定了，那么所有比编号M0更高的，且被选定的提案，其Value值必须为V0。
P2a：如果编号为M0，Value值为V0的提案（即[M0,V0]）被选定了，那么所有比编号M0更高的，且被Acceptor批准的提案，其Value值必须为V0。
P2b：如果一个提案[M0,V0]被选定后，那么之后任何Proposer产生的编号更高的提案，其Value值都为V0。
P2c：对于任意的Mn和Vn，如果提案[Mn,Vn]被提出，那么肯定存在一个由半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个：
S中不存在任何批准过编号小于Mn的提案的Acceptor。 选取S中所有Acceptor批准的编号小于Mn的提案，其中编号最大的那个提案其Value值是Vn。 推导过程为第二数学归纳法。略
Proposer生成提案
对于Proposer，获取被通过的提案比预测可能会被通过的提案简单。
Proposer选择一个新的提案编号Mn，然后向某个Acceptor集合的成员发送请求，要求该集合中的Acceptor做出如下回应。 像Proposer承诺，保证不再批准任何编号小于Mn的提案。 如果Acceptor已经批准过任何提案，那么其就向Proposer反馈当前该Acceptor已经批准的编号小于Mn但为最大编号的那个提案的值。 如果Proposer收到了来自半数以上的Acceptor的响应结果，那么它就可以产生编号为Mn、Value值为Vn的提案，这里的Vn是所有响应中编号最大的提案Value值。当然还存在一种情况，就是半数以上的Acceptor都没有批准过任何提案，即响应中不包含任何的提案，那么此时Vn值就可以由Proposer任意选择。 Acceptor批准提案
一个Acceptor可能会收到来自Proposer的两种请求，分别是Prepare请求和Accept请求，分别相应如下：
Prepare请求：Acceptor可以在任何时候响应一个Prepare请求。 Accept请求：在不违背Accept现有承诺的前提下，可以任意响应Accept请求。 算法陈述
阶段一：
Proposer发送提案编号Mn； Acceptor根据约束接收提案，如果接收过返回接收最大值Vn； 阶段二：
如果Proposer收到大多数A的响应，发送[Mn,Vn]； Acceptor根据约束接收提案； 提案的获取
通知全部Learner
选取主Learner
将主Learner改为Learner集合
通过选取主Proposer保证算法的活性
三、Paxos的工程实践 3.1 Chubby 一个分布式锁服务。解决分布式协作，元数据存储，Master选举等一系列与分布式锁服务相关的问题。
底层为Paxos算法。
3.1.1 概述 </description>
    </item>
    
    <item>
      <title>序</title>
      <link>http://121.199.2.5:6080/cffcc67d768246fd8624b3e5b98854f5/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/cffcc67d768246fd8624b3e5b98854f5/</guid>
      <description>序 大、快、多样性只是表象，大数据的真正价值在于生命性和生态性。（活数据）
第1 章 总述 如果不能对数据进行有序、有结构地分类组织和存储，如果不能有效利用并发掘它，继而产生价值，那么它同时也成为一场“灾难”。无需、无结构的数据犹如堆积如山的垃圾，给企业带来的是有令人咋舌的高额成本。
要求：
如何建设高效的数据模型和体系，是数据易用，避免重复建设和数据不一致性； 如何提供高效易用的数据开发工具； 如何做好数据质量保障； 如何有效管理和控制日益增长的存储和计算消耗； 如何保证数据服务的稳定，保证其性能； 如何设计有效的数据产品高效赋能于外部客户和内部员工 数据采集层 日志采集体系方案包括两大体系：Aplus.JS是Web端日志采集技术方案；UserTrack是App端日记采集技术方案。
在采集技术基础上面向各个场景的埋点规范。
在传输方面采用TimeTunel（TT）,它既包括数据库的增量数据传输，也包括日志数据的传输；既支持实时流式计算，也知乎此各种时间窗口的批量计算。
也通过数据同步工具（DataX和同步中心，其中同步中心是给予DataX易用性封装的）直连异构数据库（备库）来抽取各种时间窗口的数据。
数据计算层 数据只有被整合和计算,才能被用于洞察商业规律,挖掘潜在信息，从而实现大数据价值,达到赋能于商业和创造价值的目的。
阿里巴巴的数据计算层包括两大体系:数据存储及计算云平台（离线计算平台MaxCompute和实时计算平台StreamCompute）和数据整合及管理体系（内部称之为“ OneData ”） 。
MaxCompute是阿里巴巴自主研发的离线大数据平台。
StreamCompute是阿里巴巴自主研发的流式大数据平台。
OneData是数据整合及管理的方法体系和工具。
借助此体系，构建了数据公共层。
从数据计算频率角度来看，阿里数据仓库可以分为离线数据仓库（传统的数据仓库概念）和实时数据仓库（典型应用：双11实时数据）。
阿里数据仓库的数据加工链路也是遵循业界的分层理念，包括：
操作数据层（Operational Data Store，ODS）; 明细数据层（Data WareHouse Detail，DWD）； 应用数据层（Application Data Store，ADS）。 通过数据仓库不同层次之间的加工过程实现从数据资产向信息资产的转化，并且对整个过程进行有效的元数据管理及数据质量处理。
元数据模型整合及应用是一个重要的组成部分，主要包含：
数据源元数据 数据仓库元数据 数据链路元数据 工具类元数据 数据质量类元数据 元数据应用主要面向数据发现、数据管理等，如用于存储、计算和成本管理。
数据服务层 当数据已被整合和计算好之后，需要提供给产品和应用进行数据消费。
针对不同的需求，数据服务层的数据源架构在多种数据库之上，如Mysql和HBase。
数据服务层主要考虑性能、稳定性、扩展性。
OneService（数据服务平台）一数据仓库整合计算好的数据作为数据源，对外通过接口的方式提供数据服务，主要提供简单数据查询服务、复杂数据查询服务（用户识别、用户画像等）和实时数据推送服务。
数据应用层 第1篇 数据技术篇 第2章 日志采集 第2篇 数据模型篇 第8章 大数据领域建模综述 8.1 为什么需要数据建模 如何将数据进行有序、有结构地分类组织和存储？
数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用角度合理存储数据。有了适合业务和基础数据存储环境的模型，那么大数据就能获得以下好处：
性能：良好的数据模型能帮助我们快速查询所需要的数据，减少数据的I/O吞吐。 成本：良好的数据模型能极大地减少不必要的数据冗余，也能实现计算结果复用，极大地降低大数据系统中的存储和计算成本。 效率：良好的数据模型能极大地改善用户使用数据的体验，提高使用数据的效率。 质量：良好的数据模型能改善数据统计口径的不一致性，减少数据计算错误的可能性。 8.2 关系行数据库系统和数据仓库 大数据领域仍然使用关系型数据库，使用关系理论描述数据之间的关系，只是基于其数据存储的特点关系数据模型的范式上有了不同的选择。
8.3 从OLTP和OLAP系统特别看模型方法论的选择 OLTP系统通常面向的主要数据操作是随即读写，主要采用满足3NF的实体关系模型存储数据，从而在事务处理中解决数据的冗余和一致性问题；而OLAP系统面向的主要数据操作时批量读写，事物处理中的一致性不是OLAP所关注的，其主要关注数据的集合，以及在一次性的复杂大数据查询和处理中的性能，因此它需要采用一些不同的数据建模方法。
8.4 典型的数据仓库建模方法论 8.4.1 ER模型 数据仓库中的3NF和OLPT系统中的3NF的却别在于，它是站在企业角度面向主题的抽象，而不是针对某个具体业务流程的实体对象关系的抽象。其具有以下几个特点：
需要全面了解企业业务和数据； 事实周期非常长； 对建模人员的能力要求非常高。 采用ER模型建设数据仓库魔性的出发点是整合数据将个系统中的数据以整个企业角度按主题进项相似性组合和合并，并进行一致性处理，为数据分析决策服务，但是并不能直接用于分析决策。
其建模步骤分为三个阶段：
高层模型：一个高度抽象的模型，描述主要的主题以及主题间的关系，用语描述企业的业务总体概况。 中层模型：在高层模型的基础上，细化主题的数据项。 物理模型（也叫底层模型）：在中层模型的基础上，考虑物理存储，同时基于性能和平台特点进行物理属性的设计，也可能作一些表的合并、分区的设计等。 实践典型：金融业务FS-LDM。
8.4.2 维度模型 是数据仓库工程领域最流行的数据仓库建模的经典。
维度建模从分析决策的需求出发构建模型，为分析需求服务，因此它重点关注用户如何更快速地完成需求分析，同时具有较好的大规模复杂查询的响应性能。其典型的代表是星形模型，以及在一些特殊场景下使用的雪花模型。其设计分为一下几个步骤：
选择需要进行分析决策的业务过程。业务过程可以是单个业务事件，比如交易的支付、退款等；也可以时某个时间的状态，比如当前的账户余额等；还可以是一系列相关业务时间组成的业务流程，具体需要看我们分析的是某些事件发生情况，还是当前状态，或是事件流转效率； 选择粒度。在事件分析中，我们要预判所有分析需要细分的程度，从而决定选择的粒度。粒度是维度的一个组合； 识别维表。选择好粒度之后，就需要细雨此粒度设计维表，包括维度属性，用于分析时进行分组和筛选； 选择事实。确定分析需要衡量的指标。 8.4.3 Data Vault模型 ER模型的衍生。其设计的出发点也是为了实现数据的整合，但不能直接用于数据分析决策。</description>
    </item>
    
    <item>
      <title>第1章 Java的I/O演进之路</title>
      <link>http://121.199.2.5:6080/e15796699f1a419ba85731d159eba322/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/e15796699f1a419ba85731d159eba322/</guid>
      <description>第1章 Java的I/O演进之路 1.1 I/O基础入门 1.1.1 Linux网络I/O模型简介 UNIX提供了5种I/O模型：
（1）阻塞I/O模型
（2）非阻塞I/O模型
（3）I/O复用模型
（4）信号驱动I/O模型
（5）异步I/O
1.1.2 I/O多路复用技术 把多个I/O阻塞复用到同一个select的阻塞上，从而是的系统在单线程的情况下可以同时处理多个客户端请求。比多线程有性能优势，节约资源。
支持I/O多路复用的系统调用select/pselect/poll/epoll。
epoll的优点：
支持一个进程打开的socket描述符（FD）不受限制（仅受限于操作系统的最大文件句柄数（内存））。 I/O效率不会随着FD数目的增加而线性下降。 使用mmap加速内核与用户空间的消息传递。 epoll的API更加简单。 1.2 Java的I/O演进 历史题，略。
第2章 NIO入门 2.1 传统的BIO编程 C/S模型，客户端发起连接请求，三次握手，通过Socket进行通信。
2.1.1 BIO通信模型图 一请求一应答模型：每次接收到连接请求都创建一个新的线程进行链路处理。处理完成后通过输出流返回应答给客户端，线程销毁。
该模型最大的问题就是：缺乏弹性伸缩能力，当客户端并发访问量增加后，服务端的线程个数和客户端并发访问数呈1：1的正比关系。
2.1.2 同步阻塞式I/O创建的TimeServer源码分析 import java.io.IOException; import java.net.ServerSocket; import java.net.Socket; /** * &amp;lt;p&amp;gt;Description: 同步阻塞式I/O创建的TimeServer&amp;lt;/p&amp;gt; * * @author 李宏博 * @version xxx * @create 2019/8/14 17:58 */ public class TimeServer { /** * * @param args */ public static void main(String[] args) throws IOException { int port = 8080; if (args != null &amp;amp;&amp;amp; args.length &amp;gt; 0) { try { port = Integer.valueOf(args[0]); } catch (NumberFormatException e) { e.printStackTrace(); } } ServerSocket server = null; try { server = new ServerSocket(port); System.</description>
    </item>
    
    <item>
      <title>第1章 Spring框架的由来</title>
      <link>http://121.199.2.5:6080/567c6d054f1d4d53a29e9d24f213165a/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/567c6d054f1d4d53a29e9d24f213165a/</guid>
      <description>第1章 Spring框架的由来 1.1 Spring之崛起 1.2 Spring框架概述 基于POJO（Plain Old Java Object，简单Java对象）的轻量级开发理念。
Spring总体架构：
1.3 Spring大观园 1.4 小结 第2章 Spring的IOC容器 2.1 我们的理念是：让别人为你服务 2.2 手语，呼喊，还是心有灵犀 2.2.1 构造方法注入 IoC Service Provider会检查被注入对象的构造方法，取得它所需要的依赖对象列表，进而为其注 入相应的对象。同一个对象是不可能被构造两次的，因此，被注入对象的构造乃至其整个生命周期， 应该是由IoC Service Provider来管理的。
2.2.2 setter方法注入 setter方法注入虽不像构造方法注入那样，让对象构造完成后即可使用，但相对来说更宽松一些， 可以在对象构造完成后再注入。
2.2.3 接口注入 对于前两种注入方式来说，接口注入没有那么简单明了。被注入对象如果想要IoC Service Provider为其注入依赖对象，就必须实现某个接口。这个接口提供一个方法，用来为其注入依赖对象。 IoC Service Provider最终通过这些接口来了解应该为被注入对象注入什么依赖对象。
示例：
2.2.4 三种注入方式的比较 接口注入。不提倡，带有侵入性 构造方法注入。这种注入方式的优点就是，对象在构造完成之后，即已进入就绪状态，可以马上使用。缺点就是，当依赖对象比较多的时候，构造方法的参数列表会比较长。而通过反射构造对象的时候，对相同类型的参数的处理会比较困难，维护和使用上也比较麻烦。而且在Java中，构造方法无法被继承，无法设置默认值。对于非必须的依赖处理，可能需要引入多个构造方法，而参数数量的变动可能造成维护上的不便。 setter方法注入。因为方法可以命名， 所以setter方法注入在描述性上要比构造方法注入好一些。另外， setter方法可以被继承，允许设置默认值，而且有良好的IDE支持。缺点当然就是对象无法在构造完成后马上进入就绪状态。 2.3 IOC的附加值 </description>
    </item>
    
    <item>
      <title>第1章 概述</title>
      <link>http://121.199.2.5:6080/b221a7f05b774f7ab4305cff0f244d52/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/b221a7f05b774f7ab4305cff0f244d52/</guid>
      <description>第1章 概述 互联网公司的分布式两个特点：规模大、成本低。
1.1 分布式存储概念 定义：分布式存储系统是大量普通PC服务器通过Internet互联，对外作为一个整体提供存储服务。
特性：
可扩展。集群，性能随规模线性增长。 低成本。有自动容错，自动负载均衡机制。所以可以构建在普通PC机器。 高性能。 易用。提供易用的对外接口。 主要挑战：数据、状态信息的持久化，要求在自动迁移、自动容错、并发读写的过程中保证数据的一致性。
主要技术来自于分布式系统和数据库：
数据分布：如何保证均匀？如何实现跨服务器读写？ 一致性：如何复制？如何保证出现异常也一致？ 容错：如何检测？出错如何迁移？ 负载均衡：如何实现？如何做到迁移时不影响其他业务？ 事务与并发控制：如何实现分布式事务？如何实现MVCC？ 易用性：如何易用？如何方便运维？ 压缩/解压缩：如何根据数据特点设计压缩/解压缩算法？如何平衡节省的存储空间和消耗的CPU资源浪费？ 1.2 分布式存储分类 数据大致分为三类：
非结构化数据：包括所有格式的办公文档、文本、图片、图像、音频和视频信息等。 结构化数据：一般存储在关系数据库中，可用二位关系表结构来表示。 半结构化数据：与结构化的区别在于，半结构化数据的模式结构和内容混在一起，没有明显的区分，也不需要预先定义数据的模式结构。 存储系统分为四类：
分布式文件系统 图片、照片、视频等非结构化数据对象，以对象组织，对象之间没有关联，称为Blob（Binary Large Object，二进制大对象）数据。
总体上看：分布式文件系统存储三种类型的数据：Blob对象、定长块以及大文件。在系统层面，分布式文件系统内部按照数据块来组织数据，每个数据块的大小大致相同，每个数据块可以包含多个Blob对象或者定长块，一个大文件也可以拆分成多个数据块。
分布式键值系统 用于存储关系简单的半结构化数据，它只提供基于主键的CRUD功能，即根据主键创建、读取、更新或者删除一条键值记录。
与传统哈希表相似，但是支持将数据分布到多个存储节点。
分布式键值系统是分布式表格系统的一种简化实现，一般用作缓存。
分布式表格系统 用于存储较为复杂的半结构化数据。不仅仅支持简单的CRUD操作，而且支持扫面某个主键范围。
借鉴了许多关系型数据库的技术，例如支持某种程度上的事务，比如单行事务，某个实体组下的多行事务。
与分布式数据库先比，仅支持针对单张表格的操作，不支持复杂操作。
分布式数据库 一般从单机数据库扩展而来，用于存储结构化数据。
第2章 单机存储系统 单机存储引擎就是哈希表、B树等数据结构在机械磁盘、SSD等持久化介质上的实现。
2.1 硬件基础 2.1.1 CPU架构 经典的多CPU架构为对称多处理结构（Symmetric Multi-Processing，SMP），即在一个计算机上汇集了一组处理器，它们之间对称工作，无主次或从属关系，共享相同的物理内存及总线。
2.1.2 IO总线 存储系统的性能瓶颈一般在与IO。
2.1.3 网络拓扑 传统的数据中心网络拓扑，分三层。最下面是接入层，中间是汇聚层，上面是汇聚层。存在的问题：大量下层接入，导致同一个接入层下的服务器之间的带宽减小。
2.1.4 性能参数 2.1.5 存储层次架构 存储系统的性能主要包括两个维度：吞吐量以及访问延时。
2.2 单机存储引擎 2.2.1 哈希存储引擎 Bitcask是一个基于哈希表结构的键值寸尺系统，它仅支持追加操作（Append-only），即所有的写操作只追加而不修改老的数据。
在Bitcask系统中，每个文件有一定的大小限制，当文件增加到相应的大小时，就会产生一个新的文件，老的文件只读不写。在任意时刻，只有一个文件市可写的，用于数据追加，称为活跃数据文件。而其他已经达到大小限制的文件，称为老数据文件。
数据结构 一条一条写入，每条记录的数据项分别为：主键（key），value内容（value），主键长度（key_sz），value长度（value_sz），时间戳（timetamp）以及crc校验值。（删除不会删除旧的条目，而是将value设定为一个特殊的之作标识）。内存中的结构是hash表。
定期合并 为解决垃圾文件问题。将所有老数据文件中的数据扫描一遍生成一个新的数据文件。对用一个key的多个操作以保留最新的一个原则进行删除。
快速恢复 每次合并时，将内存中的哈希索引表转储到磁盘中，生成一个索引文件。这个索引文件只存储value的位置。
2.2.2 B树存储引擎 不仅支持随机读取，还支持范围扫描。
数据结构 InnoDB按照页面（Page）来组织数据，每个页面对用B+树的一个节点。
B+树的根节点是常驻内存的。修改操作首先需要记录提交日志，接着修改内存中的B+树。
缓冲区管理 LRU、LIRS
2.2.3 LSM树存储引擎 将对数据的修改增量保持在内存中，达到指定的大小先之后将这些修改操作批量写入磁盘，读取时需要合并磁盘中的历史数据和内存中最近的修改操作。</description>
    </item>
    
    <item>
      <title>第2章 InnoDB存储引擎</title>
      <link>http://121.199.2.5:6080/232f08ac38d0478dab889420f809c20e/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/232f08ac38d0478dab889420f809c20e/</guid>
      <description>第2章 InnoDB存储引擎 事务安全。
2.1 InnoDB存储引擎概述 Mysql5.5开始是默认的表存储引擎（之前尽在Window下是）。第一个完整支持ACID事务的MySQL存储引擎。其特点是行锁设计、支持MVCC、支持外键、提供一致性非锁定读，同时被设计用来最有效地利用以及使用内存和CUP。
高性能、高可用、高扩展。
2.2 InnoDB存储引擎的版本 略
2.3 InnoDB体系结构 后台线程的主要作用是负责刷新内存池中的数据，保证缓冲池中的内存缓存的是最近的数据。
此外将已修改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下InnoDB能恢复到正常运行状态。
2.3.1 后台线程 多线程模型。不同的后台线程处理不同的任务。
Master Thread 将缓冲池的数据异步刷新到磁盘，保证数据一致性，包括脏页的刷新、合并插入缓冲（INSERT BUFFER）、UNDO页的回收。
IO Thread 大量使用AIO（Async IO），极大提高性能。IO Thread主要负责这些IO请求的回调处理。
第3章 文件 参数文件：告诉MySQL启动时查找文件的地址，指定初始化参数； 日志文件：错误日志文件、二进制文件日志文件、慢查询日志文件、查询日志文件； socket：当用UNIX域套接字方式进行连接时需要的文件 pid文件：MySQL实例的进程ID文件 MySQL表结构文件：用来存放MySQL表结构定义文件 存储引擎文件：每个存储引擎都会有自己的文件来保存各种数据，这些存储引擎真正存储了记录和索引等数据。 3.1 参数文件 MySQL实例也可以不需要参数文件，这是编译MySQL时指定的默认值。但是如果在默认的数据库目录下找不到mysql架构，则会启动失败。
3.1.1 什么是参数 数据库中的参数是键值对。MySQL中无类似Oracle中的隐藏参数。
3.1.2 参数类型 动态参数：运行时可修改 静态参数：在整个实例的生命周期都不能修改。 3.2 日志文件 日志文件记录了MySQL数据库的各种类型活动。
3.2.1 错误日志 对MySQL的启动、运行、关闭过程进行了记录。当出现MySQL数据库不能正常启动时，第一个必须查找的文件应该就是错误日志文件。
3.2.2 慢查询日志 帮助DBA定位可能存在问题的SOL语句，从而进行SQL语句层面的优化。设置一个阈值（最小精度是微秒），超过（必须是大于，等于不行）时将该SQL语句记录到慢查询日志中。
默认情况下并不启动慢查询日志。
如果没有使用索引也会被记录。
3.2.3 查询日志 记录了所有对MySQL数据库请求的信息，无论这些请求是否得到了正确的执行。
3.2.4 二进制文件 记录了对MySQL数据库执行更改的所有操作，但是不包括SELEC和SHOW这类操作。
作用：
恢复：某些数据的恢复需要二进制日志。 复制：主从复制 审计：通过审计二进制文件，判断是否有数据库进行注入的攻击。 二进制日志文件在默认情况下并没有启动，需要手动指定参数来启动。会影响性能，但仅仅1%。
缓冲写宕机问题，无缓冲写宕机问题
3.3 套接字文件 3.4 pid文件 3.5 表结构定义文件 记录该存储引擎对应的表结构
3.6 InnoDB存储引擎文件 InnoDB独有的文件
3.6.1 表空间文件 InnoDB采用将存储的数据按表空间进行存放的设计。
还可设置独立表空间，用户不用将所有数据都存放于默认的表空间中。
单独的表空间仅存储表的数据、索引和插入缓冲BITMAP等信息，其余的信息存放到默认表空间。
3.6.2 重做日志文件 宕机问题。
重做日志文件与二进制文件的区别：
记录范围，二进制日志会记录所有与MySQL数据库有关的日志记录，而InnoDB仅记录该存储引擎的事务日志。 记录内容，二进制日志记录具体操作（逻辑日志），而InnoDB的重做日志文件是关于每个Page的更改的物理情况 写入时间，二进制日志文件只在事务提交前进行一次写入，而重做日志文件在事务的进行过程中，一直会被写入。 3.7 小结 重做日志文件使得InnoDB存储引擎可以提供可靠的事务。
第4章 表 4.1 索引组织表 在InnoDB存储引擎中，表都是根据主键顺序组织存放。
在InnoDB存储引擎中，每张表都有个主键，如果没有显示定义，则按如下方式创建：
首先判断表中是否含有唯一非空索引（Unique NOT NULL），如果有，则该列即为主键。（按定义索引的顺序选择） 如果不符合上述条件，InnoDB存储引擎自动创建一个6字节大小的指针。 _rowid仅适用于单个列为主键的情况。</description>
    </item>
    
    <item>
      <title>第2章 Tomcat总体架构</title>
      <link>http://121.199.2.5:6080/a9dcb2a7a21d49dea965eedc7e169c79/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/a9dcb2a7a21d49dea965eedc7e169c79/</guid>
      <description>第2章 Tomcat总体架构 系统设计及中间件设计时的参考：生命周期管理、可扩展的容器组件设计、类加载方式。
2.1 总体设计 如何设计一个应用服务器？
2.1.1 Server 最基本的功能：接收请求，业务处理，返回响应。
两个方法：
start()：启动服务器，打开Socket链接，监听端口，负责接收请求，处理及返回。 stop()：停止服务器并释放网络资源。 作为嵌入在应用系统中的远程请求处理方案，且访问量低时可行。但作为应用服务器不可行。
2.1.2 Connector和Container 请求监听与请求处理放到一起扩展性差。
Connector负责监听，返回。
Container负责处理请求。
均分别拥有自己的start()和stop()方法来加载和释放自己维护的资源。
明显的缺陷：如何让Connector与Container对应？可以维护一个复杂关系映射，但是并不必需。Container设计足够灵活。
引入Service，负责维护多个Connector和一个Container。
在Tomcat中，Container是一个更加通用的概念。为了与Tomcat中的组件命名一致，所以重新命名为Engine，用以表示整个Servlet引擎。
Engine表示整个Servlet引擎。Server表示整个Servlet容器。
2.1.3 Container设计 应用服务器是用来部署并运行Web应用的，是一个运行环境，而不是独立的业务处理系统。因此需要在Engine容器中支持管理Web应用，当接收到Connector的处理请求时，Engine容器能够找到一个合适的Web应用来处理。
使用一个Context来表示一个Web应用，并且一个Engine可以包含多个Context。
虚拟主机，加入Host。一个Host可以包含多个Context。
Tomcat的设计中Engine可以包含Host也可以包含Context，这是由具体的Engine实现确定的。Tomcat提供的默认实现StandardEngine只能包含Host。
一个Web应用可以包含多个Servlet实例。在Tomcat中，Servlet定义被称为Wrapper。
“容器”的作用都是处理请求并返回响应数据。所以引入一个Container接口：addchild()添加子容器，backgroundProcess()实现文件变更的扫描。
2.1.4 Lifecycle 所有组件均存在启动、停止这两个生命周期方法，可在此基础上扩展生命周期管理的方法，即对于生命周期管理进行一次接口抽象。
将Server接口替换为Lifecycle接口：
Init()：初始化组件 start()：启动组件 stop()：停止组件 destory()：销毁组件 addLifecycleListener：添加事件监听器（用于监听组件的状态变化） removeLifecycleListener：删除 Tomcat核心组件的默认实现均继承自LifecycleBeanBase抽象类，该类不但负责组件各个状态的转换和事件处理，还将组件自身注册为MBean，以便通过Tomcat的管理工具进行动态维护。
2.1.5 Pipeline和Valve 以上设计以保证核心架构的了可伸缩性和可扩展性。但是还要考虑各个组件的灵活性，使其同样可扩展。
责任链模式是一种比较好的选择。Tomcat即采用该模式来实现客户端请求的处理。在Tomcat中每个Container组件通过执行一个责任链来完成具体的请求处理。
Pipeline（管道）用于构造责任链，Valve（阀）代表责任链上的每个处理器。Pipeline中维护了一个基础的Valve（位于末端，最后执行）。
Tomcat的每个层级的容器（Engine、Host、Context、Wrapper）均有对应的基础Valve实现，同时维护一个Pipeline实例。即任何层级的容器都可以对请求处理进行可扩展。
2.1.6 Connector设计 基本功能：
监听服务器端口，读取来自客户端的请求。 将请求数据按照指定协议进行解析。 根据请求地址匹配正确的容器进行处理。 将响应返回客户端。 Tomcat支持多协议，默认支持HTTP和AJP。同时支持多种I/O方式，包括BIO（8.5之后移除）、NIO、APR。而且在Tomcat8之后新增了对NIO2和HTTP/2协议的支持。因此对协议和I/O进行抽象和建模时需要关注的重点。
在Tomcat中，ProtocolHandler表示一个协议处理器，其包含一个Endpoint（无此接口，仅有AbstractEndpoint抽象类）用于启动Socket监听，还包含一个Processor用于按照指定协议读取数据，并将请求交由容器处理。
在Connector启动时，Endpoint会启动线程来监听，并在接收到请求后调用Processor进行数据读取。
当Processor读取客户端请求后，需要按照请求地址映射到具体的容器进行处理，这个过程即为请求映射。由于Tomcat各个组件采用通用的生命周期管理，而且可以通过管理工具进行状态变更，因此请求映射除考虑映射规则的实现外，还要考虑容器组件的注册与销毁。
Tomcat通过Mapper和MapperListener两个类实现上述功能。前者用于维护容器映射信息，同时按照映射规则（Servlet规范）查找容器。后者实现了ContainerListener和LifecycleListener，用于在容器组件状态发生变更时，注册或者取消对应的容器映射信息。为了实现上述功能，MapperListener实现了Lifecycle接口，当其启动时（在Service启动时启动），会自动作为监听器注册到各个容器组件上，同时将已创建的容器注册到Mapper。
Tomcat通过适配器模式（Adapter）实现了Connector与Mapper、Container的解耦。实现自己的Adapter可以脱离Servlet容器又使用Tomcat链接器。
2.1.7 Excutor 并发问题的解决方案。采用线程池（默认采用JDK5的线程池，继承自Lifecycle，当作通用组件进行管理）对线程进行统一管理。
在Tomcat中Excutor由Service维护，因此同一个Service中的组件可以共享一个线程池。
如果没有定义任何线程池，相关组件（Endpoint）会自动创建线程池，此时线程池不再共享。
在Tomcat中，Endpoint会启动一组线程来监听Socket端口，当接收到客户端请求后，会创建请求处理对象，并交由线程池处理，由此支持并发处理客户端请求。
2.1.8 Bootstrap和Catalina 除开前面的核心组件外，还需要提供一套配置环境来支持系统的可配置性，便于通过修改配置来优化应用。
集群、安全等组件同样重要，但不属于通用概念。
Tomcat通过类Catalina提供了一个Shell程序，用于解析server.xml创建各种组件，同时，负责启动、停止应用服务器（只需要启动Tomcat顶层组件Server）。
Tomcat使用Digester解析XML文件，包括server.xml以及web.xml等。
最后，Tomcat提供了Bootstrap作为应用服务器启动入口。Bootstrap负责创建Catalina实例，根据执行参数调用Catalina相关方法完成针对应用服务器的操作（启动、停止）。
Bootstrap与Tomcat应用服务器完全松耦合（通过反射调用Catalina实例），它可以直接依赖JRE运行并为Tomcat应用服务器创建共享类加载器，用于构造Catalina实例以及整个Tomcat服务器。
上述是Tomcat标准的启动方式。但是Server及其子组件代表了应用服务器本身，那么我们可以不通过Bootstrap和Catalina来启动服务器。
Tomcat组件说明：
组件名称 说明 Server 表示整个Servlet容器，因此Tomcat运行环境中只有唯一一个Server实例 Service Service表示一个或者多个Connector的集合，这些Connector共享同一个Container来处理其请求。在同一个Tomcat实例内可以包含任意多个Service实例，它们彼此独立 Connector 即Tomcat链接器，用于监听并转化Socket请求，同时将读取的Socket请求交由Container处理，支持不同协议以及不同的I/O方式 Container Container表示能够执行客户端请求并返回响应的一类对象。在Tomcat中存在不同级别的容器：Engine、Host、Context、Warpper Engine Engine表示整个Servlet引擎。在Tomcat中，Engine为最高层级的容器对象。尽管Engine不是直接处理请求的容器，确实获取目标容器的入口 Host Hostz作为一类容器，表示Servlet容器中的虚拟机，与一个服务器的网络名有关，如域名等。客户端可以使用这个网络名连接服务器，这个名称必须在DNS服务器上注册。 Context Context作为一类容器，用于表示ServletContext，在Servlet规范中，一个ServletContext即表示一个独立的Web应用 Wrapper Wrapper作为一类容器，用于表示Web应用中动议的Servlet Executor 表示Tomcat组件可以共享的线程池 2.2 Tomcat启动 Tomcat默认实现在相关概念的基础上结合生命周期管理监听器完成了大量的启动工作。</description>
    </item>
    
    <item>
      <title>第6章 深入分析ClassLoader工作机制</title>
      <link>http://121.199.2.5:6080/eec901917e884002a43f917c02f47dd9/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/eec901917e884002a43f917c02f47dd9/</guid>
      <description>第6章 深入分析ClassLoader工作机制 三个作用：
将Class加载到JVM中； 审查每个类应该由谁加载（双亲委派）； 将CLass字节码重新解析成JVM统一要求的格式。 6.1 ClassLoader类结构解析 defineClass方法：将byte字节流解析成JVM能够识别的Class对象。意味着不仅仅可以通过class文件实例化对象。调用此方法生成的Class对象还没有resolve，resolve会在对象真正实例化时才进行。
findClass方法：通过直接覆盖ClassLoader父类的findClass方法来实现类的加载规则，从而取得要加载的字节码。然后调用defineClass方法生成类的Class对象。如果想在类被加载到JVM的时候就被链接（Link），那么可以接着调用另外一个resolveClass方法。
如果想实现自己的ClassLader，一般都会继承URLClassLoader。
6.2 ClassLoader的等级加载机制 双亲委派。
（1）Bootstrap ClassLoader，主要加载JVM自身工作需要的类，完全由JVM自己控制。（既没有更高一级的父加载器，也没有子加载器）。
（2）ExtClassLoader，并不是JVM亲自实现，加载System.getProperty(“java.ext.dirs”)目录下的类。
（3）AppClassLoader，父类是ExtClassLoader。加载System.getProperty(“java.class.path”)目录下的类都可以被其加载。
实现自己的类加载器，都必须最终调用getSystemClassLoader()作为父加载器。而此方法获取到的就是AppClassLoader。
注意Bootstrap ClassLoader并不是如其他文章所说，而是其并无子类也无父类。ExtClassLoader并没有父类加载器。
ExtClassLoader和AppClassLoader都继承了URLClassloader类，而URLClassLoader又实现了抽象类ClassLoader，在创建Launcher对象时会首先创建ExtClassLoader，然后讲ExtClassLoader对象作为父加载器创建AppClassLoader对象。所以如果在Java应用中没有定义其他的ClassLoader，那么除了System.getProperty(“java.ext.dirs”)目录下的类是由ExtClassloader加载，其他类都是由AppClassLoader加载。
加载class文件到内存的两种方式：隐式，显式。
6.3 如何加载class文件 加载、验证、准备、解析、初始化。
第13章 Spring框架的设计理念与设计模式分析 13.1 Spring的骨骼架构 三个核心组件：Core、Context和Bean。
13.1.1 Spring的设计理念 最核心：Bean。（面向Bean编程）
解决了一个关键问题：把对象之间的依赖关系转而用配置文件来管理（依赖注入）。
Spring通过把对象包装在Bean中，从而达到管理这些对象及做一系列额外操作的目的。
这种设计策略完全类似于OOP的设计理念。构建一个数据结构，然后根据这个数据结构设计它的生存环境，并让它在这个环境中按照一定的规律不停地运动，在它们地不停运动中设计一个系列于环境或者与其他各地完成信息交换。（同时也是大多数框架地设计理念）
13.1.12 核心组件如何协同工作 Context负责发现每个Bean之间的关系，建立关系并且维护关系。所以Context就是一个Bean关系的集合，也叫Ioc容器。
Core就是发现、建立和维护每个Bean之间的关系所需要的一系列工具。（也就是一些Util）
13.2 核心组件详解 13.2.1 Bean组件 包：org.springframework.beans。这个包下的类主要解决三件事：Bean的定义、Bean的创建及对Bean的解析。（使用者只需关心创建）
Spring是典型的工厂模式，工厂的继承层次关系图如下：
顶级接口BeanFactory有3个子接口：ListableBeanFactory、HierarchicalBeanFantory和AutowireCapableBeanFactory。
DefaultListableBeanFactory实现了所有的结构。
定义多接口的原因：每个接口有不同的使用场景，主要是为了区分Spring内部对象的传递和转化过程中，对对象的数据访问所作的限制。例如，ListableBeanFactory接口标识这些Bean是可列表的，HierarchicalBeanFactory表示这些Bean是有继承关系的，AutowireCapableBeanFactory接口定义Bean的自动装配规则。4个接口共同定义了Bean的集合、Bean之间的关系和Bean的行为。
Bean的定义主要有Beandefinition描述：
Bean是配置文件信息中&amp;lt;bean/&amp;gt;节点信息的转化。Spring解析完成后，内部就是一个BeanDefinition对象。
Bean的解析过程过于繁琐，不赘述。
13.2.2 Context组件 ApplicationContext继承了BeanFactory。
ApplicationContext的子类主要包含两个方面：
ConfigurableApplicationContext表示该Context是可修改的，也就是构建Context中，用户可以动态添加或修改已有的配置信息。 WebApplication，用于Web，可以直接访问Servletcontext。 ApplicationContext必须完成的事情：
标识一个应用环境 利用BeanFactory创建Bean对象 保存对象关系表 能够捕获各种事件 13.2.3 Core组件 其中有很多关键类，一个重要的组成部分就是定义了资源的访问方式。
Resource类相关：封装了各种可能的资源类型，也就是说对使用者来说屏蔽了文件类型的不同。通过继承InputStreamSource接口，在这个接口中有个getInputStream方法，返回InputStream类，所有资源都可以通过InputStream来获取，及屏蔽了资源的提供者。
Context把资源的加载、解析和描述工作委托给了ResourcePatternResolver类来完成。
13.2.4 Ioc容器如何工作 如何创建BeanFactory工厂 refresh方法。源码已阅就不贴了。步骤如下：
（1）构建BeanFactory
（2）注册可能感兴趣的事件
（3）创建Bean实例对象
（4）出发被监听的事件
如何创建Bean实例并构建Bean的关系网 详见源码。
Ioc容器的扩展点 BeanFactoryPostProcessor和BeanPostProcessor，分别在构建BeanFactory和构建Be&amp;rsquo;an对象时调用。还有就是InitPostProcessor和DisposableBean，它们分别在Bean实例创建和销毁时被调用。用户可以实现在这些接口中定义的方法，Spring会在适当的时候调用他们。还有一个是FactoryBean。（会扩展是精通Spring的第一步）
Ioc容器如何为我所用 扩展点。通过扩展点来改变Spring的通用行为。（AOP是一个例子，可以作为参考）
13.3 Spring中AOP的特性详解 13.3.1 动态代理的实现原理 java.lang.reflect.Proxy。
重点看公有方法。
阅读源码部分略。
13.2.2 Spring AOP如何实现 13.4 设计模式解析之代理模式 给某一个对象创建一个代理对象，有代理对象控制对原对象的引用，而创建代理对象之后可以再调用时增加一些额外的操作。
13.5 设计模式解析之策略模式 CGLIB与JDK动态代理的选择，就是策略模式的一种实现。</description>
    </item>
    
    <item>
      <title>第8章 虚拟机字节码执行系统</title>
      <link>http://121.199.2.5:6080/499b1754f0844c299d9bf24652161845/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/499b1754f0844c299d9bf24652161845/</guid>
      <description>6.4字节码指令简介 Java虚拟机的指令由一个字节长度的、代表着某种特定操作含义的数字（称为操作码，Opcode）以及跟随其后的零至多个代表此操作所需参数（称为操作数，Operands）而构成。由于Java虚拟机采用面向操作数栈而不是寄存器的架构，所以大多数的指令都不包含操作数，而只有一个操作码。
6.4.1 字节码与数据类型 Java虚拟机的指令集中，大多数的指令都包含了其操作数所对应的数据类型信息。
如果每一种与数据类型相关的指令都支持Java虚拟机所有运行时数据类型的话，那指令的数量恐怕就会超出一个字节所能表示的数量范围了
前面的被系统吞了 之后再补上
第8章 虚拟机字节码执行系统 8.1概述 8.2 运行时栈帧结构 8.2.1 局部变量表 是一组变量存储空间，用于存放方法参数和方法内部定义的局部变量。
用于存放方法参数和方法内部定义的局部变量。
容量以变量槽（Variable Slot）为最小单位，虚拟机规范中并没有明确指明一个Slot应占用的内存空间大小，知识很有导向性地说道每个Slot都应该能存放一个boolean、byte、char、short、int、float、reference或returnAddress类型的数据。
只要保证计时在64位虚拟机中使用了64位的物理内存空间去实现一个Slot，虚拟机仍要使用对齐和不败的手段让Slot在外观上看起来与32位虚拟机中的一致。
Java中占用32位以内的数据类型有boolean、byte、char、short、int、float、reference和returnAddress8种类型。（Java语言与Java虚拟机种的剧本数据类型是存在本质差别的）。reference类型表示对一个对象实例的引用。虚拟机规范没有指明长度和结构。但需要做到如下两点：
从引用中直接或间接地查找到对象在Java堆中的数据存放的起始地址索引； 引用中直接或简介地查找到对象所属数据类型在方法区中地存储的类型信息，否则无法实现Java语言规范中定义的语法约束。 returnAddress类型目前已经很少见了，为字节码指令jsr、jsr_w和ret服务的，指向一条字节码指令的地址，很古老的Java虚拟机曾经使用这几条指令来实现异常处理，现在已经由异常表替代。
Java中明确的64位的数据类型只有long和double两种。分割存储的做法与“long和double的非原子性协定”类似。
但在局部变量表中不会引起数据安全问题（线程私有）。
索引定位。访问32位数据类型的变量，索引n就代表了使用第n个Slot。64位则会同时使用n和n+1两个Slot。对于两个相邻的共同存放一个64位数据的两个Slot，不允许采用任何方式单独访问其中的某一个。
在方法执行时，如果执行的实例（非static），局部变量表中第0位索引的Slot默认时用于传递方法所属对象实例的引用，在方法中可以通过关键字“this”来访问到这个隐含的参数。
为了尽可能节省栈帧空间，局部变量表中的Slot是可以重用的，方法中定义的变量，其作用域并不一定会覆盖整个方法体。副作用：某些情况下会直接影响到GC。
实例，placeholder能否被回收的根本原因是：局部变量表中的Slot是否还存有关于placeholder数组对象的引用。
局部变量表是GC Roots的一部分。把不用的占用了大量内存的变量手动设置为null值。
但冲编码角度讲，以恰当的变量作用域来控制变量回收时间才是最优雅的解决方法。
———————————待补充———————————
局部变量不负初值会编译不通过。</description>
    </item>
    
    <item>
      <title>第一章 并发编程的挑战</title>
      <link>http://121.199.2.5:6080/f6ad1487667e4880973e3b21505810be/</link>
      <pubDate>Tue, 08 Jun 2021 17:51:46 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/f6ad1487667e4880973e3b21505810be/</guid>
      <description>本书特色 结合JDK源码介绍了Java并发框架、线程池的实现原理。
不仅仅局限于Java层面，更深入JVM，CPU。
结合线上应用，给出了一些并发编程实战技巧。
第一章 并发编程的挑战 如果希望通过多线程执行任务让程序运行得更快，会面临非常多得挑战，如上下文切换的问题、死锁的问题，以及受限于硬件和软件的资源限制问题。
1.1 上下文切换 单核处理器支持多线程执行代码，CPU通过给每个线程分配COU时间片来实现。
时间片非常短，所以CPU通过不停的切换线程执行，让我们感觉多个线程是同时执行的，几十毫秒ms。
CPU通过时间片分配算法来循环执行任务。任务从保存到加载的过程就是一次上下文切换。
上下文切换回影响多线程的执行速度。
1.1.1 多线程一定快吗 累加，并发执行的速度比串行慢是因为线程有创建和上下文切换的开销。
1.1.2 测试上下文切换次数和时长 Lmbech3可以测量上下文切换的时长。
cmstat可以测量上下文切换的次数。
CS(Content Switch)表示上下文切换的次数。
1.1.3 如何减少上下文切换 较少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。
无所并发编程：多线程竞争锁时，回引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样回造成大量线程都处于等待状态。 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 1.1.4 减少上下文切换实战 通过见扫线上大量WAITING的线程，来减少上下文切换次数。
1.2 死锁 避免死锁的几个常见方法：
避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的的情况。 1.3 资源限制的挑战 （1）什么是资源限制
资源是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。
硬件资源限制：带宽的上传/下载速读、硬盘读写速度和CPU的处理速度。
软件资源限制：数据库的连接数和scoket连接数。
（2）资源限制引发的问题
在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行，但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的问题。
（3）如何解决资源限制的问题
对于硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多级上运行。比如使用ODPS、Hadoop或者自己搭建服务器集群，不同的机器吹不同的数据。可以通过“数据ID%机器数”，计算得到一个机器编号，然后由对应编号的机器处理这笔数据。
对于软件资源限制，可以考虑使用资源池将资源复用。比如使用连接池将数据库和Socket连接复用，或者调用对方webservice接口获取数据时，只建立一个连接。
（4）在资源限制情况下进行并发编程
如何在资源限制的情况下，让程序执行得更快呢？方法就是，根据不同得资源限制调整程序得并发度。
1.4 本章小结 本章介绍了在进行并发编程时会遇到的几个挑战，并给出了一些建议。
第二章 Java并发机制的底层实现原理 JVM执行字节码，最终需要转化为汇编指令在CPU上执行，Java中所使用得并发机制依赖于JVM得实现和CPU的指令。
2.1 volatile的应用 1.volatile的定义与实现原理 volatile是轻量级的synchronized，在多处理器开发中保证了共享变量的“可见性”。使用得当的话，比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。本文分析Intel处理器是如何实现volatile的。
CPU的术语定义：
术语 英文单词 术语描述 内存屏障 memory barriers 是一组处理器指令，用于事项对内存操作的顺序限制 缓冲行 cache line 缓存中可以分配的最小存储单位。处理器填写缓存线时会加载整个缓存线，需要使用多个主内存读周期 原子操作 atomic line 不可终端的一个或一系列操作 缓存行填充 cache line fill 当处理器试别到内存中读取操作数时可缓存的，处理器读取整个缓存行到适当的缓存（L1,L2,L3的或所有） 缓存命中 cache hit 如果进行高速缓存行填充操作的内存位置仍然是下次处理器访问的地址时，处理器从缓存中读取操作数，而不是从内存读取 写命中 write hit 当处理器将操作数写回到一个内存缓存的区域时，它首先回检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则处理器将这个操作数写回到缓存，而不是写回到内存，这个操作被称为写命中 写缺失 write misses the cache 一个有效的缓存行被写入到不存在的内存区域 为了提高处理速度，处理器不直接个内存进行通信，二十先将系统内存的数据督导内部缓存（L1,L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会像处理器发送一条Lock前缀的指令，将这个变量所在缓存的数据写回到系统内存。但是就算写回到内存，如过其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，再多处理器下，未了保证各个处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的换粗你一直，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，回重新从系统内存中把数据读到处理器缓存里。
Lock前缀的指令在多核处理器下会引发两件事情：
将当前处理器缓存行的数据写回到系统内存 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效 volatile的两条实现原则：</description>
    </item>
    
    <item>
      <title>lantern 邀请码</title>
      <link>http://121.199.2.5:6080/g6ADCM/</link>
      <pubDate>Tue, 08 Jun 2021 10:09:53 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/g6ADCM/</guid>
      <description>lantern 是一款专业的代理软件，方便更好的使用互联网，输入我的邀请码 46KBX2 来获得三个月的蓝灯专业版！立即下载 https://github.com/getlantern/forum</description>
    </item>
    
    <item>
      <title>eth2 的优势</title>
      <link>http://121.199.2.5:6080/eth2/</link>
      <pubDate>Tue, 16 Feb 2021 13:43:25 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/eth2/</guid>
      <description>eth2 相对于 eth1的优势：
1、提高可扩展性 ETH1.0每秒仅支持15个左右的事务。而ETH2.0升级后最终将被划分为64（将来可能会更多）个分片，理论上可以使网络每秒完成达上千甚至上万笔交易。ETH2.0解决了主网运算能力过于集中的问题，并进一步提高了可扩展性。
2、环境可持续性 ETH现阶段仍是使用较为主流的PoW工作量证明共识机制来运行及维护网络安全，PoW模式虽然在确保安全性和去中心化程度上具有一定的优势，但对于维护网络安全的节点付出的代价却是昂贵的。最终只有一个节点会找到正确的哈希值，获得记账权和奖励，但全球所有参与其中的节点却都付出了大量的算力和电力，这样的模式不仅效率低且对于环境资源而言是一种浪费。而升级为PoS权益证明共识机制后，ETH将不再依靠大量的算力和电力来维护、运行网络，而是依靠持币权益验证的方式来创建链上的区块和交易。</description>
    </item>
    
    <item>
      <title>linux 复制文件，并创建不存在的目录</title>
      <link>http://121.199.2.5:6080/a0lZoj/</link>
      <pubDate>Tue, 02 Feb 2021 20:57:44 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/a0lZoj/</guid>
      <description>linux复制文件和目录用cp命令，--parents 参数可以确保不存在目录的创建，比如文件 cp --parents java/doc/readme.md ../doc ，如果目录 doc不存在目录：java/doc则会创建。
高级操作：
#查找当前目录下所有.md文件，并将它们复制到DataxDoc目录，如果目录不存在则双肩 find . -name &amp;#34;*.md&amp;#34; | xargs -I {} cp --parents {} ../DataxDoc </description>
    </item>
    
    <item>
      <title>CassandraReader 插件文档</title>
      <link>http://121.199.2.5:6080/b0b0444d45c74c8f8b1a9fc4d2af738a/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/b0b0444d45c74c8f8b1a9fc4d2af738a/</guid>
      <description>CassandraReader 插件文档 1 快速介绍 CassandraReader插件实现了从Cassandra读取数据。在底层实现上，CassandraReader通过datastax的java driver连接Cassandra实例，并执行相应的cql语句将数据从cassandra中SELECT出来。
2 实现原理 简而言之，CassandraReader通过java driver连接到Cassandra实例，并根据用户配置的信息生成查询SELECT CQL语句，然后发送到Cassandra，并将该CQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
对于用户配置Table、Column的信息，CassandraReader将其拼接为CQL语句发送到Cassandra。
3 功能说明 3.1 配置样例 配置一个从Cassandra同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 3 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;cassandrareader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;host&amp;#34;: &amp;#34;localhost&amp;#34;, &amp;#34;port&amp;#34;: 9042, &amp;#34;useSSL&amp;#34;: false, &amp;#34;keyspace&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;table&amp;#34;: &amp;#34;datax_src&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;textCol&amp;#34;, &amp;#34;blobCol&amp;#34;, &amp;#34;writetime(blobCol)&amp;#34;, &amp;#34;boolCol&amp;#34;, &amp;#34;smallintCol&amp;#34;, &amp;#34;tinyintCol&amp;#34;, &amp;#34;intCol&amp;#34;, &amp;#34;bigintCol&amp;#34;, &amp;#34;varintCol&amp;#34;, &amp;#34;floatCol&amp;#34;, &amp;#34;doubleCol&amp;#34;, &amp;#34;decimalCol&amp;#34;, &amp;#34;dateCol&amp;#34;, &amp;#34;timeCol&amp;#34;, &amp;#34;timeStampCol&amp;#34;, &amp;#34;uuidCol&amp;#34;, &amp;#34;inetCol&amp;#34;, &amp;#34;durationCol&amp;#34;, &amp;#34;listCol&amp;#34;, &amp;#34;mapCol&amp;#34;, &amp;#34;setCol&amp;#34; &amp;#34;tupleCol&amp;#34; &amp;#34;udtCol&amp;#34;, ] } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;:true } } } ] } } 3.2 参数说明 host
描述：Cassandra连接点的域名或ip，多个node之间用逗号分隔。 必选：是 默认值：无 port
描述：Cassandra端口。 必选：是 默认值：9042 username
描述：数据源的用户名 必选：否 默认值：无 password</description>
    </item>
    
    <item>
      <title>CassandraWriter 插件文档</title>
      <link>http://121.199.2.5:6080/056d47f442c1471da1b62d7062dd00e2/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/056d47f442c1471da1b62d7062dd00e2/</guid>
      <description>CassandraWriter 插件文档 1 快速介绍 CassandraWriter插件实现了向Cassandra写入数据。在底层实现上，CassandraWriter通过datastax的java driver连接Cassandra实例，并执行相应的cql语句将数据写入cassandra中。
2 实现原理 简而言之，CassandraWriter通过java driver连接到Cassandra实例，并根据用户配置的信息生成INSERT CQL语句，然后发送到Cassandra。
对于用户配置Table、Column的信息，CassandraReader将其拼接为CQL语句发送到Cassandra。
3 功能说明 3.1 配置样例 配置一个从内存产生到Cassandra导入的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 5 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34;: [ {&amp;#34;value&amp;#34;:&amp;#34;name&amp;#34;,&amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;}, {&amp;#34;value&amp;#34;:&amp;#34;false&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;bool&amp;#34;}, {&amp;#34;value&amp;#34;:&amp;#34;1988-08-08 08:08:08&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;date&amp;#34;}, {&amp;#34;value&amp;#34;:&amp;#34;addr&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;bytes&amp;#34;}, {&amp;#34;value&amp;#34;:1.234,&amp;#34;type&amp;#34;:&amp;#34;double&amp;#34;}, {&amp;#34;value&amp;#34;:12345678,&amp;#34;type&amp;#34;:&amp;#34;long&amp;#34;}, {&amp;#34;value&amp;#34;:2.345,&amp;#34;type&amp;#34;:&amp;#34;double&amp;#34;}, {&amp;#34;value&amp;#34;:3456789,&amp;#34;type&amp;#34;:&amp;#34;long&amp;#34;}, {&amp;#34;value&amp;#34;:&amp;#34;4a0ef8c0-4d97-11d0-db82-ebecdb03ffa5&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;string&amp;#34;}, {&amp;#34;value&amp;#34;:&amp;#34;value&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;bytes&amp;#34;}, {&amp;#34;value&amp;#34;:&amp;#34;-838383838,37377373,-383883838,27272772,393993939,-38383883,83883838,-1350403181,817650816,1630642337,251398784,-622020148&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;string&amp;#34;}, ], &amp;#34;sliceRecordCount&amp;#34;: 10000000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;cassandrawriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;host&amp;#34;: &amp;#34;localhost&amp;#34;, &amp;#34;port&amp;#34;: 9042, &amp;#34;useSSL&amp;#34;: false, &amp;#34;keyspace&amp;#34;: &amp;#34;stresscql&amp;#34;, &amp;#34;table&amp;#34;: &amp;#34;dst&amp;#34;, &amp;#34;batchSize&amp;#34;:10, &amp;#34;column&amp;#34;: [ &amp;#34;name&amp;#34;, &amp;#34;choice&amp;#34;, &amp;#34;date&amp;#34;, &amp;#34;address&amp;#34;, &amp;#34;dbl&amp;#34;, &amp;#34;lval&amp;#34;, &amp;#34;fval&amp;#34;, &amp;#34;ival&amp;#34;, &amp;#34;uid&amp;#34;, &amp;#34;value&amp;#34;, &amp;#34;listval&amp;#34; ] } } } ] } } 3.2 参数说明 host
描述：Cassandra连接点的域名或ip，多个node之间用逗号分隔。 必选：是 默认值：无 port</description>
    </item>
    
    <item>
      <title>DataX</title>
      <link>http://121.199.2.5:6080/dcdba0138c4f45f0a08968ab48edb900/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/dcdba0138c4f45f0a08968ab48edb900/</guid>
      <description>DataX DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。
DataX 商业版本 阿里云DataWorks数据集成是DataX团队在阿里云上的商业化产品，致力于提供复杂网络环境下、丰富的异构数据源之间高速稳定的数据移动能力，以及繁杂业务背景下的数据同步解决方案。目前已经支持云上近3000家客户，单日同步数据超过3万亿条。DataWorks数据集成目前支持离线50+种数据源，可以进行整库迁移、批量上云、增量同步、分库分表等各类同步解决方案。2020年更新实时同步能力，2020年更新实时同步能力，支持10+种数据源的读写任意组合。提供MySQL，Oracle等多种数据源到阿里云MaxCompute，Hologres等大数据引擎的一键全增量同步解决方案。
https://www.aliyun.com/product/bigdata/ide
Features DataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。
DataX详细介绍 请参考：DataX-Introduction Quick Start Download DataX下载地址 请点击：Quick Start Support Data Channels DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图，详情请点击：DataX数据源参考指南
类型 数据源 Reader(读) Writer(写) 文档 RDBMS 关系型数据库 MySQL √ √ 读 、写 Oracle √ √ 读 、写 SQLServer √ √ 读 、写 PostgreSQL √ √ 读 、写 DRDS √ √ 读 、写 通用RDBMS(支持所有关系型数据库) √ √ 读 、写 阿里云数仓数据存储 ODPS √ √ 读 、写 ADS √ 写 OSS √ √ 读 、写 OCS √ √ 读 、写 NoSQL数据存储 OTS √ √ 读 、写 Hbase0.94 √ √ 读 、写 Hbase1.1 √ √ 读 、写 Phoenix4.x √ √ 读 、写 Phoenix5.</description>
    </item>
    
    <item>
      <title>DataX</title>
      <link>http://121.199.2.5:6080/f4223fe7c5a64b12ae87c43ed48cc971/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/f4223fe7c5a64b12ae87c43ed48cc971/</guid>
      <description>DataX DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、SQL Server、Oracle、PostgreSQL、HDFS、Hive、HBase、OTS、ODPS 等各种异构数据源之间高效的数据同步功能。
Features DataX本身作为数据同步框架，将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件，理论上DataX框架可以支持任意数据源类型的数据同步工作。同时DataX插件体系作为一套生态系统, 每接入一套新数据源该新加入的数据源即可实现和现有的数据源互通。
System Requirements Linux JDK(1.8以上，推荐1.8) Python(推荐Python2.6.X) Apache Maven 3.x (Compile DataX) Quick Start 工具部署
方法一、直接下载DataX工具包：DataX下载地址
下载后解压至本地某个目录，进入bin目录，即可运行同步作业：
$ cd {YOUR_DATAX_HOME}/bin $ python datax.py {YOUR_JOB.json} 自检脚本： python {YOUR_DATAX_HOME}/bin/datax.py {YOUR_DATAX_HOME}/job/job.json
方法二、下载DataX源码，自己编译：DataX源码
(1)、下载DataX源码：
$ git clone git@github.com:alibaba/DataX.git (2)、通过maven打包：
$ cd {DataX_source_code_home} $ mvn -U clean package assembly:assembly -Dmaven.test.skip=true 打包成功，日志显示如下：
[INFO] BUILD SUCCESS [INFO] ----------------------------------------------------------------- [INFO] Total time: 08:12 min [INFO] Finished at: 2015-12-13T16:26:48+08:00 [INFO] Final Memory: 133M/960M [INFO] ----------------------------------------------------------------- 打包成功后的DataX包位于 {DataX_source_code_home}/target/datax/datax/ ，结构如下：
$ cd {DataX_source_code_home} $ ls ./target/datax/datax/ bin	conf	job	lib	log	log_perf	plugin	script	tmp 配置示例：从stream读取数据并打印到控制台
第一步、创建作业的配置文件（json格式）
可以通过命令查看配置模板： python datax.py -r {YOUR_READER} -w {YOUR_WRITER}</description>
    </item>
    
    <item>
      <title>datax 3.0 教程</title>
      <link>http://121.199.2.5:6080/bk-4/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/bk-4/</guid>
      <description>DataXDataX ADB PG WriterDataX ADS写入CassandraReader 插件文档CassandraWriter 插件文档Readme.mdDataX插件开发宝典DrdsReader 插件文档DataX DRDSWriterREADME.mdDataX ElasticSearchWriterDataX FtpReader 说明DataX FtpWriter 说明DataX GDBReaderDataX GDBWriterHbase094XReader &amp;amp; Hbase11XReader 插件文档Hbase094XWriter &amp;amp; Hbase11XWriter 插件文档Hbase094XReader &amp;amp; Hbase11XReader 插件文档hbase11xsqlreader 插件文档HBase11xsqlwriter插件文档Hbase094XWriter &amp;amp; Hbase11XWriter 插件文档hbase20xsqlreader 插件文档HBase20xsqlwriter插件文档DataX HdfsReader 插件文档DataX HdfsWriter 插件文档阿里云开源离线同步工具DataX3.0介绍KingbaseesReader 插件文档DataX KingbaseesWriterdatax-kudu-plugindatax-kudu-pluginsDatax MongoDBReaderDatax MongoDBWriterMysqlReader 插件文档DataX MysqlWriterDataX OCSWriter 适用memcached客户端写入ocsDataX ODPSReaderDataX ODPS写入OpenTSDBReader 插件文档OracleReader 插件文档DataX OracleWriterDataX OSSReader 说明DataX OSSWriter 说明OTSReader 插件文档TableStore增量数据导出通道：TableStoreStreamReaderOTSWriter 插件文档PostgresqlReader 插件文档DataX PostgresqlWriterRDBMSReader 插件文档RDBMSWriter 插件文档SqlServerReader 插件文档DataX SqlServerWriterDataX TransformerTSDBReader 插件文档TSDBWriter 插件文档DataX TxtFileReader 说明DataX TxtFileWriter 说明DataX</description>
    </item>
    
    <item>
      <title>DataX ADB PG Writer</title>
      <link>http://121.199.2.5:6080/1c4beb639bad47a79445e1bb8ccd68a5/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/1c4beb639bad47a79445e1bb8ccd68a5/</guid>
      <description>DataX ADB PG Writer 1 快速介绍 AdbpgWriter 插件实现了写入数据到 ABD PG版数据库的功能。在底层实现上，AdbpgWriter 插件会先缓存需要写入的数据，当缓存的 数据量达到 commitSize 时，插件会通过 JDBC 连接远程 ADB PG版 数据库，并执行 COPY 命令将数据写入 ADB PG 数据库。
AdbpgWriter 可以作为数据迁移工具为用户提供服务。
2 实现原理 AdbpgWriter 通过 DataX 框架获取 Reader 生成的协议数据，首先会将数据缓存，当缓存的数据量达到commitSize时，插件根据你配置生成相应的COPY语句，执行 COPY命令将数据写入ADB PG数据库中。
3 功能说明 3.1 配置样例 这里使用一份从内存产生到 AdbpgWriter导入的数据 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 32 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34; : [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: 19880808, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;1988-08-08 08:08:08&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; }, { &amp;#34;value&amp;#34;: true, &amp;#34;type&amp;#34;: &amp;#34;bool&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ] }, &amp;#34;sliceRecordCount&amp;#34;: 1000 }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;adbpgwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;username&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;password&amp;#34;, &amp;#34;host&amp;#34;: &amp;#34;host&amp;#34;, &amp;#34;port&amp;#34;: &amp;#34;1234&amp;#34;, &amp;#34;database&amp;#34;: &amp;#34;database&amp;#34;, &amp;#34;schema&amp;#34;: &amp;#34;schema&amp;#34;, &amp;#34;table&amp;#34;: &amp;#34;table&amp;#34;, &amp;#34;preSql&amp;#34;: [&amp;#34;delete * from table&amp;#34;], &amp;#34;postSql&amp;#34;: [&amp;#34;select * from table&amp;#34;], &amp;#34;column&amp;#34;: [&amp;#34;*&amp;#34;] } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX ADS写入</title>
      <link>http://121.199.2.5:6080/5d26f13c5f3e4cda84adc613514cdd53/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/5d26f13c5f3e4cda84adc613514cdd53/</guid>
      <description>DataX ADS写入 1 快速介绍 欢迎ADS加入DataX生态圈！ADSWriter插件实现了其他数据源向ADS写入功能，现有DataX所有的数据源均可以无缝接入ADS，实现数据快速导入ADS。
ADS写入预计支持两种实现方式：
ADSWriter 支持向ODPS中转落地导入ADS方式，优点在于当数据量较大时(&amp;gt;1KW)，可以以较快速度进行导入，缺点引入了ODPS作为落地中转，因此牵涉三方系统(DataX、ADS、ODPS)鉴权认证。
ADSWriter 同时支持向ADS直接写入的方式，优点在于小批量数据写入能够较快完成(&amp;lt;1KW)，缺点在于大数据导入较慢。
注意：
如果从ODPS导入数据到ADS，请用户提前在源ODPS的Project中授权ADS Build账号具有读取你源表ODPS的权限，同时，ODPS源表创建人和ADS写入属于同一个阿里云账号。
如果从非ODPS导入数据到ADS，请用户提前在目的端ADS空间授权ADS Build账号具备Load data权限。
以上涉及ADS Build账号请联系ADS管理员提供。
2 实现原理 ADS写入预计支持两种实现方式：
2.1 Load模式 DataX 将数据导入ADS为当前导入任务分配的ADS项目表，随后DataX通知ADS完成数据加载。该类数据导入方式实际上是写ADS完成数据同步，由于ADS是分布式存储集群，因此该通道吞吐量较大，可以支持TB级别数据导入。
DataX底层得到明文的 jdbc://host:port/dbname + username + password + table， 以此连接ADS， 执行show grants; 前置检查该用户是否有ADS中目标表的Load Data或者更高的权限。注意，此时ADSWriter使用用户填写的ADS用户名+密码信息完成登录鉴权工作。
检查通过后，通过ADS中目标表的元数据反向生成ODPS DDL，在ODPS中间project中，以ADSWriter的账户建立ODPS表（非分区表，生命周期设为1-2Day), 并调用ODPSWriter把数据源的数据写入该ODPS表中。
注意，这里需要使用中转ODPS的账号AK向中转ODPS写入数据。
写入完成后，以中转ODPS账号连接ADS，发起Load Data From ‘odps://中转project/中转table/&amp;rsquo; [overwrite] into adsdb.adstable [partition (xx,xx=xx)]; 这个命令返回一个Job ID需要记录。
注意，此时ADS使用自己的Build账号访问中转ODPS，因此需要中转ODPS对这个Build账号提前开放读取权限。
连接ADS一分钟一次轮询执行 select state from information_schema.job_instances where job_id like ‘$Job ID’，查询状态，注意这个第一个一分钟可能查不到状态记录。
Success或者Fail后返回给用户，然后删除中转ODPS表，任务结束。
上述流程是从其他非ODPS数据源导入ADS流程，对于ODPS导入ADS流程使用如下流程：
2.2 Insert模式 DataX 将数据直连ADS接口，利用ADS暴露的INSERT接口直写到ADS。该类数据导入方式写入吞吐量较小，不适合大批量数据写入。有如下注意点：
ADSWriter使用JDBC连接直连ADS，并只使用了JDBC Statement进行数据插入。ADS不支持PreparedStatement，故ADSWriter只能单行多线程进行写入。
ADSWriter支持筛选部分列，列换序等功能，即用户可以填写列。
考虑到ADS负载问题，建议ADSWriter Insert模式建议用户使用TPS限流，最高在1W TPS。
ADSWriter在所有Task完成写入任务后，Job Post单例执行flush工作，保证数据在ADS整体更新。
3 功能说明 3.1 配置样例 这里使用一份从内存产生到ADS，使用Load模式进行导入的数据。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 2 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34;: [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 100000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;adswriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;odps&amp;#34;: { &amp;#34;accessId&amp;#34;: &amp;#34;xxx&amp;#34;, &amp;#34;accessKey&amp;#34;: &amp;#34;xxx&amp;#34;, &amp;#34;account&amp;#34;: &amp;#34;xxx@aliyun.</description>
    </item>
    
    <item>
      <title>DataX DRDSWriter</title>
      <link>http://121.199.2.5:6080/d31c9e889b154a0f96e4eb4be5c50467/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/d31c9e889b154a0f96e4eb4be5c50467/</guid>
      <description>DataX DRDSWriter 1 快速介绍 DRDSWriter 插件实现了写入数据到 DRDS 的目的表的功能。在底层实现上， DRDSWriter 通过 JDBC 连接远程 DRDS 数据库的 Proxy，并执行相应的 replace into &amp;hellip; 的 sql 语句将数据写入 DRDS，特别注意执行的 Sql 语句是 replace into，为了避免数据重复写入，需要你的表具备主键或者唯一性索引(Unique Key)。
DRDSWriter 面向ETL开发工程师，他们使用 DRDSWriter 从数仓导入数据到 DRDS。同时 DRDSWriter 亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 DRDSWriter 通过 DataX 框架获取 Reader 生成的协议数据，通过 replace into...(没有遇到主键/唯一性索引冲突时，与 insert into 行为一致，冲突时会用新行替换原有行所有字段) 的语句写入数据到 DRDS。DRDSWriter 累积一定数据，提交给 DRDS 的 Proxy，该 Proxy 内部决定数据是写入一张还是多张表以及多张表写入时如何路由数据。 注意：整个任务至少需要具备 replace into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 DRDS 导入的数据。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34; : [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: 19880808, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;1988-08-08 08:08:08&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; }, { &amp;#34;value&amp;#34;: true, &amp;#34;type&amp;#34;: &amp;#34;bool&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 1000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;drdswriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;writeMode&amp;#34;: &amp;#34;insert&amp;#34;, &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;, &amp;#34;name&amp;#34; ], &amp;#34;preSql&amp;#34;: [ &amp;#34;delete from test&amp;#34; ], &amp;#34;connection&amp;#34;: [ { &amp;#34;jdbcUrl&amp;#34;: &amp;#34;jdbc:mysql://127.</description>
    </item>
    
    <item>
      <title>DataX ElasticSearchWriter</title>
      <link>http://121.199.2.5:6080/5e857fe9cf5743dc837f14fbc9f0a876/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/5e857fe9cf5743dc837f14fbc9f0a876/</guid>
      <description>DataX ElasticSearchWriter 1 快速介绍 数据导入elasticsearch的插件
2 实现原理 使用elasticsearch的rest api接口， 批量把从reader读入的数据写入elasticsearch
3 功能说明 3.1 配置样例 job.json { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { ... }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;elasticsearchwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;endpoint&amp;#34;: &amp;#34;http://xxx:9999&amp;#34;, &amp;#34;accessId&amp;#34;: &amp;#34;xxxx&amp;#34;, &amp;#34;accessKey&amp;#34;: &amp;#34;xxxx&amp;#34;, &amp;#34;index&amp;#34;: &amp;#34;test-1&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;default&amp;#34;, &amp;#34;cleanup&amp;#34;: true, &amp;#34;settings&amp;#34;: {&amp;#34;index&amp;#34; :{&amp;#34;number_of_shards&amp;#34;: 1, &amp;#34;number_of_replicas&amp;#34;: 0}}, &amp;#34;discovery&amp;#34;: false, &amp;#34;batchSize&amp;#34;: 1000, &amp;#34;splitter&amp;#34;: &amp;#34;,&amp;#34;, &amp;#34;column&amp;#34;: [ {&amp;#34;name&amp;#34;: &amp;#34;pk&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;id&amp;#34;}, { &amp;#34;name&amp;#34;: &amp;#34;col_ip&amp;#34;,&amp;#34;type&amp;#34;: &amp;#34;ip&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_double&amp;#34;,&amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_long&amp;#34;,&amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_integer&amp;#34;,&amp;#34;type&amp;#34;: &amp;#34;integer&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_keyword&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;keyword&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_text&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;analyzer&amp;#34;: &amp;#34;ik_max_word&amp;#34;}, { &amp;#34;name&amp;#34;: &amp;#34;col_geo_point&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;geo_point&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_date&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy-MM-dd HH:mm:ss&amp;#34;}, { &amp;#34;name&amp;#34;: &amp;#34;col_nested1&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;nested&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_nested2&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;nested&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_object1&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_object2&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;col_integer_array&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;integer&amp;#34;, &amp;#34;array&amp;#34;:true}, { &amp;#34;name&amp;#34;: &amp;#34;col_geo_shape&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;geo_shape&amp;#34;, &amp;#34;tree&amp;#34;: &amp;#34;quadtree&amp;#34;, &amp;#34;precision&amp;#34;: &amp;#34;10m&amp;#34;} ] } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX FtpReader 说明</title>
      <link>http://121.199.2.5:6080/0b6970ee252d4875841a98382b04f30b/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/0b6970ee252d4875841a98382b04f30b/</guid>
      <description>DataX FtpReader 说明 1 快速介绍 FtpReader提供了读取远程FTP文件系统数据存储的能力。在底层实现上，FtpReader获取远程FTP文件数据，并转换为DataX传输协议传递给Writer。
本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。
2 功能与限制 FtpReader实现了从远程FTP文件读取数据并转为DataX协议的功能，远程FTP文件本身是无结构化数据存储，对于DataX而言，FtpReader实现上类比TxtFileReader，有诸多相似之处。目前FtpReader支持功能如下：
支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。
支持类CSV格式文件，自定义分隔符。
支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量
支持递归读取、支持文件名过滤。
支持文本压缩，现有压缩格式为zip、gzip、bzip2。
多个File可以支持并发读取。
我们暂时不能做到：
单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。
单个File在压缩情况下，从技术上无法支持多线程并发读取。
3 功能说明 3.1 配置样例 { &amp;#34;setting&amp;#34;: {}, &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 2 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;ftpreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;protocol&amp;#34;: &amp;#34;sftp&amp;#34;, &amp;#34;host&amp;#34;: &amp;#34;127.0.0.1&amp;#34;, &amp;#34;port&amp;#34;: 22, &amp;#34;username&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;xxx&amp;#34;, &amp;#34;path&amp;#34;: [ &amp;#34;/home/hanfa.shf/ftpReaderTest/data&amp;#34; ], &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;index&amp;#34;: 1, &amp;#34;type&amp;#34;: &amp;#34;boolean&amp;#34; }, { &amp;#34;index&amp;#34;: 2, &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; }, { &amp;#34;index&amp;#34;: 3, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 4, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy.MM.dd&amp;#34; } ], &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;ftpWriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: &amp;#34;/home/hanfa.</description>
    </item>
    
    <item>
      <title>DataX FtpWriter 说明</title>
      <link>http://121.199.2.5:6080/8f4c3b36705842458a8550717b87b66c/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/8f4c3b36705842458a8550717b87b66c/</guid>
      <description>DataX FtpWriter 说明 1 快速介绍 FtpWriter提供了向远程FTP文件写入CSV格式的一个或者多个文件，在底层实现上，FtpWriter将DataX传输协议下的数据转换为csv格式，并使用FTP相关的网络协议写出到远程FTP服务器。
写入FTP文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。
2 功能与限制 FtpWriter实现了从DataX协议转为FTP文件功能，FTP文件本身是无结构化数据存储，FtpWriter如下几个方面约定:
支持且仅支持写入文本类型(不支持BLOB如视频数据)的文件，且要求文本中shema为一张二维表。
支持类CSV格式文件，自定义分隔符。
写出时不支持文本压缩。
支持多线程写入，每个线程写入不同子文件。
我们不能做到：
单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { &amp;#34;setting&amp;#34;: {}, &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 2 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: {}, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;ftpwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;protocol&amp;#34;: &amp;#34;sftp&amp;#34;, &amp;#34;host&amp;#34;: &amp;#34;***&amp;#34;, &amp;#34;port&amp;#34;: 22, &amp;#34;username&amp;#34;: &amp;#34;xxx&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;xxx&amp;#34;, &amp;#34;timeout&amp;#34;: &amp;#34;60000&amp;#34;, &amp;#34;connectPattern&amp;#34;: &amp;#34;PASV&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/tmp/data/&amp;#34;, &amp;#34;fileName&amp;#34;: &amp;#34;yixiao&amp;#34;, &amp;#34;writeMode&amp;#34;: &amp;#34;truncate|append|nonConflict&amp;#34;, &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34;, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;nullFormat&amp;#34;: &amp;#34;null&amp;#34;, &amp;#34;dateFormat&amp;#34;: &amp;#34;yyyy-MM-dd&amp;#34;, &amp;#34;fileFormat&amp;#34;: &amp;#34;csv&amp;#34;, &amp;#34;suffix&amp;#34;: &amp;#34;.csv&amp;#34;, &amp;#34;header&amp;#34;: [] } } } ] } } 3.2 参数说明 protocol
描述：ftp服务器协议，目前支持传输协议有ftp和sftp。 必选：是 默认值：无 host
描述：ftp服务器地址。 必选：是 默认值：无 port
描述：ftp服务器端口。 必选：否 默认值：若传输协议是sftp协议，默认值是22；若传输协议是标准ftp协议，默认值是21 timeout
描述：连接ftp服务器连接超时时间，单位毫秒。 必选：否 默认值：60000（1分钟）</description>
    </item>
    
    <item>
      <title>DataX GDBReader</title>
      <link>http://121.199.2.5:6080/ef41ae35929b479db90214d2ebe5ff8a/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/ef41ae35929b479db90214d2ebe5ff8a/</guid>
      <description>DataX GDBReader 1. 快速介绍 GDBReader插件实现读取GDB实例数据的功能，通过Gremlin Client连接远程GDB实例，按配置提供的label生成查询DSL，遍历点或边数据，包括属性数据，并将数据写入到Record中给到Writer使用。
2. 实现原理 GDBReader使用Gremlin Client连接GDB实例，按label分不同Task取点或边数据。 单个Task中按label遍历点或边的id，再切分范围分多次请求查询点或边和属性数据，最后将点或边数据根据配置转换成指定格式记录发送给下游写插件。
GDBReader按label切分多个Task并发，同一个label的数据批量异步获取来加快读取速度。如果配置读取的label列表为空，任务启动前会从GDB查询所有label再切分Task。
3. 功能说明 GDB中点和边不同，读取需要区分点和边点配置。
3.1 点配置样例 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } &amp;#34;errorLimit&amp;#34;: { &amp;#34;record&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;gdbreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;host&amp;#34;: &amp;#34;10.218.145.24&amp;#34;, &amp;#34;port&amp;#34;: 8182, &amp;#34;username&amp;#34;: &amp;#34;***&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;***&amp;#34;, &amp;#34;fetchBatchSize&amp;#34;: 100, &amp;#34;rangeSplitSize&amp;#34;: 1000, &amp;#34;labelType&amp;#34;: &amp;#34;VERTEX&amp;#34;, &amp;#34;labels&amp;#34;: [&amp;#34;label1&amp;#34;, &amp;#34;label2&amp;#34;], &amp;#34;column&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;id&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;columnType&amp;#34;: &amp;#34;primaryKey&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;label&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;columnType&amp;#34;: &amp;#34;primaryLabel&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;age&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;int&amp;#34;, &amp;#34;columnType&amp;#34;: &amp;#34;vertexProperty&amp;#34; } ] } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;: true } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX GDBWriter</title>
      <link>http://121.199.2.5:6080/3830a303d3e34cec88b98eab9934006d/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/3830a303d3e34cec88b98eab9934006d/</guid>
      <description>DataX GDBWriter 1 快速介绍 GDBWriter插件实现了写入数据到GDB实例的功能。GDBWriter通过Gremlin Client连接远程GDB实例，获取Reader的数据，生成写入DSL语句，将数据写入到GDB。
2 实现原理 GDBWriter通过DataX框架获取Reader生成的协议数据，使用g.addV/E(GDB___label).property(id, GDB___id).property(GDB___PK1, GDB___PV1)...语句写入数据到GDB实例。
可以配置Gremlin Client工作在session模式，由客户端控制事务，在一次事务中实现多个记录的批量写入。
3 功能说明 因为GDB中点和边的配置不同，导入时需要区分点和边的配置。
3.1 点配置样例 这里是一份从内存生成点数据导入GDB实例的配置 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34; : [ { &amp;#34;random&amp;#34;: &amp;#34;1,100&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; }, { &amp;#34;random&amp;#34;: &amp;#34;1000,1200&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;random&amp;#34;: &amp;#34;60,64&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;random&amp;#34;: &amp;#34;100,1000&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;random&amp;#34;: &amp;#34;32,48&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 1000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;gdbwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;host&amp;#34;: &amp;#34;gdb-endpoint&amp;#34;, &amp;#34;port&amp;#34;: 8182, &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;***&amp;#34;, &amp;#34;writeMode&amp;#34;: &amp;#34;INSERT&amp;#34;, &amp;#34;labelType&amp;#34;: &amp;#34;VERTEX&amp;#34;, &amp;#34;label&amp;#34;: &amp;#34;#{1}&amp;#34;, &amp;#34;idTransRule&amp;#34;: &amp;#34;none&amp;#34;, &amp;#34;session&amp;#34;: true, &amp;#34;maxRecordsInBatch&amp;#34;: 64, &amp;#34;column&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;id&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;#{0}&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;columnType&amp;#34;: &amp;#34;primaryKey&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;vertex_propKey&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;#{2}&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;columnType&amp;#34;: &amp;#34;vertexSetProperty&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;vertex_propKey&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;#{3}&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34;, &amp;#34;columnType&amp;#34;: &amp;#34;vertexSetProperty&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;vertex_propKey2&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;#{4}&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;columnType&amp;#34;: &amp;#34;vertexProperty&amp;#34; } ] } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX HdfsReader 插件文档</title>
      <link>http://121.199.2.5:6080/e123c1afd572427f9fa4d27ba10b1e99/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/e123c1afd572427f9fa4d27ba10b1e99/</guid>
      <description>DataX HdfsReader 插件文档 1 快速介绍 HdfsReader提供了读取分布式文件系统数据存储的能力。在底层实现上，HdfsReader获取分布式文件系统上文件的数据，并转换为DataX传输协议传递给Writer。
目前HdfsReader支持的文件格式有textfile（text）、orcfile（orc）、rcfile（rc）、sequence file（seq）和普通逻辑二维表（csv）类型格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表。
HdfsReader需要Jdk1.7及以上版本的支持。
2 功能与限制 HdfsReader实现了从Hadoop分布式文件系统Hdfs中读取文件数据并转为DataX协议的功能。textfile是Hive建表时默认使用的存储格式，数据不做压缩，本质上textfile就是以文本的形式将数据存放在hdfs中，对于DataX而言，HdfsReader实现上类比TxtFileReader，有诸多相似之处。orcfile，它的全名是Optimized Row Columnar file，是对RCFile做了优化。据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。HdfsReader利用Hive提供的OrcSerde类，读取解析orcfile文件的数据。目前HdfsReader支持的功能如下：
支持textfile、orcfile、rcfile、sequence file和csv格式的文件，且要求文件内容存放的是一张逻辑意义上的二维表。
支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量
支持递归读取、支持正则表达式（&amp;quot;*&amp;ldquo;和&amp;rdquo;?&amp;quot;）。
支持orcfile数据压缩，目前支持SNAPPY，ZLIB两种压缩方式。
多个File可以支持并发读取。
支持sequence file数据压缩，目前支持lzo压缩方式。
csv类型支持压缩格式有：gzip、bz2、zip、lzo、lzo_deflate、snappy。
目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试；
支持kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效）
我们暂时不能做到：
单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。 目前还不支持hdfs HA; 3 功能说明 3.1 配置样例 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 3 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hdfsreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: &amp;#34;/user/hive/warehouse/mytable01/*&amp;#34;, &amp;#34;defaultFS&amp;#34;: &amp;#34;hdfs://xxx:port&amp;#34;, &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;index&amp;#34;: 1, &amp;#34;type&amp;#34;: &amp;#34;boolean&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;hello&amp;#34; }, { &amp;#34;index&amp;#34;: 2, &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; } ], &amp;#34;fileType&amp;#34;: &amp;#34;orc&amp;#34;, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;: true } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX HdfsWriter 插件文档</title>
      <link>http://121.199.2.5:6080/dd5ebd2f221f4913ae09f61f3877725e/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/dd5ebd2f221f4913ae09f61f3877725e/</guid>
      <description>DataX HdfsWriter 插件文档 1 快速介绍 HdfsWriter提供向HDFS文件系统指定路径中写入TEXTFile文件和ORCFile文件,文件内容可与hive中表关联。
2 功能与限制 (1)、目前HdfsWriter仅支持textfile和orcfile两种格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表; (2)、由于HDFS是文件系统，不存在schema的概念，因此不支持对部分列写入; (3)、目前仅支持与以下Hive数据类型： 数值型：TINYINT,SMALLINT,INT,BIGINT,FLOAT,DOUBLE 字符串类型：STRING,VARCHAR,CHAR 布尔类型：BOOLEAN 时间类型：DATE,TIMESTAMP 目前不支持：decimal、binary、arrays、maps、structs、union类型; (4)、对于Hive分区表目前仅支持一次写入单个分区; (5)、对于textfile需用户保证写入hdfs文件的分隔符与在Hive上创建表时的分隔符一致,从而实现写入hdfs数据与Hive表字段关联; (6)、HdfsWriter实现过程是：首先根据用户指定的path，创建一个hdfs文件系统上不存在的临时目录，创建规则：path_随机；然后将读取的文件写入这个临时目录；全部写入后再将这个临时目录下的文件移动到用户指定目录（在创建文件时保证文件名不重复）; 最后删除临时目录。如果在中间过程发生网络中断等情况造成无法与hdfs建立连接，需要用户手动删除已经写入的文件和临时目录。 (7)、目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试； (8)、目前HdfsWriter支持Kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效） 3 功能说明 3.1 配置样例 { &amp;#34;setting&amp;#34;: {}, &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 2 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilereader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: [&amp;#34;/Users/shf/workplace/txtWorkplace/job/dataorcfull.txt&amp;#34;], &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;index&amp;#34;: 1, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;index&amp;#34;: 2, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;index&amp;#34;: 3, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;index&amp;#34;: 4, &amp;#34;type&amp;#34;: &amp;#34;DOUBLE&amp;#34; }, { &amp;#34;index&amp;#34;: 5, &amp;#34;type&amp;#34;: &amp;#34;DOUBLE&amp;#34; }, { &amp;#34;index&amp;#34;: 6, &amp;#34;type&amp;#34;: &amp;#34;STRING&amp;#34; }, { &amp;#34;index&amp;#34;: 7, &amp;#34;type&amp;#34;: &amp;#34;STRING&amp;#34; }, { &amp;#34;index&amp;#34;: 8, &amp;#34;type&amp;#34;: &amp;#34;STRING&amp;#34; }, { &amp;#34;index&amp;#34;: 9, &amp;#34;type&amp;#34;: &amp;#34;BOOLEAN&amp;#34; }, { &amp;#34;index&amp;#34;: 10, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; }, { &amp;#34;index&amp;#34;: 11, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; } ], &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;\t&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hdfswriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;defaultFS&amp;#34;: &amp;#34;hdfs://xxx:port&amp;#34;, &amp;#34;fileType&amp;#34;: &amp;#34;orc&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/user/hive/warehouse/writerorc.</description>
    </item>
    
    <item>
      <title>DataX KingbaseesWriter</title>
      <link>http://121.199.2.5:6080/a50c2cb341e14f7d995d470380e24d34/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/a50c2cb341e14f7d995d470380e24d34/</guid>
      <description>DataX KingbaseesWriter 1 快速介绍 KingbaseesWriter插件实现了写入数据到 KingbaseES主库目的表的功能。在底层实现上，KingbaseesWriter通过JDBC连接远程 KingbaseES 数据库，并执行相应的 insert into &amp;hellip; sql 语句将数据写入 KingbaseES，内部会分批次提交入库。
KingbaseesWriter面向ETL开发工程师，他们使用KingbaseesWriter从数仓导入数据到KingbaseES。同时 KingbaseesWriter亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 KingbaseesWriter通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL插入语句
insert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2. KingbaseesWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 KingbaseesWriter导入的数据。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34; : [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: 19880808, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;1988-08-08 08:08:08&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; }, { &amp;#34;value&amp;#34;: true, &amp;#34;type&amp;#34;: &amp;#34;bool&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 1000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;kingbaseeswriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;, &amp;#34;name&amp;#34; ], &amp;#34;preSql&amp;#34;: [ &amp;#34;delete from test&amp;#34; ], &amp;#34;connection&amp;#34;: [ { &amp;#34;jdbcUrl&amp;#34;: &amp;#34;jdbc:kingbase8://127.</description>
    </item>
    
    <item>
      <title>Datax MongoDBReader</title>
      <link>http://121.199.2.5:6080/41dfc822f8ca4bca812501f2801fc78e/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/41dfc822f8ca4bca812501f2801fc78e/</guid>
      <description>Datax MongoDBReader 1 快速介绍 MongoDBReader 插件利用 MongoDB 的java客户端MongoClient进行MongoDB的读操作。最新版本的Mongo已经将DB锁的粒度从DB级别降低到document级别，配合上MongoDB强大的索引功能，基本可以达到高性能的读取MongoDB的需求。
2 实现原理 MongoDBReader通过Datax框架从MongoDB并行的读取数据，通过主控的JOB程序按照指定的规则对MongoDB中的数据进行分片，并行读取，然后将MongoDB支持的类型通过逐一判断转换成Datax支持的类型。
3 功能说明 该示例从ODPS读一份数据到MongoDB。
{ &amp;quot;job&amp;quot;: { &amp;quot;setting&amp;quot;: { &amp;quot;speed&amp;quot;: { &amp;quot;channel&amp;quot;: 2 } }, &amp;quot;content&amp;quot;: [ { &amp;quot;reader&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;mongodbreader&amp;quot;, &amp;quot;parameter&amp;quot;: { &amp;quot;address&amp;quot;: [&amp;quot;127.0.0.1:27017&amp;quot;], &amp;quot;userName&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;userPassword&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;dbName&amp;quot;: &amp;quot;tag_per_data&amp;quot;, &amp;quot;collectionName&amp;quot;: &amp;quot;tag_data12&amp;quot;, &amp;quot;column&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;unique_id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;sid&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;user_id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;auction_id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;content_type&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;pool_type&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;frontcat_id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;Array&amp;quot;, &amp;quot;spliter&amp;quot;: &amp;quot;&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;categoryid&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;Array&amp;quot;, &amp;quot;spliter&amp;quot;: &amp;quot;&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;gmt_create&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;taglist&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;Array&amp;quot;, &amp;quot;spliter&amp;quot;: &amp;quot; &amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;property&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;scorea&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;int&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;scoreb&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;int&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;scorec&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;int&amp;quot; } ] } }, &amp;quot;writer&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;odpswriter&amp;quot;, &amp;quot;parameter&amp;quot;: { &amp;quot;project&amp;quot;: &amp;quot;tb_ai_recommendation&amp;quot;, &amp;quot;table&amp;quot;: &amp;quot;jianying_tag_datax_read_test01&amp;quot;, &amp;quot;column&amp;quot;: [ &amp;quot;unique_id&amp;quot;, &amp;quot;sid&amp;quot;, &amp;quot;user_id&amp;quot;, &amp;quot;auction_id&amp;quot;, &amp;quot;content_type&amp;quot;, &amp;quot;pool_type&amp;quot;, &amp;quot;frontcat_id&amp;quot;, &amp;quot;categoryid&amp;quot;, &amp;quot;gmt_create&amp;quot;, &amp;quot;taglist&amp;quot;, &amp;quot;property&amp;quot;, &amp;quot;scorea&amp;quot;, &amp;quot;scoreb&amp;quot; ], &amp;quot;accessId&amp;quot;: &amp;quot;**************&amp;quot;, &amp;quot;accessKey&amp;quot;: &amp;quot;********************&amp;quot;, &amp;quot;truncate&amp;quot;: true, &amp;quot;odpsServer&amp;quot;: &amp;quot;xxx/api&amp;quot;, &amp;quot;tunnelServer&amp;quot;: &amp;quot;xxx&amp;quot;, &amp;quot;accountType&amp;quot;: &amp;quot;aliyun&amp;quot; } } } ] } } 4 参数说明 address： MongoDB的数据地址信息，因为MonogDB可能是个集群，则ip端口信息需要以Json数组的形式给出。【必填】 userName：MongoDB的用户名。【选填】 userPassword： MongoDB的密码。【选填】 collectionName： MonogoDB的集合名。【必填】 column：MongoDB的文档列名。【必填】 name：Column的名字。【必填】 type：Column的类型。【选填】 splitter：因为MongoDB支持数组类型，但是Datax框架本身不支持数组类型，所以mongoDB读出来的数组类型要通过这个分隔符合并成字符串。【选填】 query: MongoDB的额外查询条件。【选填】 5 类型转换 DataX 内部类型 MongoDB 数据类型 Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes 6 性能报告 7 测试报告 </description>
    </item>
    
    <item>
      <title>Datax MongoDBWriter</title>
      <link>http://121.199.2.5:6080/1b755458e9fa4e6f89a7d44321cd92ec/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/1b755458e9fa4e6f89a7d44321cd92ec/</guid>
      <description>Datax MongoDBWriter 1 快速介绍 MongoDBWriter 插件利用 MongoDB 的java客户端MongoClient进行MongoDB的写操作。最新版本的Mongo已经将DB锁的粒度从DB级别降低到document级别，配合上MongoDB强大的索引功能，基本可以满足数据源向MongoDB写入数据的需求，针对数据更新的需求，通过配置业务主键的方式也可以实现。
2 实现原理 MongoDBWriter通过Datax框架获取Reader生成的数据，然后将Datax支持的类型通过逐一判断转换成MongoDB支持的类型。其中一个值得指出的点就是Datax本身不支持数组类型，但是MongoDB支持数组类型，并且数组类型的索引还是蛮强大的。为了使用MongoDB的数组类型，则可以通过参数的特殊配置，将字符串可以转换成MongoDB中的数组。类型转换之后，就可以依托于Datax框架并行的写入MongoDB。
3 功能说明 该示例从ODPS读一份数据到MongoDB。
{ &amp;quot;job&amp;quot;: { &amp;quot;setting&amp;quot;: { &amp;quot;speed&amp;quot;: { &amp;quot;channel&amp;quot;: 2 } }, &amp;quot;content&amp;quot;: [ { &amp;quot;reader&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;odpsreader&amp;quot;, &amp;quot;parameter&amp;quot;: { &amp;quot;accessId&amp;quot;: &amp;quot;********&amp;quot;, &amp;quot;accessKey&amp;quot;: &amp;quot;*********&amp;quot;, &amp;quot;project&amp;quot;: &amp;quot;tb_ai_recommendation&amp;quot;, &amp;quot;table&amp;quot;: &amp;quot;jianying_tag_datax_test&amp;quot;, &amp;quot;column&amp;quot;: [ &amp;quot;unique_id&amp;quot;, &amp;quot;sid&amp;quot;, &amp;quot;user_id&amp;quot;, &amp;quot;auction_id&amp;quot;, &amp;quot;content_type&amp;quot;, &amp;quot;pool_type&amp;quot;, &amp;quot;frontcat_id&amp;quot;, &amp;quot;categoryid&amp;quot;, &amp;quot;gmt_create&amp;quot;, &amp;quot;taglist&amp;quot;, &amp;quot;property&amp;quot;, &amp;quot;scorea&amp;quot;, &amp;quot;scoreb&amp;quot; ], &amp;quot;splitMode&amp;quot;: &amp;quot;record&amp;quot;, &amp;quot;odpsServer&amp;quot;: &amp;quot;http://xxx/api&amp;quot; } }, &amp;quot;writer&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;mongodbwriter&amp;quot;, &amp;quot;parameter&amp;quot;: { &amp;quot;address&amp;quot;: [ &amp;quot;127.0.0.1:27017&amp;quot; ], &amp;quot;userName&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;userPassword&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;dbName&amp;quot;: &amp;quot;tag_per_data&amp;quot;, &amp;quot;collectionName&amp;quot;: &amp;quot;tag_data&amp;quot;, &amp;quot;column&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;unique_id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;sid&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;user_id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;auction_id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;content_type&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;pool_type&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;frontcat_id&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;Array&amp;quot;, &amp;quot;splitter&amp;quot;: &amp;quot; &amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;categoryid&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;Array&amp;quot;, &amp;quot;splitter&amp;quot;: &amp;quot; &amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;gmt_create&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;taglist&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;Array&amp;quot;, &amp;quot;splitter&amp;quot;: &amp;quot; &amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;property&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;scorea&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;int&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;scoreb&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;int&amp;quot; }, { &amp;quot;name&amp;quot;: &amp;quot;scorec&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;int&amp;quot; } ], &amp;quot;upsertInfo&amp;quot;: { &amp;quot;isUpsert&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;upsertKey&amp;quot;: &amp;quot;unique_id&amp;quot; } } } } ] } } 4 参数说明 address： MongoDB的数据地址信息，因为MonogDB可能是个集群，则ip端口信息需要以Json数组的形式给出。【必填】 userName：MongoDB的用户名。【选填】 userPassword： MongoDB的密码。【选填】 collectionName： MonogoDB的集合名。【必填】 column：MongoDB的文档列名。【必填】 name：Column的名字。【必填】 type：Column的类型。【选填】 splitter：特殊分隔符，当且仅当要处理的字符串要用分隔符分隔为字符数组时，才使用这个参数，通过这个参数指定的分隔符，将字符串分隔存储到MongoDB的数组中。【选填】 upsertInfo：指定了传输数据时更新的信息。【选填】 isUpsert：当设置为true时，表示针对相同的upsertKey做更新操作。【选填】 upsertKey：upsertKey指定了没行记录的业务主键。用来做更新时使用。【选填】 5 类型转换 DataX 内部类型 MongoDB 数据类型 Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes 6 性能报告 7 测试报告 </description>
    </item>
    
    <item>
      <title>DataX MysqlWriter</title>
      <link>http://121.199.2.5:6080/ee3103b29d5b4fa696cb69e35fefb970/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/ee3103b29d5b4fa696cb69e35fefb970/</guid>
      <description>DataX MysqlWriter 1 快速介绍 MysqlWriter 插件实现了写入数据到 Mysql 主库的目的表的功能。在底层实现上， MysqlWriter 通过 JDBC 连接远程 Mysql 数据库，并执行相应的 insert into &amp;hellip; 或者 ( replace into &amp;hellip;) 的 sql 语句将数据写入 Mysql，内部会分批次提交入库，需要数据库本身采用 innodb 引擎。
MysqlWriter 面向ETL开发工程师，他们使用 MysqlWriter 从数仓导入数据到 Mysql。同时 MysqlWriter 亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 MysqlWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置的 writeMode 生成
insert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 或者 replace into...(没有遇到主键/唯一性索引冲突时，与 insert into 行为一致，冲突时会用新行替换原有行所有字段) 的语句写入数据到 Mysql。出于性能考虑，采用了 PreparedStatement + Batch，并且设置了：rewriteBatchedStatements=true，将数据缓冲到线程上下文 Buffer 中，当 Buffer 累计到预定阈值时，才发起写入请求。 注意：目的表所在数据库必须是主库才能写入数据；整个任务至少需要具备 insert/replace into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 Mysql 导入的数据。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34; : [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: 19880808, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;1988-08-08 08:08:08&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; }, { &amp;#34;value&amp;#34;: true, &amp;#34;type&amp;#34;: &amp;#34;bool&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 1000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;mysqlwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;writeMode&amp;#34;: &amp;#34;insert&amp;#34;, &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;, &amp;#34;name&amp;#34; ], &amp;#34;session&amp;#34;: [ &amp;#34;set session sql_mode=&amp;#39;ANSI&amp;#39;&amp;#34; ], &amp;#34;preSql&amp;#34;: [ &amp;#34;delete from test&amp;#34; ], &amp;#34;connection&amp;#34;: [ { &amp;#34;jdbcUrl&amp;#34;: &amp;#34;jdbc:mysql://127.</description>
    </item>
    
    <item>
      <title>DataX OCSWriter 适用memcached客户端写入ocs</title>
      <link>http://121.199.2.5:6080/d4b1e453f8274dda834a305a5e6a38bf/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/d4b1e453f8274dda834a305a5e6a38bf/</guid>
      <description>DataX OCSWriter 适用memcached客户端写入ocs 1 快速介绍 1.1 OCS简介 开放缓存服务( Open Cache Service，简称OCS）是基于内存的缓存服务，支持海量小数据的高速访问。OCS可以极大缓解对后端存储的压力，提高网站或应用的响应速度。OCS支持Key-Value的数据结构，兼容Memcached协议的客户端都可与OCS通信。
OCS 支持即开即用的方式快速部署；对于动态Web、APP应用，可通过缓存服务减轻对数据库的压力，从而提高网站整体的响应速度。
与本地MemCache相同之处在于OCS兼容Memcached协议，与用户环境兼容，可直接用于OCS服务 不同之处在于硬件和数据部署在云端，有完善的基础设施、网络安全保障、系统维护服务。所有的这些服务，都不需要投资，只需根据使用量进行付费即可。
1.2 OCSWriter简介 OCSWriter是DataX实现的，基于Memcached协议的数据写入OCS通道。
2 功能说明 2.1 配置样例 这里使用一份从内存产生的数据导入到OCS。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34;: [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: 19880808, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;1988-08-08 08:08:08&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; }, { &amp;#34;value&amp;#34;: true, &amp;#34;type&amp;#34;: &amp;#34;bool&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 1000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;ocswriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;proxy&amp;#34;: &amp;#34;xxxx&amp;#34;, &amp;#34;port&amp;#34;: &amp;#34;11211&amp;#34;, &amp;#34;userName&amp;#34;: &amp;#34;user&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;******&amp;#34;, &amp;#34;writeMode&amp;#34;: &amp;#34;set|add|replace|append|prepend&amp;#34;, &amp;#34;writeFormat&amp;#34;: &amp;#34;text|binary&amp;#34;, &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;\u0001&amp;#34;, &amp;#34;expireTime&amp;#34;: 1000, &amp;#34;indexes&amp;#34;: &amp;#34;0,2&amp;#34;, &amp;#34;batchSize&amp;#34;: 1000 } } } ] } } 2.</description>
    </item>
    
    <item>
      <title>DataX ODPSReader</title>
      <link>http://121.199.2.5:6080/5b1b6009df684895ab2ef943e412ee8a/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/5b1b6009df684895ab2ef943e412ee8a/</guid>
      <description>DataX ODPSReader 1 快速介绍 ODPSReader 实现了从 ODPS读取数据的功能，有关ODPS请参看(https://help.aliyun.com/document_detail/27800.html?spm=5176.doc27803.6.101.NxCIgY)。 在底层实现上，ODPSReader 根据你配置的 源头项目 / 表 / 分区 / 表字段 等信息，通过 Tunnel 从 ODPS 系统中读取数据。
注意 1、如果你需要使用ODPSReader/Writer插件，由于 AccessId/AccessKey 解密的需要，请务必使用 JDK 1.6.32 及以上版本。JDK 安装事项，请联系 PE 处理 2、ODPSReader 不是通过 ODPS SQL （select ... from ... where ... ）来抽取数据的 3、注意区分你要读取的表是线上环境还是线下环境 4、目前 DataX3 依赖的 SDK 版本是： &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.aliyun.odps&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;odps-sdk-core-internal&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.13.2&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 2 实现原理 ODPSReader 支持读取分区表、非分区表，不支持读取虚拟视图。当要读取分区表时，需要指定出具体的分区配置，比如读取 t0 表，其分区为 pt=1,ds=hangzhou 那么你需要在配置中配置该值。当要读取非分区表时，你不能提供分区配置。表字段可以依序指定全部列，也可以指定部分列，或者调整列顺序，或者指定常量字段，但是表字段中不能指定分区列（分区列不是表字段）。
注意：要特别注意 odpsServer、project、table、accessId、accessKey 的配置，因为直接影响到是否能够加载到你需要读取数据的表。很多权限问题都出现在这里。 3 功能说明 3.1 配置样例 这里使用一份读出 ODPS 数据然后打印到屏幕的配置样板。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;odpsreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;accessId&amp;#34;: &amp;#34;accessId&amp;#34;, &amp;#34;accessKey&amp;#34;: &amp;#34;accessKey&amp;#34;, &amp;#34;project&amp;#34;: &amp;#34;targetProjectName&amp;#34;, &amp;#34;table&amp;#34;: &amp;#34;tableName&amp;#34;, &amp;#34;partition&amp;#34;: [ &amp;#34;pt=1,ds=hangzhou&amp;#34; ], &amp;#34;column&amp;#34;: [ &amp;#34;customer_id&amp;#34;, &amp;#34;nickname&amp;#34; ], &amp;#34;packageAuthorizedProject&amp;#34;: &amp;#34;yourCurrentProjectName&amp;#34;, &amp;#34;splitMode&amp;#34;: &amp;#34;record&amp;#34;, &amp;#34;odpsServer&amp;#34;: &amp;#34;http://xxx/api&amp;#34;, &amp;#34;tunnelServer&amp;#34;: &amp;#34;http://dt.</description>
    </item>
    
    <item>
      <title>DataX ODPS写入</title>
      <link>http://121.199.2.5:6080/ab33f89c494e47eaae02f9c21465b322/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/ab33f89c494e47eaae02f9c21465b322/</guid>
      <description>DataX ODPS写入 1 快速介绍 ODPSWriter插件用于实现往ODPS插入或者更新数据，主要提供给etl开发同学将业务数据导入odps，适合于TB,GB数量级的数据传输，如果需要传输PB量级的数据，请选择dt task工具 ;
2 实现原理 在底层实现上，ODPSWriter是通过DT Tunnel写入ODPS系统的，有关ODPS的更多技术细节请参看 ODPS主站 https://data.aliyun.com/product/odps 和ODPS产品文档 https://help.aliyun.com/product/27797.html
目前 DataX3 依赖的 SDK 版本是：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.aliyun.odps&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;odps-sdk-core-internal&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.13.2&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 注意: 如果你需要使用ODPSReader/Writer插件，请务必使用JDK 1.6-32及以上版本 使用java -version查看Java版本号
3 功能说明 3.1 配置样例 这里使用一份从内存产生到ODPS导入的数据。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;byte&amp;#34;: 1048576 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34;: [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 100000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;odpswriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;project&amp;#34;: &amp;#34;chinan_test&amp;#34;, &amp;#34;table&amp;#34;: &amp;#34;odps_write_test00_partitioned&amp;#34;, &amp;#34;partition&amp;#34;: &amp;#34;school=SiChuan-School,class=1&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;, &amp;#34;name&amp;#34; ], &amp;#34;accessId&amp;#34;: &amp;#34;xxx&amp;#34;, &amp;#34;accessKey&amp;#34;: &amp;#34;xxxx&amp;#34;, &amp;#34;truncate&amp;#34;: true, &amp;#34;odpsServer&amp;#34;: &amp;#34;http://sxxx/api&amp;#34;, &amp;#34;tunnelServer&amp;#34;: &amp;#34;http://xxx&amp;#34;, &amp;#34;accountType&amp;#34;: &amp;#34;aliyun&amp;#34; } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX OracleWriter</title>
      <link>http://121.199.2.5:6080/1eb2fdd8968d4f6e8bbc0312c6647246/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/1eb2fdd8968d4f6e8bbc0312c6647246/</guid>
      <description>DataX OracleWriter 1 快速介绍 OracleWriter 插件实现了写入数据到 Oracle 主库的目的表的功能。在底层实现上， OracleWriter 通过 JDBC 连接远程 Oracle 数据库，并执行相应的 insert into &amp;hellip; sql 语句将数据写入 Oracle，内部会分批次提交入库。
OracleWriter 面向ETL开发工程师，他们使用 OracleWriter 从数仓导入数据到 Oracle。同时 OracleWriter 亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 OracleWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL语句
insert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2.OracleWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 Oracle 导入的数据。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34; : [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: 19880808, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;1988-08-08 08:08:08&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; }, { &amp;#34;value&amp;#34;: true, &amp;#34;type&amp;#34;: &amp;#34;bool&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 1000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;oraclewriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;, &amp;#34;name&amp;#34; ], &amp;#34;preSql&amp;#34;: [ &amp;#34;delete from test&amp;#34; ], &amp;#34;connection&amp;#34;: [ { &amp;#34;jdbcUrl&amp;#34;: &amp;#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]&amp;#34;, &amp;#34;table&amp;#34;: [ &amp;#34;test&amp;#34; ] } ] } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX OSSReader 说明</title>
      <link>http://121.199.2.5:6080/bf15e9112ab546c9a6d8baab000d9601/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/bf15e9112ab546c9a6d8baab000d9601/</guid>
      <description>DataX OSSReader 说明 1 快速介绍 OSSReader提供了读取OSS数据存储的能力。在底层实现上，OSSReader使用OSS官方Java SDK获取OSS数据，并转换为DataX传输协议传递给Writer。
OSS 产品介绍, 参看[阿里云OSS Portal] OSS Java SDK, 参看[阿里云OSS Java SDK] 2 功能与限制 OSSReader实现了从OSS读取数据并转为DataX协议的功能，OSS本身是无结构化数据存储，对于DataX而言，OSSReader实现上类比TxtFileReader，有诸多相似之处。目前OSSReader支持功能如下：
支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。
支持类CSV格式文件，自定义分隔符。
支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量
支持递归读取、支持文件名过滤。
支持文本压缩，现有压缩格式为zip、gzip、bzip2。注意，一个压缩包不允许多文件打包压缩。
多个object可以支持并发读取。
我们暂时不能做到：
单个Object(File)支持多线程并发读取，这里涉及到单个Object内部切分算法。二期考虑支持。
单个Object在压缩情况下，从技术上无法支持多线程并发读取。
3 功能说明 3.1 配置样例 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: {}, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;ossreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;endpoint&amp;#34;: &amp;#34;http://oss.aliyuncs.com&amp;#34;, &amp;#34;accessId&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;accessKey&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;bucket&amp;#34;: &amp;#34;myBucket&amp;#34;, &amp;#34;object&amp;#34;: [ &amp;#34;bazhen/*&amp;#34; ], &amp;#34;column&amp;#34;: [ { &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34;, &amp;#34;index&amp;#34;: 0 }, { &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;alibaba&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;index&amp;#34;: 1, &amp;#34;format&amp;#34;: &amp;#34;yyyy-MM-dd&amp;#34; } ], &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;\t&amp;#34;, &amp;#34;compress&amp;#34;: &amp;#34;gzip&amp;#34; } }, &amp;#34;writer&amp;#34;: {} } ] } } 3.2 参数说明 endpoint
描述：OSS Server的EndPoint地址，例如http://oss.</description>
    </item>
    
    <item>
      <title>DataX OSSWriter 说明</title>
      <link>http://121.199.2.5:6080/516202a7c10f4658ad32aa517c141a05/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/516202a7c10f4658ad32aa517c141a05/</guid>
      <description>DataX OSSWriter 说明 1 快速介绍 OSSWriter提供了向OSS写入类CSV格式的一个或者多个表文件。
写入OSS内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。
OSS 产品介绍, 参看[阿里云OSS Portal] OSS Java SDK, 参看[阿里云OSS Java SDK] 2 功能与限制 OSSWriter实现了从DataX协议转为OSS中的TXT文件功能，OSS本身是无结构化数据存储，OSSWriter需要在如下几个方面增加:
支持且仅支持写入 TXT的文件，且要求TXT中shema为一张二维表。
支持类CSV格式文件，自定义分隔符。
暂时不支持文本压缩。
支持多线程写入，每个线程写入不同子文件。
文件支持滚动，当文件大于某个size值或者行数值，文件需要切换。 [暂不支持]
我们不能做到：
单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: {}, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;osswriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;endpoint&amp;#34;: &amp;#34;http://oss.aliyuncs.com&amp;#34;, &amp;#34;accessId&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;accessKey&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;bucket&amp;#34;: &amp;#34;myBucket&amp;#34;, &amp;#34;object&amp;#34;: &amp;#34;cdo/datax&amp;#34;, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34;, &amp;#34;writeMode&amp;#34;: &amp;#34;truncate|append|nonConflict&amp;#34; } } } ] } } 3.2 参数说明 endpoint
描述：OSS Server的EndPoint地址，例如http://oss.aliyuncs.com。
必选：是 默认值：无 accessId
描述：OSS的accessId 必选：是 默认值：无 accessKey
描述：OSS的accessKey 必选：是 默认值：无 bucket
描述：OSS的bucket 必选：是 默认值：无 object
描述：OSSWriter写入的文件名，OSS使用文件名模拟目录的实现。 使用&amp;quot;object&amp;quot;: &amp;ldquo;datax&amp;rdquo;，写入object以datax开头，后缀添加随机字符串。 使用&amp;quot;object&amp;quot;: &amp;ldquo;cdo/datax&amp;rdquo;，写入的object以cdo/datax开头，后缀随机添加字符串，/作为OSS模拟目录的分隔符。
必选：是 默认值：无 writeMode</description>
    </item>
    
    <item>
      <title>DataX PostgresqlWriter</title>
      <link>http://121.199.2.5:6080/1c01500e456b4ef6b6c14eb5a072696e/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/1c01500e456b4ef6b6c14eb5a072696e/</guid>
      <description>DataX PostgresqlWriter 1 快速介绍 PostgresqlWriter插件实现了写入数据到 PostgreSQL主库目的表的功能。在底层实现上，PostgresqlWriter通过JDBC连接远程 PostgreSQL 数据库，并执行相应的 insert into &amp;hellip; sql 语句将数据写入 PostgreSQL，内部会分批次提交入库。
PostgresqlWriter面向ETL开发工程师，他们使用PostgresqlWriter从数仓导入数据到PostgreSQL。同时 PostgresqlWriter亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 PostgresqlWriter通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL插入语句
insert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2. PostgresqlWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 PostgresqlWriter导入的数据。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;column&amp;#34; : [ { &amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;value&amp;#34;: 19880808, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;1988-08-08 08:08:08&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; }, { &amp;#34;value&amp;#34;: true, &amp;#34;type&amp;#34;: &amp;#34;bool&amp;#34; }, { &amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34; } ], &amp;#34;sliceRecordCount&amp;#34;: 1000 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;postgresqlwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;, &amp;#34;name&amp;#34; ], &amp;#34;preSql&amp;#34;: [ &amp;#34;delete from test&amp;#34; ], &amp;#34;connection&amp;#34;: [ { &amp;#34;jdbcUrl&amp;#34;: &amp;#34;jdbc:postgresql://127.</description>
    </item>
    
    <item>
      <title>DataX SqlServerWriter</title>
      <link>http://121.199.2.5:6080/f23fb4346cfa45459b6eaa8537ff5d17/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/f23fb4346cfa45459b6eaa8537ff5d17/</guid>
      <description>DataX SqlServerWriter 1 快速介绍 SqlServerWriter 插件实现了写入数据到 SqlServer 库的目的表的功能。在底层实现上， SqlServerWriter 通过 JDBC 连接远程 SqlServer 数据库，并执行相应的 insert into &amp;hellip; sql 语句将数据写入 SqlServer，内部会分批次提交入库。
SqlServerWriter 面向ETL开发工程师，他们使用 SqlServerWriter 从数仓导入数据到 SqlServer。同时 SqlServerWriter 亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 SqlServerWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL语句
insert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2.SqlServerWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 SqlServer 导入的数据。 { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 5 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: {}, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;sqlserverwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;db_id&amp;#34;, &amp;#34;db_type&amp;#34;, &amp;#34;db_ip&amp;#34;, &amp;#34;db_port&amp;#34;, &amp;#34;db_role&amp;#34;, &amp;#34;db_name&amp;#34;, &amp;#34;db_username&amp;#34;, &amp;#34;db_password&amp;#34;, &amp;#34;db_modify_time&amp;#34;, &amp;#34;db_modify_user&amp;#34;, &amp;#34;db_description&amp;#34;, &amp;#34;db_tddl_info&amp;#34; ], &amp;#34;connection&amp;#34;: [ { &amp;#34;table&amp;#34;: [ &amp;#34;db_info_for_writer&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: &amp;#34;jdbc:sqlserver://[HOST_NAME]:PORT;DatabaseName=[DATABASE_NAME]&amp;#34; } ], &amp;#34;preSql&amp;#34;: [ &amp;#34;delete from @table where db_id = -1;&amp;#34; ], &amp;#34;postSql&amp;#34;: [ &amp;#34;update @table set db_modify_time = now() where db_id = 1;&amp;#34; ] } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX Transformer</title>
      <link>http://121.199.2.5:6080/760d0498cfcd435bafba3fe96793cd01/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/760d0498cfcd435bafba3fe96793cd01/</guid>
      <description>DataX Transformer Transformer定义 在数据同步、传输过程中，存在用户对于数据传输进行特殊定制化的需求场景，包括裁剪列、转换列等工作，可以借助ETL的T过程实现(Transformer)。DataX包含了完整的E(Extract)、T(Transformer)、L(Load)支持。
运行模型 UDF手册 dx_substr 参数：3个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：字段值的开始位置。 第三个参数：目标字段长度。 返回： 从字符串的指定位置（包含）截取指定长度的字符串。如果开始位置非法抛出异常。如果字段为空值，直接返回（即不参与本transformer） 举例： dx_substr(1,&amp;#34;2&amp;#34;,&amp;#34;5&amp;#34;) column 1的value为“dataxTest”=&amp;gt;&amp;#34;taxTe&amp;#34; dx_substr(1,&amp;#34;5&amp;#34;,&amp;#34;10&amp;#34;) column 1的value为“dataxTest”=&amp;gt;&amp;#34;Test&amp;#34; dx_pad 参数：4个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：&amp;ldquo;l&amp;rdquo;,&amp;ldquo;r&amp;rdquo;, 指示是在头进行pad，还是尾进行pad。 第三个参数：目标字段长度。 第四个参数：需要pad的字符。 返回： 如果源字符串长度小于目标字段长度，按照位置添加pad字符后返回。如果长于，直接截断（都截右边）。如果字段为空值，转换为空字符串进行pad，即最后的字符串全是需要pad的字符 举例： dx_pad(1,&amp;#34;l&amp;#34;,&amp;#34;4&amp;#34;,&amp;#34;A&amp;#34;), 如果column 1 的值为 xyz=&amp;gt; Axyz， 值为 xyzzzzz =&amp;gt; xyzz dx_pad(1,&amp;#34;r&amp;#34;,&amp;#34;4&amp;#34;,&amp;#34;A&amp;#34;), 如果column 1 的值为 xyz=&amp;gt; xyzA， 值为 xyzzzzz =&amp;gt; xyzz dx_replace 参数：4个 第一个参数：字段编号，对应record中第几个字段。 第二个参数：字段值的开始位置。 第三个参数：需要替换的字段长度。 第四个参数：需要替换的字符串。 返回： 从字符串的指定位置（包含）替换指定长度的字符串。如果开始位置非法抛出异常。如果字段为空值，直接返回（即不参与本transformer） 举例： dx_replace(1,&amp;#34;2&amp;#34;,&amp;#34;4&amp;#34;,&amp;#34;****&amp;#34;) column 1的value为“dataxTest”=&amp;gt;&amp;#34;da****est&amp;#34; dx_replace(1,&amp;#34;5&amp;#34;,&amp;#34;10&amp;#34;,&amp;#34;****&amp;#34;) column 1的value为“dataxTest”=&amp;gt;&amp;#34;data****&amp;#34; dx_filter （关联filter暂不支持，即多个字段的联合判断，函参太过复杂，用户难以使用。） 参数： 第一个参数：字段编号，对应record中第几个字段。 第二个参数：运算符，支持一下运算符：like, not like, &amp;gt;, =, &amp;lt;, &amp;gt;=, !=, &amp;lt;= 第三个参数：正则表达式（java正则表达式）、值。 返回： 如果匹配正则表达式，返回Null，表示过滤该行。不匹配表达式时，表示保留该行。（注意是该行）。对于&amp;gt;=&amp;lt;都是对字段直接compare的结果. like ， not like是将字段转换成String，然后和目标正则表达式进行全匹配。 , =, &amp;lt;, &amp;gt;=, !=, &amp;lt;= 对于DoubleColumn比较double值，对于LongColumn和DateColumn比较long值，其他StringColumn，BooleanColumn以及ByteColumn均比较的是StringColumn值。
如果目标colunn为空（null），对于 = null的过滤条件，将满足条件，被过滤。！=null的过滤条件，null不满足过滤条件，不被过滤。 like，字段为null不满足条件，不被过滤，和not like，字段为null满足条件，被过滤。 举例： dx_filter(1,&amp;#34;like&amp;#34;,&amp;#34;dataTest&amp;#34;) dx_filter(1,&amp;#34;&amp;gt;=&amp;#34;,&amp;#34;10&amp;#34;) dx_groovy 参数。 第一个参数： groovy code 第二个参数（列表或者为空）：extraPackage 备注： dx_groovy只能调用一次。不能多次调用。 groovy code中支持java.</description>
    </item>
    
    <item>
      <title>DataX TxtFileReader 说明</title>
      <link>http://121.199.2.5:6080/cbeab748638041dfa63c78ebe4fa91db/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/cbeab748638041dfa63c78ebe4fa91db/</guid>
      <description>DataX TxtFileReader 说明 1 快速介绍 TxtFileReader提供了读取本地文件系统数据存储的能力。在底层实现上，TxtFileReader获取本地文件数据，并转换为DataX传输协议传递给Writer。
本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。
2 功能与限制 TxtFileReader实现了从本地文件读取数据并转为DataX协议的功能，本地文件本身是无结构化数据存储，对于DataX而言，TxtFileReader实现上类比OSSReader，有诸多相似之处。目前TxtFileReader支持功能如下：
支持且仅支持读取TXT的文件，且要求TXT中shema为一张二维表。
支持类CSV格式文件，自定义分隔符。
支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量
支持递归读取、支持文件名过滤。
支持文本压缩，现有压缩格式为zip、gzip、bzip2。
多个File可以支持并发读取。
我们暂时不能做到：
单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。
单个File在压缩情况下，从技术上无法支持多线程并发读取。
3 功能说明 3.1 配置样例 { &amp;#34;setting&amp;#34;: {}, &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 2 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilereader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: [&amp;#34;/home/haiwei.luo/case00/data&amp;#34;], &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;index&amp;#34;: 1, &amp;#34;type&amp;#34;: &amp;#34;boolean&amp;#34; }, { &amp;#34;index&amp;#34;: 2, &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; }, { &amp;#34;index&amp;#34;: 3, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 4, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy.MM.dd&amp;#34; } ], &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilewriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: &amp;#34;/home/haiwei.luo/case00/result&amp;#34;, &amp;#34;fileName&amp;#34;: &amp;#34;luohw&amp;#34;, &amp;#34;writeMode&amp;#34;: &amp;#34;truncate&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy-MM-dd&amp;#34; } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>DataX TxtFileWriter 说明</title>
      <link>http://121.199.2.5:6080/599f9ae387e54760ab26e177bd423e82/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/599f9ae387e54760ab26e177bd423e82/</guid>
      <description>DataX TxtFileWriter 说明 1 快速介绍 TxtFileWriter提供了向本地文件写入类CSV格式的一个或者多个表文件。TxtFileWriter服务的用户主要在于DataX开发、测试同学。
写入本地文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。
2 功能与限制 TxtFileWriter实现了从DataX协议转为本地TXT文件功能，本地文件本身是无结构化数据存储，TxtFileWriter如下几个方面约定:
支持且仅支持写入 TXT的文件，且要求TXT中shema为一张二维表。
支持类CSV格式文件，自定义分隔符。
支持文本压缩，现有压缩格式为gzip、bzip2。
支持多线程写入，每个线程写入不同子文件。
文件支持滚动，当文件大于某个size值或者行数值，文件需要切换。 [暂不支持]
我们不能做到：
单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { &amp;#34;setting&amp;#34;: {}, &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 2 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilereader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: [&amp;#34;/home/haiwei.luo/case00/data&amp;#34;], &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;type&amp;#34;: &amp;#34;long&amp;#34; }, { &amp;#34;index&amp;#34;: 1, &amp;#34;type&amp;#34;: &amp;#34;boolean&amp;#34; }, { &amp;#34;index&amp;#34;: 2, &amp;#34;type&amp;#34;: &amp;#34;double&amp;#34; }, { &amp;#34;index&amp;#34;: 3, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 4, &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy.MM.dd&amp;#34; } ], &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilewriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: &amp;#34;/home/haiwei.luo/case00/result&amp;#34;, &amp;#34;fileName&amp;#34;: &amp;#34;luohw&amp;#34;, &amp;#34;writeMode&amp;#34;: &amp;#34;truncate&amp;#34;, &amp;#34;dateFormat&amp;#34;: &amp;#34;yyyy-MM-dd&amp;#34; } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>datax-kudu-plugin</title>
      <link>http://121.199.2.5:6080/3909c1ec487c48198ae49ab46ba98dcb/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/3909c1ec487c48198ae49ab46ba98dcb/</guid>
      <description>datax-kudu-plugin datax kudu的writer插件
仅在kudu11进行过测试</description>
    </item>
    
    <item>
      <title>datax-kudu-plugins</title>
      <link>http://121.199.2.5:6080/23a56c95c6c4402998fb8bf16e29fc05/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/23a56c95c6c4402998fb8bf16e29fc05/</guid>
      <description>datax-kudu-plugins datax kudu的writer插件
eg:
{ &amp;#34;name&amp;#34;: &amp;#34;kuduwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;kuduConfig&amp;#34;: { &amp;#34;kudu.master_addresses&amp;#34;: &amp;#34;***&amp;#34;, &amp;#34;timeout&amp;#34;: 60000, &amp;#34;sessionTimeout&amp;#34;: 60000 }, &amp;#34;table&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;replicaCount&amp;#34;: 3, &amp;#34;truncate&amp;#34;: false, &amp;#34;writeMode&amp;#34;: &amp;#34;upsert&amp;#34;, &amp;#34;partition&amp;#34;: { &amp;#34;range&amp;#34;: { &amp;#34;column1&amp;#34;: [ { &amp;#34;lower&amp;#34;: &amp;#34;2020-08-25&amp;#34;, &amp;#34;upper&amp;#34;: &amp;#34;2020-08-26&amp;#34; }, { &amp;#34;lower&amp;#34;: &amp;#34;2020-08-26&amp;#34;, &amp;#34;upper&amp;#34;: &amp;#34;2020-08-27&amp;#34; }, { &amp;#34;lower&amp;#34;: &amp;#34;2020-08-27&amp;#34;, &amp;#34;upper&amp;#34;: &amp;#34;2020-08-28&amp;#34; } ] }, &amp;#34;hash&amp;#34;: { &amp;#34;column&amp;#34;: [ &amp;#34;column1&amp;#34; ], &amp;#34;number&amp;#34;: 3 } }, &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;name&amp;#34;: &amp;#34;c1&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;primaryKey&amp;#34;: true }, { &amp;#34;index&amp;#34;: 1, &amp;#34;name&amp;#34;: &amp;#34;c2&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;compress&amp;#34;: &amp;#34;DEFAULT_COMPRESSION&amp;#34;, &amp;#34;encoding&amp;#34;: &amp;#34;AUTO_ENCODING&amp;#34;, &amp;#34;comment&amp;#34;: &amp;#34;注解xxxx&amp;#34; } ], &amp;#34;batchSize&amp;#34;: 1024, &amp;#34;bufferSize&amp;#34;: 2048, &amp;#34;skipFail&amp;#34;: false, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34; } } 必须参数：</description>
    </item>
    
    <item>
      <title>DataX插件开发宝典</title>
      <link>http://121.199.2.5:6080/7c5af007072741d2a1b8a4578137e185/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/7c5af007072741d2a1b8a4578137e185/</guid>
      <description>DataX插件开发宝典 本文面向DataX插件开发人员，尝试尽可能全面地阐述开发一个DataX插件所经过的历程，力求消除开发者的困惑，让插件开发变得简单。
一、开发之前 路走对了，就不怕远。✓ 路走远了，就不管对不对。✕
当你打开这篇文档，想必已经不用在此解释什么是DataX了。那下一个问题便是：
DataX为什么要使用插件机制？ 从设计之初，DataX就把异构数据源同步作为自身的使命，为了应对不同数据源的差异、同时提供一致的同步原语和扩展能力，DataX自然而然地采用了框架 + 插件 的模式：
插件只需关心数据的读取或者写入本身。 而同步的共性问题，比如：类型转换、性能、统计，则交由框架来处理。 作为插件开发人员，则需要关注两个问题：
数据源本身的读写数据正确性。 如何与框架沟通、合理正确地使用框架。 开工前需要想明白的问题 就插件本身而言，希望在您动手coding之前，能够回答我们列举的这些问题，不然路走远了发现没走对，就尴尬了。
二、插件视角看框架 逻辑执行模型 插件开发者不用关心太多，基本只需要关注特定系统读和写，以及自己的代码在逻辑上是怎样被执行的，哪一个方法是在什么时候被调用的。在此之前，需要明确以下概念：
Job: Job是DataX用以描述从一个源头到一个目的端的同步作业，是DataX数据同步的最小业务单元。比如：从一张mysql的表同步到odps的一个表的特定分区。 Task: Task是为最大化而把Job拆分得到的最小执行单元。比如：读一张有1024个分表的mysql分库分表的Job，拆分成1024个读Task，用若干个并发执行。 TaskGroup: 描述的是一组Task集合。在同一个TaskGroupContainer执行下的Task集合称之为TaskGroup JobContainer: Job执行器，负责Job全局拆分、调度、前置语句和后置语句等工作的工作单元。类似Yarn中的JobTracker TaskGroupContainer: TaskGroup执行器，负责执行一组Task的工作单元，类似Yarn中的TaskTracker。 简而言之， Job拆分成Task，在分别在框架提供的容器中执行，插件只需要实现Job和Task两部分逻辑。
物理执行模型 框架为插件提供物理上的执行能力（线程）。DataX框架有三种运行模式：
Standalone: 单进程运行，没有外部依赖。 Local: 单进程运行，统计信息、错误信息汇报到集中存储。 Distrubuted: 分布式多进程运行，依赖DataX Service服务。 当然，上述三种模式对插件的编写而言没有什么区别，你只需要避开一些小错误，插件就能够在单机/分布式之间无缝切换了。 当JobContainer和TaskGroupContainer运行在同一个进程内时，就是单机模式（Standalone和Local）；当它们分布在不同的进程中执行时，就是分布式（Distributed）模式。
是不是很简单？
编程接口 那么，Job和Task的逻辑应是怎么对应到具体的代码中的？
首先，插件的入口类必须扩展Reader或Writer抽象类，并且实现分别实现Job和Task两个内部抽象类，Job和Task的实现必须是 内部类 的形式，原因见 加载原理 一节。以Reader为例：
public class SomeReader extends Reader { public static class Job extends Reader.Job { @Override public void init() { } @Override public void prepare() { } @Override public List&amp;lt;Configuration&amp;gt; split(int adviceNumber) { return null; } @Override public void post() { } @Override public void destroy() { } } public static class Task extends Reader.</description>
    </item>
    
    <item>
      <title>DrdsReader 插件文档</title>
      <link>http://121.199.2.5:6080/20f3c6090e2448838d8be1baca3a5e1c/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/20f3c6090e2448838d8be1baca3a5e1c/</guid>
      <description>DrdsReader 插件文档 1 快速介绍 DrdsReader插件实现了从DRDS(分布式RDS)读取数据。在底层实现上，DrdsReader通过JDBC连接远程DRDS数据库，并执行相应的sql语句将数据从DRDS库中SELECT出来。
DRDS的插件目前DataX只适配了Mysql引擎的场景，DRDS对于DataX而言，就是一套分布式Mysql数据库，并且大部分通信协议遵守Mysql使用场景。
2 实现原理 简而言之，DrdsReader通过JDBC连接器连接到远程的DRDS数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程DRDS数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
对于用户配置Table、Column、Where的信息，DrdsReader将其拼接为SQL语句发送到DRDS数据库。不同于普通的Mysql数据库，DRDS作为分布式数据库系统，无法适配所有Mysql的协议，包括复杂的Join等语句，DRDS暂时无法支持。
3 功能说明 3.1 配置样例 配置一个从DRDS数据库同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. &amp;#34;byte&amp;#34;: 1048576 } //出错限制 &amp;#34;errorLimit&amp;#34;: { //出错的record条数上限，当大于该值即报错。 &amp;#34;record&amp;#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% &amp;#34;percentage&amp;#34;: 0.02 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;drdsReader&amp;#34;, &amp;#34;parameter&amp;#34;: { // 数据库连接用户名 &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, // 数据库连接密码 &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;，&amp;#34;name&amp;#34; ], &amp;#34;connection&amp;#34;: [ { &amp;#34;table&amp;#34;: [ &amp;#34;table&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:mysql://127.0.0.1:3306/database&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { //writer类型 &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, //是否打印内容 &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;drdsreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;where&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;querySql&amp;#34;: [ &amp;#34;select db_id,on_line_flag from db_info where db_id &amp;lt; 10;&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:drds://localhost:3306/database&amp;#34;] } ] } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;: false, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34; } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>Hbase094XReader &amp; Hbase11XReader 插件文档</title>
      <link>http://121.199.2.5:6080/34ec6d7caba24a86b7a5e9a2f37d8c46/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/34ec6d7caba24a86b7a5e9a2f37d8c46/</guid>
      <description>Hbase094XReader &amp;amp; Hbase11XReader 插件文档 1 快速介绍 HbaseReader 插件实现了从 Hbase中读取数据。在底层实现上，HbaseReader 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 Scan 方式读取你指定 rowkey 范围内的数据，并将读取的数据使用 DataX 自定义的数据类型拼装为抽象的数据集，并传递给下游 Writer 处理。
1.1支持的功能 1、目前HbaseReader支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。
若您的hbase版本为Hbase0.94.x，reader端的插件请选择：hbase094xreader，即：
&amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase094xreader&amp;#34; } 若您的hbase版本为Hbase1.1.x，reader端的插件请选择：hbase11xreader,即：
&amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase11xreader&amp;#34; } 2、目前HbaseReader支持两模式读取：normal 模式、multiVersionFixedColumn模式；
normal 模式：把HBase中的表，当成普通二维表（横表）进行读取,读取最新版本数据。如：
hbase(main):017:0&amp;gt; scan &amp;lsquo;users&amp;rsquo; ROW COLUMN+CELL lisi column=address:city, timestamp=1457101972764, value=beijing lisi column=address:contry, timestamp=1457102773908, value=china lisi column=address:province, timestamp=1457101972736, value=beijing lisi column=info:age, timestamp=1457101972548, value=27 lisi column=info:birthday, timestamp=1457101972604, value=1987-06-17 lisi column=info:company, timestamp=1457101972653, value=baidu xiaoming column=address:city, timestamp=1457082196082, value=hangzhou xiaoming column=address:contry, timestamp=1457082195729, value=china xiaoming column=address:province, timestamp=1457082195773, value=zhejiang xiaoming column=info:age, timestamp=1457082218735, value=29 xiaoming column=info:birthday, timestamp=1457082186830, value=1987-06-17 xiaoming column=info:company, timestamp=1457082189826, value=alibaba 2 row(s) in 0.0580 seconds</description>
    </item>
    
    <item>
      <title>Hbase094XReader &amp; Hbase11XReader 插件文档</title>
      <link>http://121.199.2.5:6080/7c5fc71d1a124a5396b1a17e8293be71/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/7c5fc71d1a124a5396b1a17e8293be71/</guid>
      <description>Hbase094XReader &amp;amp; Hbase11XReader 插件文档 1 快速介绍 HbaseReader 插件实现了从 Hbase中读取数据。在底层实现上，HbaseReader 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 Scan 方式读取你指定 rowkey 范围内的数据，并将读取的数据使用 DataX 自定义的数据类型拼装为抽象的数据集，并传递给下游 Writer 处理。
1.1支持的功能 1、目前HbaseReader支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。
若您的hbase版本为Hbase0.94.x，reader端的插件请选择：hbase094xreader，即：
&amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase094xreader&amp;#34; } 若您的hbase版本为Hbase1.1.x，reader端的插件请选择：hbase11xreader,即：
&amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase11xreader&amp;#34; } 2、目前HbaseReader支持两模式读取：normal 模式、multiVersionFixedColumn模式；
normal 模式：把HBase中的表，当成普通二维表（横表）进行读取,读取最新版本数据。如：
hbase(main):017:0&amp;gt; scan &amp;lsquo;users&amp;rsquo; ROW COLUMN+CELL lisi column=address:city, timestamp=1457101972764, value=beijing lisi column=address:contry, timestamp=1457102773908, value=china lisi column=address:province, timestamp=1457101972736, value=beijing lisi column=info:age, timestamp=1457101972548, value=27 lisi column=info:birthday, timestamp=1457101972604, value=1987-06-17 lisi column=info:company, timestamp=1457101972653, value=baidu xiaoming column=address:city, timestamp=1457082196082, value=hangzhou xiaoming column=address:contry, timestamp=1457082195729, value=china xiaoming column=address:province, timestamp=1457082195773, value=zhejiang xiaoming column=info:age, timestamp=1457082218735, value=29 xiaoming column=info:birthday, timestamp=1457082186830, value=1987-06-17 xiaoming column=info:company, timestamp=1457082189826, value=alibaba 2 row(s) in 0.0580 seconds</description>
    </item>
    
    <item>
      <title>Hbase094XWriter &amp; Hbase11XWriter 插件文档</title>
      <link>http://121.199.2.5:6080/0172fbcbd4674f98a8782403fc6a7ca5/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/0172fbcbd4674f98a8782403fc6a7ca5/</guid>
      <description>Hbase094XWriter &amp;amp; Hbase11XWriter 插件文档 1 快速介绍 HbaseWriter 插件实现了从向Hbase中写取数据。在底层实现上，HbaseWriter 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 put 方式写入Hbase。
1.1支持功能 1、目前HbaseWriter支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。
若您的hbase版本为Hbase0.94.x，writer端的插件请选择：hbase094xwriter，即：
&amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase094xwriter&amp;#34; } 若您的hbase版本为Hbase1.1.x，writer端的插件请选择：hbase11xwriter,即：
&amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase11xwriter&amp;#34; } 2、目前HbaseWriter支持源端多个字段拼接作为hbase 表的 rowkey，具体配置参考：rowkeyColumn配置；
3、写入hbase的时间戳（版本）支持：用当前时间作为版本，指定源端列作为版本，指定一个时间 三种方式作为版本；
4、HbaseWriter中有一个必填配置项是：hbaseConfig，需要你联系 HBase PE，将hbase-site.xml 中与连接 HBase 相关的配置项提取出来，以 json 格式填入，同时可以补充更多HBase client的配置来优化与服务器的交互。
如：hbase-site.xml的配置内容如下
&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://ip:9000/hbase&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;***&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt; 转换后的json为：
&amp;#34;hbaseConfig&amp;#34;: { &amp;#34;hbase.rootdir&amp;#34;: &amp;#34;hdfs: //ip: 9000/hbase&amp;#34;, &amp;#34;hbase.cluster.distributed&amp;#34;: &amp;#34;true&amp;#34;, &amp;#34;hbase.zookeeper.quorum&amp;#34;: &amp;#34;***&amp;#34; } 1.2 限制 1、目前只支持源端为横表写入，不支持竖表（源端读出的为四元组: rowKey，family:qualifier，timestamp，value）模式的数据写入；本期目标主要是替换DataX2中的habsewriter，下次迭代考虑支持。
2、目前不支持写入hbase前清空表数据，若需要清空数据请联系HBase PE
2 实现原理 简而言之，HbaseWriter 通过 HBase 的 Java 客户端，通过 HTable, Put等 API，将从上游Reader读取的数据写入HBase你hbase11xwriter与hbase094xwriter的主要不同在于API的调用不同，Hbase1.1.x废弃了很多Hbase0.94.x的api。
3 功能说明 3.1 配置样例 配置一个从本地写入hbase1.1.x的作业： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 5 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilereader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: &amp;#34;/Users/shf/workplace/datax_test/hbase11xwriter/txt/normal.</description>
    </item>
    
    <item>
      <title>Hbase094XWriter &amp; Hbase11XWriter 插件文档</title>
      <link>http://121.199.2.5:6080/745df21fdbd349ab9a9fc5c94551b336/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/745df21fdbd349ab9a9fc5c94551b336/</guid>
      <description>Hbase094XWriter &amp;amp; Hbase11XWriter 插件文档 1 快速介绍 HbaseWriter 插件实现了从向Hbase中写取数据。在底层实现上，HbaseWriter 通过 HBase 的 Java 客户端连接远程 HBase 服务，并通过 put 方式写入Hbase。
1.1支持功能 1、目前HbaseWriter支持的Hbase版本有：Hbase0.94.x和Hbase1.1.x。
若您的hbase版本为Hbase0.94.x，writer端的插件请选择：hbase094xwriter，即：
&amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase094xwriter&amp;#34; } 若您的hbase版本为Hbase1.1.x，writer端的插件请选择：hbase11xwriter,即：
&amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase11xwriter&amp;#34; } 2、目前HbaseWriter支持源端多个字段拼接作为hbase 表的 rowkey，具体配置参考：rowkeyColumn配置；
3、写入hbase的时间戳（版本）支持：用当前时间作为版本，指定源端列作为版本，指定一个时间 三种方式作为版本；
1.2 限制 1、目前只支持源端为横表写入，不支持竖表（源端读出的为四元组: rowKey，family:qualifier，timestamp，value）模式的数据写入；本期目标主要是替换DataX2中的habsewriter，下次迭代考虑支持。
2、目前不支持写入hbase前清空表数据，若需要清空数据请联系HBase PE
2 实现原理 简而言之，HbaseWriter 通过 HBase 的 Java 客户端，通过 HTable, Put等 API，将从上游Reader读取的数据写入HBase你hbase11xwriter与hbase094xwriter的主要不同在于API的调用不同，Hbase1.1.x废弃了很多Hbase0.94.x的api。
3 功能说明 3.1 配置样例 配置一个从本地写入hbase1.1.x的作业： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 5 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilereader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: &amp;#34;/Users/shf/workplace/datax_test/hbase11xwriter/txt/normal.txt&amp;#34;, &amp;#34;charset&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;type&amp;#34;: &amp;#34;String&amp;#34; }, { &amp;#34;index&amp;#34;: 1, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 2, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 3, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 4, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 5, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 6, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; } ], &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase11xwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;hbaseConfig&amp;#34;: { &amp;#34;hbase.</description>
    </item>
    
    <item>
      <title>hbase11xsqlreader  插件文档</title>
      <link>http://121.199.2.5:6080/283a89fffad54e6f909c2936a1ff08ad/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/283a89fffad54e6f909c2936a1ff08ad/</guid>
      <description>hbase11xsqlreader 插件文档 1 快速介绍 hbase11xsqlreader插件实现了从Phoenix(HBase SQL)读取数据。在底层实现上，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并执行相应的sql语句将数据从Phoenix库中SELECT出来。
2 实现原理 简而言之，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并根据用户配置的信息生成查询SELECT 语句，然后发送到HBase集群，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
hbase11xsqlreader 插件文档 1 快速介绍 hbase11xsqlreader插件实现了从Phoenix(HBase SQL)读取数据。在底层实现上，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并执行相应的sql语句将数据从Phoenix库中SELECT出来。
2 实现原理 简而言之，hbase11xsqlreader通过Phoenix客户端去连接远程的HBase集群，并根据用户配置的信息生成查询SELECT 语句，然后发送到HBase集群，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
3 功能说明 3.1 配置样例 配置一个从Phoenix同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. &amp;#34;byte&amp;#34;:10485760 }, //出错限制 &amp;#34;errorLimit&amp;#34;: { //出错的record条数上限，当大于该值即报错。 &amp;#34;record&amp;#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% &amp;#34;percentage&amp;#34;: 0.02 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { //指定插件为hbase11xsqlreader &amp;#34;name&amp;#34;: &amp;#34;hbase11xsqlreader&amp;#34;, &amp;#34;parameter&amp;#34;: { //填写连接Phoenix的hbase集群zk地址 &amp;#34;hbaseConfig&amp;#34;: { &amp;#34;hbase.zookeeper.quorum&amp;#34;: &amp;#34;hb-proxy-xxx-002.hbase.rds.aliyuncs.com,hb-proxy-xxx-001.hbase.rds.aliyuncs.com,hb-proxy-xxx-003.hbase.rds.aliyuncs.com&amp;#34; }, //填写要读取的phoenix的表名 &amp;#34;table&amp;#34;: &amp;#34;US_POPULATION&amp;#34;, //填写要读取的列名，不填读取所有列 &amp;#34;column&amp;#34;: [ ] } }, &amp;#34;writer&amp;#34;: { //writer类型 &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, //是否打印内容 &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;:true, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34; } } } ] } } 3.2 参数说明 hbaseConfig
描述：hbase11xsqlreader需要通过Phoenix客户端去连接hbase集群，因此这里需要填写对应hbase集群的zkurl地址，注意不要添加2181。
必选：是 默认值：无 table
描述：编写Phoenix中的表名,如果有namespace，该值设置为&amp;rsquo;namespace.tablename&#39;
必选：是 默认值：无 column</description>
    </item>
    
    <item>
      <title>HBase11xsqlwriter插件文档</title>
      <link>http://121.199.2.5:6080/1c329829c22c499d8e684735252966c1/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/1c329829c22c499d8e684735252966c1/</guid>
      <description>HBase11xsqlwriter插件文档 1. 快速介绍 HBase11xsqlwriter实现了向hbase中的SQL表(phoenix)批量导入数据的功能。Phoenix因为对rowkey做了数据编码，所以，直接使用HBaseAPI进行写入会面临手工数据转换的问题，麻烦且易错。本插件提供了单间的SQL表的数据导入方式。
在底层实现上，通过Phoenix的JDBC驱动，执行UPSERT语句向hbase写入数据。
1.1 支持的功能 支持带索引的表的数据导入，可以同步更新所有的索引表 1.2 限制 仅支持1.x系列的hbase 仅支持通过phoenix创建的表，不支持原生HBase表 不支持带时间戳的数据导入 2. 实现原理 通过Phoenix的JDBC驱动，执行UPSERT语句向表中批量写入数据。因为使用上层接口，所以，可以同步更新索引表。
3. 配置说明 3.1 配置样例 { &amp;#34;job&amp;#34;: { &amp;#34;entry&amp;#34;: { &amp;#34;jvm&amp;#34;: &amp;#34;-Xms2048m -Xmx2048m&amp;#34; }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilereader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: &amp;#34;/Users/shf/workplace/datax_test/hbase11xsqlwriter/txt/normal.txt&amp;#34;, &amp;#34;charset&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;type&amp;#34;: &amp;#34;String&amp;#34; }, { &amp;#34;index&amp;#34;: 1, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 2, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 3, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; } ], &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase11xsqlwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;batchSize&amp;#34;: &amp;#34;256&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;UID&amp;#34;, &amp;#34;TS&amp;#34;, &amp;#34;EVENTID&amp;#34;, &amp;#34;CONTENT&amp;#34; ], &amp;#34;hbaseConfig&amp;#34;: { &amp;#34;hbase.zookeeper.quorum&amp;#34;: &amp;#34;目标hbase集群的ZK服务器地址，向PE咨询&amp;#34;, &amp;#34;zookeeper.znode.parent&amp;#34;: &amp;#34;目标hbase集群的znode，向PE咨询&amp;#34; }, &amp;#34;nullMode&amp;#34;: &amp;#34;skip&amp;#34;, &amp;#34;table&amp;#34;: &amp;#34;目标hbase表名，大小写有关&amp;#34; } } } ], &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 5 } } } } 3.</description>
    </item>
    
    <item>
      <title>hbase20xsqlreader  插件文档</title>
      <link>http://121.199.2.5:6080/d74e56a13f78424182e14164c8b4e5e4/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/d74e56a13f78424182e14164c8b4e5e4/</guid>
      <description>hbase20xsqlreader 插件文档 1 快速介绍 hbase20xsqlreader插件实现了从Phoenix(HBase SQL)读取数据，对应版本为HBase2.X和Phoenix5.X。
2 实现原理 简而言之，hbase20xsqlreader通过Phoenix轻客户端去连接Phoenix QueryServer，并根据用户配置信息生成查询SELECT 语句，然后发送到QueryServer读取HBase数据，并将返回结果使用DataX自定义的数据类型拼装为抽象的数据集，最终传递给下游Writer处理。
3 功能说明 3.1 配置样例 配置一个从Phoenix同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase20xsqlreader&amp;#34;, //指定插件为hbase20xsqlreader &amp;#34;parameter&amp;#34;: { &amp;#34;queryServerAddress&amp;#34;: &amp;#34;http://127.0.0.1:8765&amp;#34;, //填写连接Phoenix QueryServer地址 &amp;#34;serialization&amp;#34;: &amp;#34;PROTOBUF&amp;#34;, //QueryServer序列化格式 &amp;#34;table&amp;#34;: &amp;#34;TEST&amp;#34;, //读取表名 &amp;#34;column&amp;#34;: [&amp;#34;ID&amp;#34;, &amp;#34;NAME&amp;#34;], //所要读取列名 &amp;#34;splitKey&amp;#34;: &amp;#34;ID&amp;#34; //切分列，必须是表主键 } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;print&amp;#34;: true } } } ], &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: &amp;#34;3&amp;#34; } } } } 3.2 参数说明 queryServerAddress
描述：hbase20xsqlreader需要通过Phoenix轻客户端去连接Phoenix QueryServer，因此这里需要填写对应QueryServer地址。 增强版/Lindorm 用户若需透传user, password参数，可以在queryServerAddress后增加对应可选属性. 格式参考：http://127.0.0.1:8765;user=root;password=root
必选：是 默认值：无 serialization
描述：QueryServer使用的序列化协议
必选：否 默认值：PROTOBUF table
描述：所要读取表名
必选：是 默认值：无 schema
描述：表所在的schema
必选：否 默认值：无 column
描述：填写需要从phoenix表中读取的列名集合，使用JSON的数组描述字段信息，空值表示读取所有列。
必选： 否
默认值：全部列 splitKey</description>
    </item>
    
    <item>
      <title>HBase20xsqlwriter插件文档</title>
      <link>http://121.199.2.5:6080/28250260969448579bc8cee8c2963319/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/28250260969448579bc8cee8c2963319/</guid>
      <description>HBase20xsqlwriter插件文档 1. 快速介绍 HBase20xsqlwriter实现了向hbase中的SQL表(phoenix)批量导入数据的功能。Phoenix因为对rowkey做了数据编码，所以，直接使用HBaseAPI进行写入会面临手工数据转换的问题，麻烦且易错。本插件提供了SQL方式直接向Phoenix表写入数据。
在底层实现上，通过Phoenix QueryServer的轻客户端驱动，执行UPSERT语句向Phoenix写入数据。
1.1 支持的功能 支持带索引的表的数据导入，可以同步更新所有的索引表 1.2 限制 要求版本为Phoenix5.x及HBase2.x 仅支持通过Phoenix QeuryServer导入数据，因此您Phoenix必须启动QueryServer服务才能使用本插件 不支持清空已有表数据 仅支持通过phoenix创建的表，不支持原生HBase表 不支持带时间戳的数据导入 2. 实现原理 通过Phoenix轻客户端，连接Phoenix QueryServer服务，执行UPSERT语句向表中批量写入数据。因为使用上层接口，所以，可以同步更新索引表。
3. 配置说明 3.1 配置样例 { &amp;#34;job&amp;#34;: { &amp;#34;entry&amp;#34;: { &amp;#34;jvm&amp;#34;: &amp;#34;-Xms2048m -Xmx2048m&amp;#34; }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;txtfilereader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;path&amp;#34;: &amp;#34;/Users/shf/workplace/datax_test/hbase20xsqlwriter/txt/normal.txt&amp;#34;, &amp;#34;charset&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;column&amp;#34;: [ { &amp;#34;index&amp;#34;: 0, &amp;#34;type&amp;#34;: &amp;#34;String&amp;#34; }, { &amp;#34;index&amp;#34;: 1, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 2, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; }, { &amp;#34;index&amp;#34;: 3, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34; } ], &amp;#34;fieldDelimiter&amp;#34;: &amp;#34;,&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;hbase20xsqlwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;batchSize&amp;#34;: &amp;#34;100&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;UID&amp;#34;, &amp;#34;TS&amp;#34;, &amp;#34;EVENTID&amp;#34;, &amp;#34;CONTENT&amp;#34; ], &amp;#34;queryServerAddress&amp;#34;: &amp;#34;http://127.0.0.1:8765&amp;#34;, &amp;#34;nullMode&amp;#34;: &amp;#34;skip&amp;#34;, &amp;#34;table&amp;#34;: &amp;#34;目标hbase表名，大小写有关&amp;#34; } } } ], &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 5 } } } } 3.</description>
    </item>
    
    <item>
      <title>KingbaseesReader 插件文档</title>
      <link>http://121.199.2.5:6080/cc7bd03f72154435ae9af2d67a214f6d/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/cc7bd03f72154435ae9af2d67a214f6d/</guid>
      <description>KingbaseesReader 插件文档 1 快速介绍 KingbaseesReader插件实现了从KingbaseES读取数据。在底层实现上，KingbaseesReader通过JDBC连接远程KingbaseES数据库，并执行相应的sql语句将数据从KingbaseES库中SELECT出来。
2 实现原理 简而言之，KingbaseesReader通过JDBC连接器连接到远程的KingbaseES数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程KingbaseES数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
对于用户配置Table、Column、Where的信息，KingbaseesReader将其拼接为SQL语句发送到KingbaseES数据库；对于用户配置querySql信息，KingbaseesReader直接将其发送到KingbaseES数据库。
3 功能说明 3.1 配置样例 配置一个从KingbaseES数据库同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. &amp;#34;byte&amp;#34;: 1048576 }, //出错限制 &amp;#34;errorLimit&amp;#34;: { //出错的record条数上限，当大于该值即报错。 &amp;#34;record&amp;#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% &amp;#34;percentage&amp;#34;: 0.02 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;kingbaseesreader&amp;#34;, &amp;#34;parameter&amp;#34;: { // 数据库连接用户名 &amp;#34;username&amp;#34;: &amp;#34;xx&amp;#34;, // 数据库连接密码 &amp;#34;password&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;，&amp;#34;name&amp;#34; ], //切分主键 &amp;#34;splitPk&amp;#34;: &amp;#34;id&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;table&amp;#34;: [ &amp;#34;table&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:kingbase8://host:port/database&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { //writer类型 &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, //是否打印内容 &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: 1048576 }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;kingbaseesreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;where&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;querySql&amp;#34;: [ &amp;#34;select db_id,on_line_flag from db_info where db_id &amp;lt; 10;&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:kingbase8://host:port/database&amp;#34;, &amp;#34;jdbc:kingbase8://host:port/database&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;: false, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34; } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>MysqlReader 插件文档</title>
      <link>http://121.199.2.5:6080/29875f5b780c46929670f9b8699ed462/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/29875f5b780c46929670f9b8699ed462/</guid>
      <description>MysqlReader 插件文档 1 快速介绍 MysqlReader插件实现了从Mysql读取数据。在底层实现上，MysqlReader通过JDBC连接远程Mysql数据库，并执行相应的sql语句将数据从mysql库中SELECT出来。
不同于其他关系型数据库，MysqlReader不支持FetchSize.
2 实现原理 简而言之，MysqlReader通过JDBC连接器连接到远程的Mysql数据库，并根据用户配置的信息生成查询SELECT SQL语句，然后发送到远程Mysql数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
对于用户配置Table、Column、Where的信息，MysqlReader将其拼接为SQL语句发送到Mysql数据库；对于用户配置querySql信息，MysqlReader直接将其发送到Mysql数据库。
3 功能说明 3.1 配置样例 配置一个从Mysql数据库同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 3 }, &amp;#34;errorLimit&amp;#34;: { &amp;#34;record&amp;#34;: 0, &amp;#34;percentage&amp;#34;: 0.02 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;mysqlreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;, &amp;#34;name&amp;#34; ], &amp;#34;splitPk&amp;#34;: &amp;#34;db_id&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;table&amp;#34;: [ &amp;#34;table&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:mysql://127.0.0.1:3306/database&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;:true } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;:1 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;mysqlreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;querySql&amp;#34;: [ &amp;#34;select db_id,on_line_flag from db_info where db_id &amp;lt; 10;&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:mysql://bad_ip:3306/database&amp;#34;, &amp;#34;jdbc:mysql://127.</description>
    </item>
    
    <item>
      <title>OpenTSDBReader 插件文档</title>
      <link>http://121.199.2.5:6080/4e39a8595b6247fabfd6f625f053b0b6/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/4e39a8595b6247fabfd6f625f053b0b6/</guid>
      <description>OpenTSDBReader 插件文档 1 快速介绍 OpenTSDBReader 插件实现了从 OpenTSDB 读取数据。OpenTSDB 是主要由 Yahoo 维护的、可扩展的、分布式时序数据库，与阿里巴巴自研 TSDB 的关系与区别详见阿里云官网：《相比 OpenTSDB 优势》
2 实现原理 在底层实现上，OpenTSDBReader 通过 HTTP 请求链接到 OpenTSDB 实例，利用 /api/config 接口获取到其底层存储 HBase 的连接信息，再利用 AsyncHBase 框架连接 HBase，通过 Scan 的方式将数据点扫描出来。整个同步的过程通过 metric 和时间段进行切分，即某个 metric 在某一个小时内的数据迁移，组合成一个迁移 Task。
3 功能说明 3.1 配置样例 配置一个从 OpenTSDB 数据库同步抽取数据到本地的作业： { &amp;#34;job&amp;#34;: { &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;opentsdbreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;endpoint&amp;#34;: &amp;#34;http://localhost:4242&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;m&amp;#34; ], &amp;#34;beginDateTime&amp;#34;: &amp;#34;2019-01-01 00:00:00&amp;#34;, &amp;#34;endDateTime&amp;#34;: &amp;#34;2019-01-01 03:00:00&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;print&amp;#34;: true } } } ], &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } } } } 3.2 参数说明 name
描述：本插件的名称 必选：是 默认值：opentsdbreader parameter</description>
    </item>
    
    <item>
      <title>OracleReader 插件文档</title>
      <link>http://121.199.2.5:6080/0ffdd59e828b4bb0a53cef538910db12/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/0ffdd59e828b4bb0a53cef538910db12/</guid>
      <description>OracleReader 插件文档 1 快速介绍 OracleReader插件实现了从Oracle读取数据。在底层实现上，OracleReader通过JDBC连接远程Oracle数据库，并执行相应的sql语句将数据从Oracle库中SELECT出来。
2 实现原理 简而言之，OracleReader通过JDBC连接器连接到远程的Oracle数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程Oracle数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
对于用户配置Table、Column、Where的信息，OracleReader将其拼接为SQL语句发送到Oracle数据库；对于用户配置querySql信息，Oracle直接将其发送到Oracle数据库。
3 功能说明 3.1 配置样例 配置一个从Oracle数据库同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { //设置传输速度 byte/s 尽量逼近这个速度但是不高于它. // channel 表示通道数量，byte表示通道速度，如果单通道速度1MB，配置byte为1048576表示一个channel &amp;#34;byte&amp;#34;: 1048576 }, //出错限制 &amp;#34;errorLimit&amp;#34;: { //先选择record &amp;#34;record&amp;#34;: 0, //百分比 1表示100% &amp;#34;percentage&amp;#34;: 0.02 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;oraclereader&amp;#34;, &amp;#34;parameter&amp;#34;: { // 数据库连接用户名 &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, // 数据库连接密码 &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;,&amp;#34;name&amp;#34; ], //切分主键 &amp;#34;splitPk&amp;#34;: &amp;#34;db_id&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;table&amp;#34;: [ &amp;#34;table&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { //writer类型 &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, // 是否打印内容 &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;: true } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 5 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;oraclereader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;where&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;querySql&amp;#34;: [ &amp;#34;select db_id,on_line_flag from db_info where db_id &amp;lt; 10&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:oracle:thin:@[HOST_NAME]:PORT:[DATABASE_NAME]&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;visible&amp;#34;: false, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34; } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>OTSReader 插件文档</title>
      <link>http://121.199.2.5:6080/2dc5ba5f4d58498e8453bd2d8efde079/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/2dc5ba5f4d58498e8453bd2d8efde079/</guid>
      <description>OTSReader 插件文档 1 快速介绍 OTSReader插件实现了从OTS读取数据，并可以通过用户指定抽取数据范围可方便的实现数据增量抽取的需求。目前支持三种抽取方式：
全表抽取 范围抽取 指定分片抽取 OTS是构建在阿里云飞天分布式系统之上的 NoSQL数据库服务，提供海量结构化数据的存储和实时访问。OTS 以实例和表的形式组织数据，通过数据分片和负载均衡技术，实现规模上的无缝扩展。
2 实现原理 简而言之，OTSReader通过OTS官方Java SDK连接到OTS服务端，获取并按照DataX官方协议标准转为DataX字段信息传递给下游Writer端。
OTSReader会根据OTS的表范围，按照Datax并发的数目N，将范围等分为N份Task。每个Task都会有一个OTSReader线程来执行。
3 功能说明 3.1 配置样例 配置一个从OTS全表同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;otsreader&amp;#34;, &amp;#34;parameter&amp;#34;: { /* ----------- 必填 --------------*/ &amp;#34;endpoint&amp;#34;:&amp;#34;&amp;#34;, &amp;#34;accessId&amp;#34;:&amp;#34;&amp;#34;, &amp;#34;accessKey&amp;#34;:&amp;#34;&amp;#34;, &amp;#34;instanceName&amp;#34;:&amp;#34;&amp;#34;, // 导出数据表的表名 &amp;#34;table&amp;#34;:&amp;#34;&amp;#34;, // 需要导出的列名，支持重复列和常量列，区分大小写 // 常量列：类型支持STRING，INT，DOUBLE，BOOL和BINARY // 备注：BINARY需要通过Base64转换为对应的字符串传入插件 &amp;#34;column&amp;#34;:[ {&amp;#34;name&amp;#34;:&amp;#34;col1&amp;#34;}, // 普通列 {&amp;#34;name&amp;#34;:&amp;#34;col2&amp;#34;}, // 普通列 {&amp;#34;name&amp;#34;:&amp;#34;col3&amp;#34;}, // 普通列 {&amp;#34;type&amp;#34;:&amp;#34;STRING&amp;#34;, &amp;#34;value&amp;#34; : &amp;#34;bazhen&amp;#34;}, // 常量列(字符串) {&amp;#34;type&amp;#34;:&amp;#34;INT&amp;#34;, &amp;#34;value&amp;#34; : &amp;#34;&amp;#34;}, // 常量列(整形) {&amp;#34;type&amp;#34;:&amp;#34;DOUBLE&amp;#34;, &amp;#34;value&amp;#34; : &amp;#34;&amp;#34;}, // 常量列(浮点) {&amp;#34;type&amp;#34;:&amp;#34;BOOL&amp;#34;, &amp;#34;value&amp;#34; : &amp;#34;&amp;#34;}, // 常量列(布尔) {&amp;#34;type&amp;#34;:&amp;#34;BINARY&amp;#34;, &amp;#34;value&amp;#34; : &amp;#34;Base64(bin)&amp;#34;} // 常量列(二进制),使用Base64编码完成 ], &amp;#34;range&amp;#34;:{ // 导出数据的起始范围 // 支持INF_MIN, INF_MAX, STRING, INT &amp;#34;begin&amp;#34;:[ {&amp;#34;type&amp;#34;:&amp;#34;INF_MIN&amp;#34;}, ], // 导出数据的结束范围 // 支持INF_MIN, INF_MAX, STRING, INT &amp;#34;end&amp;#34;:[ {&amp;#34;type&amp;#34;:&amp;#34;INF_MAX&amp;#34;}, ] } } }, &amp;#34;writer&amp;#34;: {} } ] } } 配置一个定义抽取范围的OTSReader： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;byte&amp;#34;:10485760 }, &amp;#34;errorLimit&amp;#34;:0.</description>
    </item>
    
    <item>
      <title>OTSWriter 插件文档</title>
      <link>http://121.199.2.5:6080/7d2496e6853044daa824d7a78226b07d/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/7d2496e6853044daa824d7a78226b07d/</guid>
      <description>OTSWriter 插件文档 1 快速介绍 OTSWriter插件实现了向OTS写入数据，目前支持三种写入方式：
PutRow，对应于OTS API PutRow，插入数据到指定的行，如果该行不存在，则新增一行；若该行存在，则覆盖原有行。
UpdateRow，对应于OTS API UpdateRow，更新指定行的数据，如果该行不存在，则新增一行；若该行存在，则根据请求的内容在这一行中新增、修改或者删除指定列的值。
DeleteRow，对应于OTS API DeleteRow，删除指定行的数据。
OTS是构建在阿里云飞天分布式系统之上的 NoSQL数据库服务，提供海量结构化数据的存储和实时访问。OTS 以实例和表的形式组织数据，通过数据分片和负载均衡技术，实现规模上的无缝扩展。
2 实现原理 简而言之，OTSWriter通过OTS官方Java SDK连接到OTS服务端，并通过SDK写入OTS服务端。OTSWriter本身对于写入过程做了很多优化，包括写入超时重试、异常写入重试、批量提交等Feature。
3 功能说明 3.1 配置样例 配置一个写入OTS作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: {}, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;otswriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;endpoint&amp;#34;:&amp;#34;&amp;#34;, &amp;#34;accessId&amp;#34;:&amp;#34;&amp;#34;, &amp;#34;accessKey&amp;#34;:&amp;#34;&amp;#34;, &amp;#34;instanceName&amp;#34;:&amp;#34;&amp;#34;, // 导出数据表的表名 &amp;#34;table&amp;#34;:&amp;#34;&amp;#34;, // Writer支持不同类型之间进行相互转换 // 如下类型转换不支持: // ================================ // int -&amp;gt; binary // double -&amp;gt; bool, binary // bool -&amp;gt; binary // bytes -&amp;gt; int, double, bool // ================================ // 需要导入的PK列名，区分大小写 // 类型支持：STRING，INT // 1. 支持类型转换，注意类型转换时的精度丢失 // 2. 顺序不要求和表的Meta一致 &amp;#34;primaryKey&amp;#34; : [ {&amp;#34;name&amp;#34;:&amp;#34;pk1&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;string&amp;#34;}, {&amp;#34;name&amp;#34;:&amp;#34;pk2&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;int&amp;#34;} ], // 需要导入的列名，区分大小写 // 类型支持STRING，INT，DOUBLE，BOOL和BINARY &amp;#34;column&amp;#34; : [ {&amp;#34;name&amp;#34;:&amp;#34;col2&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;INT&amp;#34;}, {&amp;#34;name&amp;#34;:&amp;#34;col3&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;STRING&amp;#34;}, {&amp;#34;name&amp;#34;:&amp;#34;col4&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;STRING&amp;#34;}, {&amp;#34;name&amp;#34;:&amp;#34;col5&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;BINARY&amp;#34;}, {&amp;#34;name&amp;#34;:&amp;#34;col6&amp;#34;, &amp;#34;type&amp;#34;:&amp;#34;DOUBLE&amp;#34;} ], // 写入OTS的方式 // PutRow : 等同于OTS API中PutRow操作，检查条件是ignore // UpdateRow : 等同于OTS API中UpdateRow操作，检查条件是ignore // DeleteRow: 等同于OTS API中DeleteRow操作，检查条件是ignore &amp;#34;writeMode&amp;#34; : &amp;#34;PutRow&amp;#34; } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>PostgresqlReader 插件文档</title>
      <link>http://121.199.2.5:6080/0adf103bd70440a1978ecbcc7dd63787/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/0adf103bd70440a1978ecbcc7dd63787/</guid>
      <description>PostgresqlReader 插件文档 1 快速介绍 PostgresqlReader插件实现了从PostgreSQL读取数据。在底层实现上，PostgresqlReader通过JDBC连接远程PostgreSQL数据库，并执行相应的sql语句将数据从PostgreSQL库中SELECT出来。
2 实现原理 简而言之，PostgresqlReader通过JDBC连接器连接到远程的PostgreSQL数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程PostgreSQL数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
对于用户配置Table、Column、Where的信息，PostgresqlReader将其拼接为SQL语句发送到PostgreSQL数据库；对于用户配置querySql信息，PostgresqlReader直接将其发送到PostgreSQL数据库。
3 功能说明 3.1 配置样例 配置一个从PostgreSQL数据库同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { //设置传输速度，单位为byte/s，DataX运行会尽可能达到该速度但是不超过它. &amp;#34;byte&amp;#34;: 1048576 }, //出错限制 &amp;#34;errorLimit&amp;#34;: { //出错的record条数上限，当大于该值即报错。 &amp;#34;record&amp;#34;: 0, //出错的record百分比上限 1.0表示100%，0.02表示2% &amp;#34;percentage&amp;#34;: 0.02 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;postgresqlreader&amp;#34;, &amp;#34;parameter&amp;#34;: { // 数据库连接用户名 &amp;#34;username&amp;#34;: &amp;#34;xx&amp;#34;, // 数据库连接密码 &amp;#34;password&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34;，&amp;#34;name&amp;#34; ], //切分主键 &amp;#34;splitPk&amp;#34;: &amp;#34;id&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;table&amp;#34;: [ &amp;#34;table&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:postgresql://host:port/database&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { //writer类型 &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, //是否打印内容 &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;:true, } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: 1048576 }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;postgresqlreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;xx&amp;#34;, &amp;#34;where&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;querySql&amp;#34;: [ &amp;#34;select db_id,on_line_flag from db_info where db_id &amp;lt; 10;&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:postgresql://host:port/database&amp;#34;, &amp;#34;jdbc:postgresql://host:port/database&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;: false, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34; } } } ] } } 3.</description>
    </item>
    
    <item>
      <title>RDBMSReader 插件文档</title>
      <link>http://121.199.2.5:6080/34c79077064f4502a247f840e8b64c46/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/34c79077064f4502a247f840e8b64c46/</guid>
      <description>RDBMSReader 插件文档 1 快速介绍 RDBMSReader插件实现了从RDBMS读取数据。在底层实现上，RDBMSReader通过JDBC连接远程RDBMS数据库，并执行相应的sql语句将数据从RDBMS库中SELECT出来。目前支持达梦、db2、PPAS、Sybase数据库的读取。RDBMSReader是一个通用的关系数据库读插件，您可以通过注册数据库驱动等方式增加任意多样的关系数据库读支持。
2 实现原理 简而言之，RDBMSReader通过JDBC连接器连接到远程的RDBMS数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程RDBMS数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
对于用户配置Table、Column、Where的信息，RDBMSReader将其拼接为SQL语句发送到RDBMS数据库；对于用户配置querySql信息，RDBMS直接将其发送到RDBMS数据库。
3 功能说明 3.1 配置样例 配置一个从RDBMS数据库同步抽取数据作业: {&amp;#34;job&amp;#34;: {&amp;#34;setting&amp;#34;: {&amp;#34;speed&amp;#34;: {&amp;#34;byte&amp;#34;: 1048576},&amp;#34;errorLimit&amp;#34;: {&amp;#34;record&amp;#34;: 0,&amp;#34;percentage&amp;#34;: 0.02}},&amp;#34;content&amp;#34;: [{&amp;#34;reader&amp;#34;: {&amp;#34;name&amp;#34;: &amp;#34;rdbmsreader&amp;#34;,&amp;#34;parameter&amp;#34;: {&amp;#34;username&amp;#34;: &amp;#34;xxx&amp;#34;,&amp;#34;password&amp;#34;: &amp;#34;xxx&amp;#34;,&amp;#34;column&amp;#34;: [&amp;#34;id&amp;#34;,&amp;#34;name&amp;#34;],&amp;#34;splitPk&amp;#34;: &amp;#34;pk&amp;#34;,&amp;#34;connection&amp;#34;: [{&amp;#34;table&amp;#34;: [&amp;#34;table&amp;#34;],&amp;#34;jdbcUrl&amp;#34;: [&amp;#34;jdbc:dm://ip:port/database&amp;#34;]}],&amp;#34;fetchSize&amp;#34;: 1024,&amp;#34;where&amp;#34;: &amp;#34;1 = 1&amp;#34;}},&amp;#34;writer&amp;#34;: {&amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;,&amp;#34;parameter&amp;#34;: {&amp;#34;print&amp;#34;: true}}}]}} 配置一个自定义SQL的数据库同步任务到ODPS的作业： {&amp;#34;job&amp;#34;: {&amp;#34;setting&amp;#34;: {&amp;#34;speed&amp;#34;: {&amp;#34;byte&amp;#34;: 1048576},&amp;#34;errorLimit&amp;#34;: {&amp;#34;record&amp;#34;: 0,&amp;#34;percentage&amp;#34;: 0.</description>
    </item>
    
    <item>
      <title>RDBMSWriter 插件文档</title>
      <link>http://121.199.2.5:6080/5d9d57b2e00d4d8ca994dc15ece38148/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/5d9d57b2e00d4d8ca994dc15ece38148/</guid>
      <description>RDBMSWriter 插件文档 1 快速介绍 RDBMSWriter 插件实现了写入数据到 RDBMS 主库的目的表的功能。在底层实现上， RDBMSWriter 通过 JDBC 连接远程 RDBMS 数据库，并执行相应的 insert into &amp;hellip; 的 sql 语句将数据写入 RDBMS。 RDBMSWriter是一个通用的关系数据库写插件，您可以通过注册数据库驱动等方式增加任意多样的关系数据库写支持。
RDBMSWriter 面向ETL开发工程师，他们使用 RDBMSWriter 从数仓导入数据到 RDBMS。同时 RDBMSWriter 亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 RDBMSWriter 通过 DataX 框架获取 Reader 生成的协议数据，RDBMSWriter 通过 JDBC 连接远程 RDBMS 数据库，并执行相应的 insert into &amp;hellip; 的 sql 语句将数据写入 RDBMS。
3 功能说明 3.1 配置样例 配置一个写入RDBMS的作业。 {&amp;#34;job&amp;#34;: {&amp;#34;setting&amp;#34;: {&amp;#34;speed&amp;#34;: {&amp;#34;channel&amp;#34;: 1}},&amp;#34;content&amp;#34;: [{&amp;#34;reader&amp;#34;: {&amp;#34;name&amp;#34;: &amp;#34;streamreader&amp;#34;,&amp;#34;parameter&amp;#34;: {&amp;#34;column&amp;#34;: [{&amp;#34;value&amp;#34;: &amp;#34;DataX&amp;#34;,&amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;},{&amp;#34;value&amp;#34;: 19880808,&amp;#34;type&amp;#34;: &amp;#34;long&amp;#34;},{&amp;#34;value&amp;#34;: &amp;#34;1988-08-08 08:08:08&amp;#34;,&amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;},{&amp;#34;value&amp;#34;: true,&amp;#34;type&amp;#34;: &amp;#34;bool&amp;#34;},{&amp;#34;value&amp;#34;: &amp;#34;test&amp;#34;,&amp;#34;type&amp;#34;: &amp;#34;bytes&amp;#34;}],&amp;#34;sliceRecordCount&amp;#34;: 1000}},&amp;#34;writer&amp;#34;: {&amp;#34;name&amp;#34;: &amp;#34;rdbmswriter&amp;#34;,&amp;#34;parameter&amp;#34;: {&amp;#34;connection&amp;#34;: [{&amp;#34;jdbcUrl&amp;#34;: &amp;#34;jdbc:dm://ip:port/database&amp;#34;,&amp;#34;table&amp;#34;: [&amp;#34;table&amp;#34;]}],&amp;#34;username&amp;#34;: &amp;#34;username&amp;#34;,&amp;#34;password&amp;#34;: &amp;#34;password&amp;#34;,&amp;#34;table&amp;#34;: &amp;#34;table&amp;#34;,&amp;#34;column&amp;#34;: [&amp;#34;*&amp;#34;],&amp;#34;preSql&amp;#34;: [&amp;#34;delete from XXX;&amp;#34;]}}}]}} 3.</description>
    </item>
    
    <item>
      <title>Readme.md</title>
      <link>http://121.199.2.5:6080/b007b0bc6dca40458f17c7c1826e9da5/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/b007b0bc6dca40458f17c7c1826e9da5/</guid>
      <description>some script here.</description>
    </item>
    
    <item>
      <title>README.md</title>
      <link>http://121.199.2.5:6080/9dbd1273dcb848bbaa69f53c0105d5c4/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/9dbd1273dcb848bbaa69f53c0105d5c4/</guid>
      <description>本插件仅在Elasticsearch 5.x上测试</description>
    </item>
    
    <item>
      <title>SqlServerReader 插件文档</title>
      <link>http://121.199.2.5:6080/e30dc23cda1d465fa0872475e5c976a7/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/e30dc23cda1d465fa0872475e5c976a7/</guid>
      <description>SqlServerReader 插件文档 1 快速介绍 SqlServerReader插件实现了从SqlServer读取数据。在底层实现上，SqlServerReader通过JDBC连接远程SqlServer数据库，并执行相应的sql语句将数据从SqlServer库中SELECT出来。
2 实现原理 简而言之，SqlServerReader通过JDBC连接器连接到远程的SqlServer数据库，并根据用户配置的信息生成查询SELECT SQL语句并发送到远程SqlServer数据库，并将该SQL执行返回结果使用DataX自定义的数据类型拼装为抽象的数据集，并传递给下游Writer处理。
对于用户配置Table、Column、Where的信息，SqlServerReader将其拼接为SQL语句发送到SqlServer数据库；对于用户配置querySql信息，SqlServer直接将其发送到SqlServer数据库。
3 功能说明 3.1 配置样例 配置一个从SqlServer数据库同步抽取数据到本地的作业: { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;byte&amp;#34;: 1048576 } }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;sqlserverreader&amp;#34;, &amp;#34;parameter&amp;#34;: { // 数据库连接用户名 &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, // 数据库连接密码 &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;id&amp;#34; ], &amp;#34;splitPk&amp;#34;: &amp;#34;db_id&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;table&amp;#34;: [ &amp;#34;table&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:sqlserver://localhost:3433;DatabaseName=dbname&amp;#34; ] } ] } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;print&amp;#34;: true, &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34; } } } ] } } 配置一个自定义SQL的数据库同步任务到本地内容的作业： { &amp;#34;job&amp;#34;: { &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: 1048576 }, &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;sqlserverreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;username&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;root&amp;#34;, &amp;#34;where&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;connection&amp;#34;: [ { &amp;#34;querySql&amp;#34;: [ &amp;#34;select db_id,on_line_flag from db_info where db_id &amp;lt; 10;&amp;#34; ], &amp;#34;jdbcUrl&amp;#34;: [ &amp;#34;jdbc:sqlserver://bad_ip:3433;DatabaseName=dbname&amp;#34;, &amp;#34;jdbc:sqlserver://127.</description>
    </item>
    
    <item>
      <title>TableStore增量数据导出通道：TableStoreStreamReader</title>
      <link>http://121.199.2.5:6080/6851df08c78c4eb1aab41d312b920bf9/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/6851df08c78c4eb1aab41d312b920bf9/</guid>
      <description>TableStore增量数据导出通道：TableStoreStreamReader 快速介绍 TableStoreStreamReader插件主要用于TableStore的增量数据导出，增量数据可以看作操作日志，除了数据本身外还附有操作信息。
与全量导出插件不同，增量导出插件只有多版本模式，同时不支持指定列。这是与增量导出的原理有关的，导出的格式下面有详细介绍。
使用插件前必须确保表上已经开启Stream功能，可以在建表的时候指定开启，或者使用SDK的UpdateTable接口开启。
开启Stream的方法： SyncClient client = new SyncClient(&amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;); 1. 建表的时候开启： CreateTableRequest createTableRequest = new CreateTableRequest(tableMeta); createTableRequest.setStreamSpecification(new StreamSpecification(true, 24)); // 24代表增量数据保留24小时 client.createTable(createTableRequest); 2. 如果建表时未开启，可以通过UpdateTable开启: UpdateTableRequest updateTableRequest = new UpdateTableRequest(&amp;quot;tableName&amp;quot;); updateTableRequest.setStreamSpecification(new StreamSpecification(true, 24)); client.updateTable(updateTableRequest); 实现原理 首先用户使用SDK的UpdateTable功能，指定开启Stream并设置过期时间，即开启了增量功能。
开启后，TableStore服务端就会将用户的操作日志额外保存起来， 每个分区有一个有序的操作日志队列，每条操作日志会在一定时间后被垃圾回收，这个时间即用户指定的过期时间。
TableStore的SDK提供了几个Stream相关的API用于将这部分操作日志读取出来，增量插件也是通过TableStore SDK的接口获取到增量数据的，并将 增量数据转化为多个6元组的形式(pk, colName, version, colValue, opType, sequenceInfo)导入到ODPS中。
Reader的配置模版： &amp;quot;reader&amp;quot;: { &amp;quot;name&amp;quot; : &amp;quot;otsstreamreader&amp;quot;, &amp;quot;parameter&amp;quot; : { &amp;quot;endpoint&amp;quot; : &amp;quot;&amp;quot;, &amp;quot;accessId&amp;quot; : &amp;quot;&amp;quot;, &amp;quot;accessKey&amp;quot; : &amp;quot;&amp;quot;, &amp;quot;instanceName&amp;quot; : &amp;quot;&amp;quot;, //dataTable即需要导出数据的表。 &amp;quot;dataTable&amp;quot; : &amp;quot;&amp;quot;, //statusTable是Reader用于保存状态的表，若该表不存在，Reader会自动创建该表。 //一次离线导出任务完成后，用户不应删除该表，该表中记录的状态可用于下次导出任务中。 &amp;quot;statusTable&amp;quot; : &amp;quot;TableStoreStreamReaderStatusTable&amp;quot;, //增量数据的时间范围（左闭右开）的左边界。 &amp;quot;startTimestampMillis&amp;quot; : &amp;quot;&amp;quot;, //增量数据的时间范围（左闭右开）的右边界。 &amp;quot;endTimestampMillis&amp;quot; : &amp;quot;&amp;quot;, //采云间调度只支持天级别，所以提供该配置，作用与startTimestampMillis和endTimestampMillis类似。 &amp;quot;date&amp;quot;: &amp;quot;&amp;quot;, //是否导出时序信息。 &amp;quot;isExportSequenceInfo&amp;quot;: true, //从TableStore中读增量数据时，每次请求的最大重试次数，默认为30。 &amp;quot;maxRetries&amp;quot; : 30 } } 参数说明 名称 说明 类型 必选 endpoint TableStoreServer的Endpoint地址。 String 是 accessId 用于访问TableStore服务的accessId。 String 是 accessKey 用于访问TableStore服务的accessKey。 String 是 instanceName TableStore的实例名称。 String 是 dataTable 需要导出增量数据的表的名称。该表需要开启Stream，可以在建表时开启，或者使用UpdateTable接口开启。 String 是 statusTable Reader插件用于记录状态的表的名称，这些状态可用于减少对非目标范围内的数据的扫描，从而加快导出速度。 1.</description>
    </item>
    
    <item>
      <title>TSDBReader 插件文档</title>
      <link>http://121.199.2.5:6080/66980d1599cc4aec9a2d6616d1f0c5e7/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/66980d1599cc4aec9a2d6616d1f0c5e7/</guid>
      <description>TSDBReader 插件文档 1 快速介绍 TSDBReader 插件实现了从阿里云 TSDB 读取数据。阿里云时间序列数据库 ( Time Series Database , 简称 TSDB) 是一种集时序数据高效读写，压缩存储，实时计算能力为一体的数据库服务，可广泛应用于物联网和互联网领域，实现对设备及业务服务的实时监控，实时预测告警。详见 TSDB 的阿里云官网。
2 实现原理 在底层实现上，TSDBReader 通过 HTTP 请求链接到 阿里云 TSDB 实例，利用 /api/query 或者 /api/mquery 接口将数据点扫描出来（更多细节详见：时序数据库 TSDB - HTTP API 概览）。而整个同步的过程，是通过时间线和查询时间线范围进行切分。
3 功能说明 3.1 配置样例 配置一个从 阿里云 TSDB 数据库同步抽取数据到本地的作业，并以时序数据的格式输出： 时序数据样例：
{&amp;#34;metric&amp;#34;:&amp;#34;m&amp;#34;,&amp;#34;tags&amp;#34;:{&amp;#34;app&amp;#34;:&amp;#34;a19&amp;#34;,&amp;#34;cluster&amp;#34;:&amp;#34;c5&amp;#34;,&amp;#34;group&amp;#34;:&amp;#34;g10&amp;#34;,&amp;#34;ip&amp;#34;:&amp;#34;i999&amp;#34;,&amp;#34;zone&amp;#34;:&amp;#34;z1&amp;#34;},&amp;#34;timestamp&amp;#34;:1546272263,&amp;#34;value&amp;#34;:1} { &amp;#34;job&amp;#34;: { &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;tsdbreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;sinkDbType&amp;#34;: &amp;#34;TSDB&amp;#34;, &amp;#34;endpoint&amp;#34;: &amp;#34;http://localhost:8242&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;m&amp;#34; ], &amp;#34;splitIntervalMs&amp;#34;: 60000, &amp;#34;beginDateTime&amp;#34;: &amp;#34;2019-01-01 00:00:00&amp;#34;, &amp;#34;endDateTime&amp;#34;: &amp;#34;2019-01-01 01:00:00&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;streamwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;encoding&amp;#34;: &amp;#34;UTF-8&amp;#34;, &amp;#34;print&amp;#34;: true } } } ], &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 3 } } } } 配置一个从 阿里云 TSDB 数据库同步抽取数据到本地的作业，并以关系型数据的格式输出： 关系型数据样例：</description>
    </item>
    
    <item>
      <title>TSDBWriter 插件文档</title>
      <link>http://121.199.2.5:6080/2a52431a701d4468a150a7986b3b4752/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/2a52431a701d4468a150a7986b3b4752/</guid>
      <description>TSDBWriter 插件文档 1 快速介绍 TSDBWriter 插件实现了将数据点写入到阿里巴巴自研 TSDB 数据库中（后续简称 TSDB）。
时间序列数据库（Time Series Database , 简称 TSDB）是一种高性能，低成本，稳定可靠的在线时序数据库服务；提供高效读写，高压缩比存储、时序数据插值及聚合计算，广泛应用于物联网（IoT）设备监控系统 ，企业能源管理系统（EMS），生产安全监控系统，电力检测系统等行业场景。 TSDB 提供百万级时序数据秒级写入，高压缩比低成本存储、预降采样、插值、多维聚合计算，查询结果可视化功能；解决由于设备采集点数量巨大，数据采集频率高，造成的存储成本高，写入和查询分析效率低的问题。更多关于 TSDB 的介绍，详见阿里云 TSDB 官网。
2 实现原理 通过 HTTP 连接 TSDB 实例，并通过 /api/put 接口将数据点写入。关于写入接口详见 TSDB 的接口说明文档。
3 功能说明 3.1 配置样例 配置一个从 OpenTSDB 数据库同步抽取数据到 TSDB： { &amp;#34;job&amp;#34;: { &amp;#34;content&amp;#34;: [ { &amp;#34;reader&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;opentsdbreader&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;endpoint&amp;#34;: &amp;#34;http://localhost:4242&amp;#34;, &amp;#34;column&amp;#34;: [ &amp;#34;m&amp;#34; ], &amp;#34;startTime&amp;#34;: &amp;#34;2019-01-01 00:00:00&amp;#34;, &amp;#34;endTime&amp;#34;: &amp;#34;2019-01-01 03:00:00&amp;#34; } }, &amp;#34;writer&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;tsdbhttpwriter&amp;#34;, &amp;#34;parameter&amp;#34;: { &amp;#34;endpoint&amp;#34;: &amp;#34;http://localhost:8242&amp;#34; } } } ], &amp;#34;setting&amp;#34;: { &amp;#34;speed&amp;#34;: { &amp;#34;channel&amp;#34;: 1 } } } } 3.2 参数说明 name
描述：本插件的名称 必选：是 默认值：tsdbhttpwriter parameter
endpoint 描述：TSDB 的 HTTP 连接地址 必选：是 格式：http://IP:Port 默认值：无 batchSize</description>
    </item>
    
    <item>
      <title>阿里云开源离线同步工具DataX3.0介绍</title>
      <link>http://121.199.2.5:6080/408be8b90df543789f79c7b43375e5a3/</link>
      <pubDate>Tue, 02 Feb 2021 17:45:01 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/408be8b90df543789f79c7b43375e5a3/</guid>
      <description>阿里云开源离线同步工具DataX3.0介绍 一. DataX3.0概览 ​	DataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。
设计理念 为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。
当前使用现状 DataX在阿里巴巴集团内被广泛使用，承担了所有大数据的离线同步业务，并已持续稳定运行了6年之久。目前每天完成同步8w多道作业，每日传输数据量超过300TB。
此前已经开源DataX1.0版本，此次介绍为阿里云开源全新版本DataX3.0，有了更多更强大的功能和更好的使用体验。Github主页地址：https://github.com/alibaba/DataX
二、DataX3.0框架设计 DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。
Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework。 Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。 Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。 三. DataX3.0插件体系 ​	经过几年积累，DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入。DataX目前支持数据如下：
类型 数据源 Reader(读) Writer(写) 文档 RDBMS 关系型数据库 MySQL √ √ 读 、写 Oracle √ √ 读 、写 SQLServer √ √ 读 、写 PostgreSQL √ √ 读 、写 DRDS √ √ 读 、写 达梦 √ √ 读 、写 通用RDBMS(支持所有关系型数据库) √ √ 读 、写 阿里云数仓数据存储 ODPS √ √ 读 、写 ADS √ 写 OSS √ √ 读 、写 OCS √ √ 读 、写 NoSQL数据存储 OTS √ √ 读 、写 Hbase0.94 √ √ 读 、写 Hbase1.1 √ √ 读 、写 MongoDB √ √ 读 、写 Hive √ √ 读 、写 无结构化数据存储 TxtFile √ √ 读 、写 FTP √ √ 读 、写 HDFS √ √ 读 、写 Elasticsearch √ 写 DataX Framework提供了简单的接口与插件交互，提供简单的插件接入机制，只需要任意加上一种插件，就能无缝对接其他数据源。详情请看：DataX数据源指南</description>
    </item>
    
    <item>
      <title>mysql replace </title>
      <link>http://121.199.2.5:6080/nfvw4R/</link>
      <pubDate>Mon, 01 Feb 2021 09:48:42 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/nfvw4R/</guid>
      <description>mysql replace 可以将字段的字符串内容替换，语法：
Update `table_name` SET `field_name` = replace (`field_name`,’from_str’,&amp;#39;to_str’) Where `field_name` LIKE ‘%from_str%’ 使用样例：
# 将 blog 表的字段 coentent 里的 &amp;#34;https://haokiu.com&amp;#34; 替换为 &amp;#34;http://haokiu.com&amp;#34; update blog set content = replace(content, &amp;#34;https://haokiu.com&amp;#34;, &amp;#34;http://haokiu.com&amp;#34;) where content like &amp;#39;%https://haokiu.com%&amp;#39; </description>
    </item>
    
    <item>
      <title>Vim 从入门到精通</title>
      <link>http://121.199.2.5:6080/fhRygg/</link>
      <pubDate>Thu, 14 Jan 2021 15:20:19 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/fhRygg/</guid>
      <description>什么是 Vim？ Vim 是一个历史悠久的文本编辑器，可以追溯到 qed。 Bram Moolenaar 于 1991 年发布初始版本。
Linux、Mac 用户，可以使用包管理器安装 Vim，对于 Windows 用户，可以从 我的网盘 下载。 该版本可轻易添加 python 、python3 、lua 等支持，只需要安装 python、lua 即可。
项目在 Github 上开发，项目讨论请订阅 vim_dev 邮件列表。
通过阅读 Why, oh WHY, do those #?@! nutheads use vi? 来对 Vim 进行大致的了解。
Vim 哲学 Vim 采用模式编辑的理念，即它提供了多种模式，按键在不同的模式下作用不同。 你可以在普通模式 下浏览文件，在插入模式下插入文本， 在可视模式下选择行，在命令模式下执行命令等等。起初这听起来可能很复杂， 但是这有一个很大的优点：不需要通过同时按住多个键来完成操作， 大多数时候你只需要依次按下这些按键即可。越常用的操作，所需要的按键数量越少。
和模式编辑紧密相连的概念是 操作符 和 动作。操作符 指的是开始某个行为， 例如：修改、删除或者选择文本，之后你要用一个 动作 来指定需要操作的文本区域。 比如，要改变括号内的文本，需要执行 ci( （读做 change inner parentheses）； 删除整个段落的内容，需要执行 dap （读做：delete around paragraph）。
如果你能看见 Vim 老司机操作，你会发现他们使用 Vim 脚本语言就如同钢琴师弹钢琴一样。复杂的操作只需要几个按键就能完成。他们甚至不用刻意去想，因为这已经成为肌肉记忆了。这减少认识负荷并帮助人们专注于实际任务。
入门 Vim 自带一个交互式的教程，内含你需要了解的最基础的信息，你可以通过终端运行以下命令打开教程：
$ vimtutor 不要因为这个看上去很无聊而跳过，按照此教程多练习。你以前用的 IDE 或者其他编辑器很少是有“模式”概念的，因此一开始你会很难适应模式切换。但是你 Vim 使用的越多，肌肉记忆 将越容易形成。
Vim 基于一个 vi 克隆，叫做 Stevie，支持两种运行模式：&amp;ldquo;compatible&amp;rdquo; 和 &amp;ldquo;nocompatible&amp;rdquo;。在兼容模式下运行 Vim 意味着使用 vi 的默认设置，而不是 Vim 的默认设置。除非你新建一个用户的 vimrc 或者使用 vim -N 命令启动 Vim，否则就是在兼容模式下运行 Vim！请大家不要在兼容模式下运行 Vim。</description>
    </item>
    
    <item>
      <title>购买比特币</title>
      <link>http://121.199.2.5:6080/0PyB8g/</link>
      <pubDate>Mon, 11 Jan 2021 09:44:36 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/0PyB8g/</guid>
      <description>数字货币的交易所有很多。
比特儿 邀请注册：https://www.gate.io/signup/633017 ，点击这个链接注册的用户可以获取90$的交易点卡哦。
gate.io 8年长期稳定运营，行业口碑品牌，安全可靠，注册即可获得手续费优惠!
Gate.io 作为前十的交易所它最吸引人的地方是秒充秒提，在速度上要比其他交易所好很多。同时它还独有地址共享技术，充错币的情况也不会发生。除此之外他们平台的活动也很多，不是只有交易才能获得奖励，写一句话写篇文章都是可以获得奖励的，类似的活动非常多。所以总的来说，Gate.io交易所还是很不错的。
Gate.io 上可以购买的币种也非常多，可以购买比特币、购买eth，购买dot，所有主流的数字货币都可以购买。
最后一句提醒各位，数字货币价格波动很大，投资须谨慎！</description>
    </item>
    
    <item>
      <title>docker apt-get</title>
      <link>http://121.199.2.5:6080/d5nhvK/</link>
      <pubDate>Tue, 24 Nov 2020 13:56:48 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/d5nhvK/</guid>
      <description>为了节省空间，docker 容器里面有很多命令是没有安装的，提示： Reading package lists&amp;hellip; Done Building dependency tree Reading state information&amp;hellip; Done E: Unable to locate package vim
更新apt-get源 这时候需要敲：apt-get update
这个命令的作用是：同步 /etc/apt/sources.list 和 /etc/apt/sources.list.d 中列出的源的索引，这样才能获取到最新的软件包。
安装vim apt-get install vim
安装telnet apt-get install telnet
安装ifconfig apt-get install net-tools</description>
    </item>
    
    <item>
      <title>git 查看远程地址</title>
      <link>http://121.199.2.5:6080/ss9sfI/</link>
      <pubDate>Mon, 09 Nov 2020 09:25:45 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/ss9sfI/</guid>
      <description>有时需要查看本地开发的git项目的远程仓库地址
git remote -v 可以方便的查看远程仓库的地址
git remote -v origin https://gitee.com/leonardodacn/pixiublog.git (fetch) origin https://gitee.com/leonardodacn/pixiublog.git (push) -v : verbose，冗余的意思</description>
    </item>
    
    <item>
      <title>OLAP</title>
      <link>http://121.199.2.5:6080/OLAP/</link>
      <pubDate>Thu, 05 Nov 2020 10:53:39 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/OLAP/</guid>
      <description>OLAP 在数据仓库一章中我们讨论了数据仓库及其模型的概念，而数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别。操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发的支持用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题历史数据进行分析，支持管理决策。
OLAP 与 OLTP 在互联网浪潮出现之前，企业的数据量普遍不大，特别是核心的业务数据，通常一个单机的数据库就可以保存。那时候的存储并不需要复杂的架构，所有的线上请求(OLTP, Online Transactional Processing) 和后台分析 (OLAP, Online Analytical Processing) 都跑在同一个数据库实例上。后来渐渐的业务越来越复杂，数据量越来越大，DBA 们再也优化不动 SQL 了。其中一个显著问题是：单机数据库支持线上的 TP 请求已经非常吃力，没办法再跑比较重的 AP 分析型任务。跑起来要么 OOM，要么影响线上业务，要么做了主从分离、分库分表之后很难实现业务需求。
在这样的背景下，以 Hadoop 为代表的大数据技术开始蓬勃发展，它用许多相对廉价的 x86 机器构建了一个数据分析平台，用并行的能力破解大数据集的计算问题。所以从某种程度上说，大数据技术可以算是传统关系型数据库技术发展过程的一个分支。当然在过程中大数据领域也发展出了属于自己的全新场景，诞生了许多新的技术，这个不深入提了。
由此，架构师把存储划分成线上业务和数据分析两个模块。如下图所示，业务库的数据通过 ETL 工具抽取出来，导入专用的分析平台。业务数据库专注提供 TP 能力，分析平台提供 AP 能力，各施其职，看起来已经很完美了。但其实这个架构也有自己的不足。
HTAP 首先是复杂性问题。本身 ETL 过程就是一个很繁琐的过程，一个例证是 ETL 做的好，可以成为一个商业模式。因为是两个系统，必然带来更高的学习成本、维护成本和整合成本。如果你使用的是开源的大数据工具搭建的分析平台，那么肯定会遇到各种工具之间的磨合的问题，还有由于各种工具良莠不齐所导致的质量问题。
其次是实时性问题。通常我们认为越接近实时的数据，它的价值越大。很多业务场景，对实时性有很高的要求，比如风控系统，它需要对数据不停的分析，并且在险情出现之后尽快响应。而通常的 ETL 是一个周期性的操作，比如一天或者一个小时导一次数据，数据实时性是没有办法保证的。最后是一致性问题。一致性在数据库里面是很重要的概念，数据库的事务就是用来保证一致性的。如果把数据分表存储在两个不同的系统内，那么很难保证一致性，即 AP 系统的查询结果没有办法与线上业务正确对应。那么这两个系统的联动效应就会受到限制，比如用户没办法在一个事务里面，同时访问两个系统的数据。
由于现有的数据平台存在的以上局限性，我们认为开发一个HTAP（Hybrid Transactional/Analytical Processing）融合型数据库产品可以缓解大家在 TP or AP 抉择上的焦虑，或者说，让数据库的使用者不用考虑过度复杂的架构，在一套数据库中既能满足 OLTP 类需求，也能满足 OLAP 类需求。</description>
    </item>
    
    <item>
      <title>数据中台</title>
      <link>http://121.199.2.5:6080/3OKmkr/</link>
      <pubDate>Thu, 05 Nov 2020 10:50:55 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/3OKmkr/</guid>
      <description>数据中台 阿里在 2018 年提出了所谓“数据中台”的概念：即数据被统一采集，规范数据语义和业务口径形成企业基础数据模型，提供统一的分析查询和新业务的数据对接能力。数据中台并不是新的颠覆式技术，而是一种企业数据资产管理和应用方法学，涵盖了数据集成、数据质量管理、元数据与主数据管理、数仓建模、支持高并发访问的数据服务接口层开发等内容。
在数据中台建设中，结合企业自身的业务需求特点，架构和功能可能各不相同，但其中一个最基本的需求是数据采集的实时性和完整性。数据从源端产生，到被采集到数据汇集层的时间要尽可能短，至少应做到秒级延迟，这样中台的数据模型更新才可能做到近实时，构建在中台之上依赖实时数据流驱动的应用（例如商品推荐、欺诈检测等）才能够满足业务的需求。
以阿里双十一为例，在极高的并发情况下，订单产生到大屏统计数据更新延迟不能超过 5s，一般在 2s 内。中台对外提供的数据应该是完整的，源端数据的 Create、Update 和 Delete 都要能够被捕获，不能少也不能多，即数据需要有端到端一致性的能力（Exactly Once Semantic，EOS）。当然，EOS 并非在任何业务场景下都需要，但从平台角度必须具备这种能力，并且允许用户根据业务需求灵活开启和关闭。
数据中台的产生背景 起初，企业只有一个主营业务，比如电商，但随着公司战略和发展需要，会新增多支业务线，由于存在负责业务线开发的团队不一致，随之而来的就是风格迥异的代码风格和数据烟囱问题。
数据中台的产生就是为了解决数据烟囱的问题，打通数据孤岛，让数据活起来，让数据产生价值，结合前台能力，达到快速响应用户的目标。
中台只会同步能服务于超过两个业务线的数据，如果仅仅带有自身业务属性(不存在共性)的数据，不在中台的考虑范围内。例如:电商的产品产地信息，对于金融业务来说，其实是没有价值的，但电商的用户收货地址对金融业务来说是有价值的。所以不要简单的认为数据中台会汇集企业的所有数据，还是有侧重点的。导致这个结果的原因还包括数据中台建设本身是一个长周期的事，如果数据仅仅作用于一方，由业务方(前台)自行开发，更符合敏捷开发的特性。
关于何时应该建立数据中台这个问题，我的思考是这样的。复杂的业务线、丰富的数据维度和公司上层领导主推。三者缺一，都没有实行的必要。 一只手都能数的过来的业务线量，跨多个项目的需求相对还是比较少的，取数也比较方便，直接走接口方式基本就能满足。反而，通过数据中台流转，将问题复杂化了。
数据的维度越丰富，数据的价值越大。只知道性别数据，与知道性别和年龄，所得到的用户画像，肯定是维度丰富的准确性高。维度不丰富的情况下，没有计算的价值。
可能会很奇怪为什么一定需要公司上层的同意。这里就可能涉及到动了谁的奶酪的问题，数据是每个业务线最重要的资源，在推行中台过程中，势必会遇到阻力，只有成为全公司的战略任务，才有可能把事情做好。
中台如果没有考虑通用的业务能力，也会导致无法更专注于对中台技术的深入研究。中台如果不从抽象度、共性等角度出发，很有可能局限于某单一业务，导致中台无法很好地适应其他相关业务的要求，从而不能很好地应对业务的变化。如果中台的抽象程度低、扩展性差，则会导致中台无法满足前台业务需求。这时前台应用又因为业务本身的发展目标和压力不得不自行组织团队完成这部分功能，由此可能发生本应由中台提供的能力却最终实现在业务应用中，失去了中台存在的价值。</description>
    </item>
    
    <item>
      <title>简单脚本实现服务监控</title>
      <link>http://121.199.2.5:6080/5XKHkb/</link>
      <pubDate>Fri, 16 Oct 2020 17:10:58 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/5XKHkb/</guid>
      <description>服务经常会重启失败，写了个简单的shell叫脚本，可以发现服务端口是否起来，没有发现监听端口会重启服务
1.监控脚本 if [ -z &amp;#34;`lsof -i:443 | grep LISTEN`&amp;#34; ];then echo -e &amp;#34;restart $(date +&amp;#39;%Y-%m-%d %H:%M:%S&amp;#39;)&amp;#34; systemctl restart pixiublog fi 2.系统定时任务 */3 * * * * /usr/local/pixiublog/monitor.sh &amp;gt;&amp;gt; /usr/local/pixiublog/monitor.log 2&amp;gt;&amp;amp;1 </description>
    </item>
    
    <item>
      <title>haokiu网站启动脚本</title>
      <link>http://121.199.2.5:6080/v3yn45/</link>
      <pubDate>Thu, 15 Oct 2020 17:08:15 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/v3yn45/</guid>
      <description>haokiu 的启动脚本，配置 https 可以参考beego 通过acms.sh 使用 https
#!/bin/sh siteDir=&amp;#39;/usr/local/pixiublog&amp;#39; appName=&amp;#39;pixiublogMain&amp;#39; echo &amp;#34;kill the running program&amp;#34; ps -ef | grep $appName | grep -v grep | awk &amp;#39;{print $2}&amp;#39; | xargs kill -9 echo &amp;#34;sleep 3 secons for app to shutdown&amp;#34; sleep 3 echo &amp;#34;start program&amp;#34; cd $siteDir nohup $siteDir/$appName &amp;gt;&amp;gt; $siteDir/console.log 2&amp;gt;&amp;amp;1 &amp;amp; </description>
    </item>
    
    <item>
      <title>presto 官网</title>
      <link>http://121.199.2.5:6080/FhpG3z/</link>
      <pubDate>Mon, 28 Sep 2020 14:50:12 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/FhpG3z/</guid>
      <description>现在 presto 有两个官网：
旧presto官网 新presto官网 </description>
    </item>
    
    <item>
      <title>presto 支持的数据源</title>
      <link>http://121.199.2.5:6080/7mlwkW/</link>
      <pubDate>Mon, 28 Sep 2020 14:40:32 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/7mlwkW/</guid>
      <description>presto 有很多连接器以支持不同的数据源：
Accumulo BigQuery Black Hole Cassandra Druid Elasticsearch Google Sheets Iceberg Hive JMX Kafka Kinesis Kudu Local File Memory MemSQL MongoDB MySQL Oracle Phoenix Pinot PostgreSQL Prometheus Redis Redshift SQL Server System Thrift TPCDS TPCH </description>
    </item>
    
    <item>
      <title>idea 付费版和免费版的区别</title>
      <link>http://121.199.2.5:6080/oqQib8/</link>
      <pubDate>Thu, 24 Sep 2020 11:08:33 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/oqQib8/</guid>
      <description>idea 付费版和免费版的区别 IntelliJ IDEA Ultimate IntelliJ IDEA Community Edition Java, Kotlin, Groovy, Scala + + Android + + Maven, Gradle, sbt + + Git, SVN, Mercurial + + Debugger + + Profiling tools + - Spring, Java EE, Micronaut, Quarkus, Helidon, and more + - Swagger, Open API Specifications + - JavaScript, TypeScript + - Database Tools, SQL + - </description>
    </item>
    
    <item>
      <title>okex</title>
      <link>http://121.199.2.5:6080/4Li9s1/</link>
      <pubDate>Wed, 23 Sep 2020 17:16:15 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/4Li9s1/</guid>
      <description>okex 是一个老牌的数字货币交易所，我一直在用OKEx安全简便地交易数字货币。
用我的邀请链接注册OKEx可以免费获得比特币奖励！点击这里 或者 使用这个地址： https://www.ouyi.fit/join/1841513
okex 是全球著名的数字资产交易平台之一，主要面向全球用户提供比特币、莱特币、以太币等数字资产的币币和衍生品交易服务。</description>
    </item>
    
    <item>
      <title>sql 优化</title>
      <link>http://121.199.2.5:6080/XEhpYv/</link>
      <pubDate>Wed, 23 Sep 2020 16:59:17 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/XEhpYv/</guid>
      <description>SQL 优化已经成为衡量程序猿优秀与否的硬性指标，甚至在各大厂招聘岗位职能上都有明码标注。
有朋友疑问到，SQL 优化真的有这么重要么？如下图所示，SQL 优化在提升系统性能中是：成本最低和优化效果最明显的途径。
**优化成本：**硬件&amp;gt;系统配置&amp;gt;数据库表结构&amp;gt;SQL 及索引。
**优化效果：**硬件&amp;lt;系统配置&amp;lt;数据库表结构&amp;lt;SQL 及索引。
对于MySQL层优化我一般遵从五个原则：
**减少数据访问：**设置合理的字段类型，启用压缩，通过索引访问等减少磁盘 IO。 **返回更少的数据：**只返回需要的字段和数据分页处理，减少磁盘 IO 及网络 IO。 **减少交互次数：**批量 DML 操作，函数存储等减少数据连接次数。 **减少服务器 CPU 开销：**尽量减少数据库排序操作以及全表查询，减少 CPU 内存占用。 **利用更多资源：**使用表分区，可以增加并行操作，更大限度利用 CPU 资源。 总结到 SQL 优化中，就如下三点：
最大化利用索引。 尽可能避免全表扫描。 减少无效数据的查询。 理解 SQL 优化原理 ，首先要搞清楚 SQL 执行顺序。
SELECT 语句，语法顺序如下：
1. SELECT 2. DISTINCT &amp;lt;select_list&amp;gt; 3. FROM &amp;lt;left_table&amp;gt; 4. &amp;lt;join_type&amp;gt; JOIN &amp;lt;right_table&amp;gt; 5. ON &amp;lt;join_condition&amp;gt; 6. WHERE &amp;lt;where_condition&amp;gt; 7. GROUP BY &amp;lt;group_by_list&amp;gt; 8. HAVING &amp;lt;having_condition&amp;gt; 9. ORDER BY &amp;lt;order_by_condition&amp;gt; 10.LIMIT &amp;lt;limit_number&amp;gt; SELECT 语句，执行顺序如下：
FROM &amp;lt;表名&amp;gt; # 选取表，将多个表数据通过笛卡尔积变成一个表。 ON &amp;lt;筛选条件&amp;gt; # 对笛卡尔积的虚表进行筛选 JOIN &amp;lt;join, left join, right join...&amp;gt; &amp;lt;join表&amp;gt; # 指定join，用于添加数据到on之后的虚表中，例如left join会将左表的剩余数据添加到虚表中 WHERE &amp;lt;where条件&amp;gt; # 对上述虚表进行筛选 GROUP BY &amp;lt;分组条件&amp;gt; # 分组 &amp;lt;SUM()等聚合函数&amp;gt; # 用于having子句进行判断，在书写上这类聚合函数是写在having判断里面的 HAVING &amp;lt;分组筛选&amp;gt; # 对分组后的结果进行聚合筛选 SELECT &amp;lt;返回数据列表&amp;gt; # 返回的单列必须在group by子句中，聚合函数除外 DISTINCT # 数据除重 ORDER BY &amp;lt;排序条件&amp;gt; # 排序 LIMIT &amp;lt;行数限制&amp;gt; 以下 SQL 优化策略适用于数据量较大的场景下，如果数据量较小，没必要以此为准，以免画蛇添足。</description>
    </item>
    
    <item>
      <title>elasticsearch</title>
      <link>http://121.199.2.5:6080/3XTrh1/</link>
      <pubDate>Wed, 23 Sep 2020 16:53:39 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/3XTrh1/</guid>
      <description>基本概念 Node 与 Cluster Elastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。
单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。
Index Elastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。
所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。
Document Index 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。Document 使用 JSON 格式表示。同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。
Type Document 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。
不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。
在6.0之前的版本，一个ElasticSearch索引中，可以有多个类型；从6.0版本开始，，一个ElasticSearch索引中，只有1个类型。一个类型是索引的一个逻辑上的分类，通常具有一组相同字段的文档组成。ElasticSearch的类型概念相当于关系数据库的数据表。
shard 当数据量较大时，索引的存储空间需求超出单个节点磁盘容量的限制，或者出现单个节点处理速度较慢。为了解决这些问题，ElasticSearch将索引中的数据进行切分成多个分片（shard），每个分片存储这个索引的一部分数据，分布在不同节点上。当需要查询索引时，ElasticSearch将查询发送到每个相关分片，之后将查询结果合并，这个过程对ElasticSearch应用来说是透明的，用户感知不到分片的存在。 一个索引的分片一旦指定，不再修改。
副本 其实，分片全称是主分片，简称为分片。主分片是相对于副本来说的，副本是对主分片的一个或多个复制版本（或称拷贝），这些复制版本（拷贝）可以称为复制分片，可以直接称之为副本。当主分片丢失时，集群可以将一个副本升级为新的主分片。
对比 ElasticSearch RDBMS 索引（index） 数据库（database） 类型（type） 表（table） 文档（document） 行（row） 字段（field） 列（column） 映射（mapping） 表结构（schema） 全文索引 索引 查询DSL SQL GET select PUT/POST update DELETE delete 节点信息 查看节点信息 curl localhost:9200 查看节点健康度 curl localhost:9200/_cat/health?v&amp;amp;pretty=true 查看集群状况 curl localhost:9200/_cat/nodes?v&amp;amp;pretty=true index(索引)操作 查看当前节点的所有 Index。 curl -X GET &amp;#39;http://localhost:9200/_cat/indices?v&amp;#39; 可以列出每个 Index 所包含的 Type curl &amp;#39;localhost:9200/_mapping?pretty=true&amp;#39; 新建一个名叫weather的 Index。 curl -X PUT &amp;#39;localhost:9200/weather&amp;#39; 删除这个 Index。 curl -X DELETE &amp;#39;localhost:9200/weather&amp;#39; 文档操作 获取 GET /website/blog/123?</description>
    </item>
    
    <item>
      <title>github 的 master 分支将更改为 main</title>
      <link>http://121.199.2.5:6080/LcW9jL/</link>
      <pubDate>Tue, 22 Sep 2020 09:22:43 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/LcW9jL/</guid>
      <description>从今年 10 月 1 日起，GitHub 在该平台上创建的所有新的源代码仓库将默认被命名为 &amp;ldquo;main&amp;rdquo;，而不是原先的&amp;quot;master&amp;quot;。值得注意的是，现有的存储库不会受到此更改影响。
早在今年 6 月份，受美国大规模的 “Black Lives Matter”运动影响，为了安抚愈演愈烈的民众情绪，GitHub 就宣布将替换掉 master 等术语，以避免联想奴隶制。现如今，在外界一些声音的催促下，这一举措则终于要正式落地了。
除 GitHub 外，为了避免带有所谓的“种族歧视色彩”，许多科技巨头或知名软件也都调整了自己的业务和产品，以平息社会舆论。包括有：MySQL 宣布删除 master、黑名单白名单等术语；Linus Torvalds 通过了 Linux 中避免 master/slave 等术语的提案；还有 Twitter 、GitHub、微软、LinkedIn、Ansible、Splunk、OpenZFS、OpenSSL、JP Morgan、 Android 移动操作系统、Go 编程语言、PHPUnit 和 Curl 等宣布要对此类术语进行删除或更改。同时，IBM、亚马逊、微软也都接连调整面部识别平台业务，以防加深歧视或遭受指责。
且最初在 Git 中写下“master”一词的开发者 Petr Baudis 也于 6 月份在社交网站上表明立场称，自己当年不该使用“master”这个可能给别人造成伤害的词语。并表示，他曾多次希望可以将“master”改成“main”(和“upstream”）。不过直到现在，才由 GitHub 开始主导替换工作。
而对于为何选择“main”而不是其他替换词汇，Github 方面给出的解释为，main 是他们在平台上看到的最受欢迎的 master 替代品。并且 main 这个词汇很短，可以帮助用户形成良好的肌肉记忆；在很多种语言中翻译起来也都很容易。
此外，Github 还透露，截至今年年底，他们将使现有存储库无缝重命名其默认分支。当用户重命名分支机构时，他们将重新定位打开的 PR 和草稿版本、移动分支机构保护策略等，且所有的这些都将自动完成。
事实上，计算机术语政治正确性早已不是新鲜话题。2004 年，“master/slave”曾被全球语言检测机构评为年度最不政治正确的十大词汇之一，时任主席称这是政治渗透到计算机技术控制中的表现。2008 年，开源软件 Drupal 在社区发布消息，高调站队，将“master/slave”重命名为“client/server”。2018年，IETF 也在草案当中指出，要求开源软件更改“master/slave”和“blacklist/whitelist”两项表述。
但是值得思考的是，在计算机源码领域中，“master/slave”和“blacklist/whitelist”之类的技术用语有错吗？一味的“一刀切”的话，会不会导致所谓的矫枉过正呢</description>
    </item>
    
    <item>
      <title>es6 简介</title>
      <link>http://121.199.2.5:6080/Cqhoxc/</link>
      <pubDate>Mon, 21 Sep 2020 09:30:35 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/Cqhoxc/</guid>
      <description>ECMAScript 6.0（以下简称 ES6）是 JavaScript 语言的下一代标准，已经在 2015 年 6 月正式发布了。它的目标，是使得 JavaScript 语言可以用来编写复杂的大型应用程序，成为企业级开发语言。
ECMAScript 和 JavaScript 的关系 一个常见的问题是，ECMAScript 和 JavaScript 到底是什么关系？
要讲清楚这个问题，需要回顾历史。1996 年 11 月，JavaScript 的创造者 Netscape 公司，决定将 JavaScript 提交给标准化组织 ECMA，希望这种语言能够成为国际标准。次年，ECMA 发布 262 号标准文件（ECMA-262）的第一版，规定了浏览器脚本语言的标准，并将这种语言称为 ECMAScript，这个版本就是 1.0 版。
该标准从一开始就是针对 JavaScript 语言制定的，但是之所以不叫 JavaScript，有两个原因。一是商标，Java 是 Sun 公司的商标，根据授权协议，只有 Netscape 公司可以合法地使用 JavaScript 这个名字，且 JavaScript 本身也已经被 Netscape 公司注册为商标。二是想体现这门语言的制定者是 ECMA，不是 Netscape，这样有利于保证这门语言的开放性和中立性。
因此，ECMAScript 和 JavaScript 的关系是，前者是后者的规格，后者是前者的一种实现（另外的 ECMAScript 方言还有 JScript 和 ActionScript）。日常场合，这两个词是可以互换的。
Babel 转码器 Babel 是一个广泛使用的 ES6 转码器，可以将 ES6 代码转为 ES5 代码，从而在现有环境执行。这意味着，你可以用 ES6 的方式编写程序，又不用担心现有环境是否支持。下面是一个例子。
// 转码前 input.map(item =&amp;gt; item + 1); // 转码后 input.map(function (item) { return item + 1; }); 上面的原始代码用了箭头函数，Babel 将其转为普通函数，就能在不支持箭头函数的 JavaScript 环境执行了。</description>
    </item>
    
    <item>
      <title>免费的云存储</title>
      <link>http://121.199.2.5:6080/0K1gzu/</link>
      <pubDate>Fri, 18 Sep 2020 09:47:06 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/0K1gzu/</guid>
      <description>七牛云存储 提供了每月10G的免费存储，一般的网站已经够用，如果超额了，费用也很低，可以考虑将网站的图片存在七牛，安全可靠。</description>
    </item>
    
    <item>
      <title>网站部署脚本</title>
      <link>http://121.199.2.5:6080/1gTd1h/</link>
      <pubDate>Thu, 17 Sep 2020 21:52:12 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/1gTd1h/</guid>
      <description>网站的部署脚本
#!/bin/sh #部署目录 siteDir=&amp;#39;/usr/local/pixiublog&amp;#39; # 源代码目录 cd /root/pixiublog echo &amp;#34;update code&amp;#34; git pull echo &amp;#34;build pixiublog&amp;#34; go build main.go echo &amp;#34;remove old&amp;#34; rm -rf $siteDir/views rm -rf $siteDir/static rm -rf $siteDir/pixiublog echo &amp;#34;mv new program to $siteDir&amp;#34; cp main $siteDir/pixiublog cp -rf views $siteDir/ cp -rf static $siteDir/ echo &amp;#34;kill the running program&amp;#34; ps -ef | grep &amp;#39;pixiublog&amp;#39; | grep -v grep | awk &amp;#39;{print $2}&amp;#39; | xargs kill -9 echo &amp;#34;start program&amp;#34; cd $siteDir nohup $siteDir/pixiublog &amp;gt;&amp;gt; $siteDir/console.log 2&amp;gt;&amp;amp;1 &amp;amp; </description>
    </item>
    
    <item>
      <title>在线交互式学习k8s/docker/linux</title>
      <link>http://121.199.2.5:6080/baIB8x/</link>
      <pubDate>Thu, 17 Sep 2020 13:58:36 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/baIB8x/</guid>
      <description>katacoda is Interactive Learning and Training Platform for Software Engineers Helping Developers Learn and Companies Increase Adoption
可以交互式的学习各种前沿技术：k8s linux docker 机器学习等</description>
    </item>
    
    <item>
      <title>vscode 列选择 快捷键</title>
      <link>http://121.199.2.5:6080/QCpYff/</link>
      <pubDate>Mon, 14 Sep 2020 09:27:25 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/QCpYff/</guid>
      <description>vscode 列选择 快捷键: shift + alt + 鼠标左键</description>
    </item>
    
    <item>
      <title>frp 使用</title>
      <link>http://121.199.2.5:6080/HYawYY/</link>
      <pubDate>Tue, 08 Sep 2020 09:50:15 +0000</pubDate>
      
      <guid>http://121.199.2.5:6080/HYawYY/</guid>
      <description>frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp, http, https 协议。本文简单的介绍frp的配置使用。
实现内网穿需要有一台公网服务器。本文将公网服务器称为服务端，内网服务器称为客户端。需要开启相关的端口。相关端口没开通，访问就会失败。
frp git地址:https://github.com/fatedier/frp 中文文档：https://github.com/fatedier/frp/blob/master/README_zh.md frp下载地址：https://github.com/fatedier/frp/releases 本文使用软件：frp_0.21.0_linux_amd64.tar.gz 本文使用系统：centos7（公网一台，内网一台）
本文使用软件：frp_0.21.0_linux_amd64.tar.gz，frp的客户端和服务端都在同一个包里。
文件说明 frps.ini: 服务端配置文件 frps: 服务端软件 frpc.ini: 客户端配置文件 frpc: 客户端软件
frps.ini配置 [common] bind_port = 7000 # auth token token = Qwert123 dashboard_port = 7500 # dashboard 用户名密码，默认都为 admin dashboard_user = admin dashboard_pwd = Qwert123 vhost_http_port = 7083 开启服务端服务 ./frps -c ./frps.ini
可以用脚本来启动：
#!/bin/sh nohup /usr/local/frp/frps -c /usr/local/frp/frps.ini &amp;amp; frpc.ini配置 [common] #服务器ip地址 server_addr = 121.199.2.XXX server_port = 7000 #开放api，提供reload服务 admin_addr = 127.0.0.1 admin_port = 7400 # auth token token = Qwert123 [ssh] type = tcp local_ip = 127.0.0.1 #ssh端口 local_port = 22 remote_port = 1022 可以使用脚本来启动(start.sh)
#!/bin/sh sudo nohup /usr/local/frp/frpc -c /usr/local/frp/frpc.</description>
    </item>
    
    
  </channel>
</rss>
